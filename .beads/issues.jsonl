{"id":"infra-08x","title":"Test and deploy idempotency test","description":"## Testing Strategy\n1. Run script manually first to verify logic\n2. Test with a host that has a known changed task\n3. Verify healthcheck.io receives correct status/body\n4. Deploy CronJob and monitor first few runs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:44:27.064194-05:00","created_by":"coneill","updated_at":"2026-01-09T16:13:03.850796-05:00","closed_at":"2026-01-09T16:13:03.850796-05:00","close_reason":"Tested and deployed - PR #1108 merged, CronJob running successfully","dependencies":[{"issue_id":"infra-08x","depends_on_id":"infra-a0y","type":"blocks","created_at":"2026-01-07T08:44:49.855343-05:00","created_by":"coneill"},{"issue_id":"infra-08x","depends_on_id":"infra-d1y","type":"blocks","created_at":"2026-01-07T08:44:49.90763-05:00","created_by":"coneill"}]}
{"id":"infra-10o","title":"Evaluate PR #1130: Update hertzg/rtl_433:25.12 Docker digest to 04b2c79","description":"Run the renovate-eval skill against PR #1130 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.500241-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:33.994968-05:00","closed_at":"2026-01-12T18:48:33.994968-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-10o","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.120948-05:00","created_by":"coneill"}]}
{"id":"infra-19z","title":"Test GitHub Actions job summary","description":"Push to main and verify: 1) Workflow triggers 2) Job summary contains deployment links 3) Links are clickable in GitHub UI","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T19:17:17.80951-05:00","created_by":"coneill","updated_at":"2026-01-03T19:19:04.690856-05:00","closed_at":"2026-01-03T19:19:04.690856-05:00","close_reason":"Merged into infra-oty.4","dependencies":[{"issue_id":"infra-19z","depends_on_id":"infra-25p","type":"blocks","created_at":"2026-01-03T19:17:23.920093-05:00","created_by":"coneill"}]}
{"id":"infra-1aa","title":"Security and Backward Compatibility","description":"**Core Questions:**\n- IPv6-specific firewall rules at UniFi/network level?\n- Does IPv6 change threat surface (link-local, neighbor discovery)?\n- Dual-stack: can we enable IPv6 without breaking IPv4?\n- Hosts that can't support IPv6: how to exclude them?\n- Phased rollout vs. all-at-once deployment?\n\n**Investigation Needed:**\n- Audit hosts for IPv6 capability\n- Review UniFi firewall configuration for IPv4 rules to replicate\n- Check CI/CD and monitoring for IPv4-only assumptions\n\n**Critical Files:**\n- UniFi firewall configuration (if managed by Terraform)\n- Kubernetes network policies\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:24.482231-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.45916-05:00","closed_at":"2026-01-22T23:32:54.45916-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-1ff","title":"Configure beads for infra repo","description":"Set up beads properly for infra repo with amend-friendly workflow. Address bd doctor warnings: hooks, sync config, untracked files, merge artifacts, duplicates","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-03T19:23:13.710598-05:00","created_by":"coneill","updated_at":"2026-01-04T00:53:55.794377-05:00","closed_at":"2026-01-04T00:53:55.794377-05:00","close_reason":"Closed"}
{"id":"infra-1r4","title":"Evaluate PR #1135: Update dependency esphome to v2025.12.2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.607602-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-1rh","title":"Evaluate PR #1130: Update hertzg/rtl_433:25.12 Docker digest to 04b2c79","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.848012-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-22e","title":"Evaluate PR #1151: Update mariadb Docker tag to v12","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.436165-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-23a","title":"Fix INJECT_FACTS_AS_VARS in host_vars/flux.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:52:54.404975-05:00","created_by":"coneill","updated_at":"2026-01-13T19:24:55.623966-05:00","closed_at":"2026-01-13T19:24:55.623966-05:00","close_reason":"Fixed: changed ansible_* to ansible_facts['*'] syntax"}
{"id":"infra-25p","title":"Test semaphore-deploy summary output locally","description":"Run semaphore-deploy manually and verify summary output:\n\n1. **Single attempt (success):** Run `./scripts/semaphore-deploy --hosts k1 --project Infra --template ansible-deploy`\n   - Verify summary prints to console\n   - Verify format shows direct links: `**Semaphore:** [URL]` and `**ARA:** [URL]`\n\n2. **Multiple attempts:** Force retry scenario (use flaky host or --max-attempts with intentional failure)\n   - Verify list format with status indicators (✅/❌)\n   - Verify each attempt shows its Semaphore and ARA URLs\n\nBlocks: infra-oty.4 (end-to-end test)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T19:17:17.267124-05:00","created_by":"coneill","updated_at":"2026-01-04T11:35:37.014657-05:00","closed_at":"2026-01-04T11:35:37.014657-05:00","close_reason":"Testing completed; downstream tasks infra-19z and infra-oty.4 closed"}
{"id":"infra-29t","title":"Terraform Data Structure Changes","description":"**Core Questions:**\n- Add optional `ipv6` field to `infrastructure_hosts`, or compute it?\n- Add explicit `vlan` field, or derive from IP?\n- Migration path: can derivation be automatic for hosts without explicit IPv6?\n- Store derived IPv6 in state, or compute on every apply?\n- ESPHome special handling: should `gen-esphome-hosts` also generate IPv6?\n\n**Investigation Needed:**\n- Review `scripts/gen-esphome-hosts` to understand current logic (lines 1-100 read)\n- Check UniFi provider schema: does `unifi_user` support IPv6 fixed addresses?\n- Verify UDMP supports DHCPv6 reservations (not just SLAAC)\n\n**Critical Files:**\n- `opentofu/modules/dns/variables.tf` - current type schema\n- `scripts/gen-esphome-hosts` - auto-generation logic\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:08.980484-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.462166-05:00","closed_at":"2026-01-22T23:32:54.462166-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1.","dependencies":[{"issue_id":"infra-29t","depends_on_id":"infra-gaf","type":"blocks","created_at":"2026-01-20T14:07:44.010761-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-29t","depends_on_id":"infra-66l","type":"blocks","created_at":"2026-01-20T14:07:45.667865-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-2cw","title":"Audit all versioned artifacts for Renovate coverage","description":"Comprehensive evaluation of all versioned artifacts in this repo (Docker images, Helm charts, GitHub Actions, Python packages, etc.) to ensure Renovate is tracking and updating them all.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T20:13:32.215832-05:00","created_by":"coneill","updated_at":"2026-01-06T21:20:49.474613-05:00","closed_at":"2026-01-06T21:20:49.474613-05:00","close_reason":"Audit complete - ~98% Renovate coverage. Documented findings: 3 cleanup items, 1 version drift, ARA bug tracked in #998"}
{"id":"infra-2jr","title":"Fix INJECT_FACTS_AS_VARS in roles/kubeadm/tasks/master.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:52:58.812423-05:00","created_by":"coneill","updated_at":"2026-01-13T19:24:55.625608-05:00","closed_at":"2026-01-13T19:24:55.625608-05:00","close_reason":"Fixed: changed ansible_* to ansible_facts['*'] syntax"}
{"id":"infra-2ky","title":"Evaluate PR #1147: Update Helm release tailscale-operator to v1.92.4","description":"Run the renovate-eval skill against PR #1147 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.360317-05:00","created_by":"coneill","updated_at":"2026-01-12T22:37:14.603405-05:00","closed_at":"2026-01-12T22:37:14.603405-05:00","close_reason":"PR #1147 merged after manual deploy validation","dependencies":[{"issue_id":"infra-2ky","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.147937-05:00","created_by":"coneill"}]}
{"id":"infra-2r9","title":"Refactor semaphore-deploy into shared library","description":"Extract SemaphoreDeployer class, DeploymentError exception, and constants into scripts/lib/semaphore.py. Update scripts/semaphore-deploy to import from lib, keeping CLI interface unchanged.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-07T08:38:50.073946-05:00","created_by":"coneill","updated_at":"2026-01-07T08:39:24.915251-05:00","deleted_at":"2026-01-07T08:39:24.915251-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-2wn","title":"Enable IPv6 for Kubernetes workloads","status":"open","priority":3,"issue_type":"feature","owner":"clayton@oneill.net","created_at":"2026-01-20T11:00:45.762375-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-20T11:00:45.762375-05:00","dependencies":[{"issue_id":"infra-2wn","depends_on_id":"infra-ec1","type":"blocks","created_at":"2026-01-22T23:35:01.086379-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-35e","title":"Evaluate PR #1140: Update prom/prometheus Docker tag to v3.8.1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.247031-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-36z","title":"Kubernetes cluster health remediation","description":"Remediate issues discovered during Jan 2026 cluster health audit","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-10T10:33:19.248376-05:00","created_by":"coneill","updated_at":"2026-01-13T10:20:56.736843-05:00","closed_at":"2026-01-13T10:20:56.736843-05:00","close_reason":"All child tasks completed"}
{"id":"infra-36z.1","title":"Remove node k4 from cluster","description":"k4 has been down since Jan 3rd (kubelet stopped). Force delete stuck pods (alloy, spegel) then delete the node. Already removed from Ansible inventory.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T10:33:38.234928-05:00","created_by":"coneill","updated_at":"2026-01-10T10:51:46.896001-05:00","closed_at":"2026-01-10T10:51:46.896001-05:00","close_reason":"Node k4 removed from cluster. Force deleted stuck pods (alloy, spegel) then deleted node.","dependencies":[{"issue_id":"infra-36z.1","depends_on_id":"infra-36z","type":"parent-child","created_at":"2026-01-10T10:33:38.235705-05:00","created_by":"coneill"}]}
{"id":"infra-36z.2","title":"Delete orphaned sabnzbd ArgoCD application","description":"Directory kubernetes/sabnzbd was removed in commit c3c6fe1e but ArgoCD app still exists with sync status Unknown.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T10:34:02.939311-05:00","created_by":"coneill","updated_at":"2026-01-10T10:53:26.47458-05:00","closed_at":"2026-01-10T10:53:26.47458-05:00","close_reason":"Removed finalizer and deleted orphaned ArgoCD application","dependencies":[{"issue_id":"infra-36z.2","depends_on_id":"infra-36z","type":"parent-child","created_at":"2026-01-10T10:34:02.940262-05:00","created_by":"coneill"}]}
{"id":"infra-36z.3","title":"Delete orphaned cwa-oidc ExternalSecret","description":"CWA (Calibre Web Automated) was removed in commit fb705056. The ExternalSecret is orphaned and failing.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T10:34:14.858288-05:00","created_by":"coneill","updated_at":"2026-01-10T10:53:47.621216-05:00","closed_at":"2026-01-10T10:53:47.621216-05:00","close_reason":"Deleted orphaned ExternalSecret - CWA was previously removed","dependencies":[{"issue_id":"infra-36z.3","depends_on_id":"infra-36z","type":"parent-child","created_at":"2026-01-10T10:34:14.85918-05:00","created_by":"coneill"}]}
{"id":"infra-36z.4","title":"Investigate awair-exporter crash","description":"Exporter crashes with IndexError at line 36 - Awair API returning empty data for device 'My Office'. Device still in use - need physical device check or API investigation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T10:34:41.0312-05:00","created_by":"coneill","updated_at":"2026-01-13T10:20:53.21776-05:00","closed_at":"2026-01-13T10:20:53.21776-05:00","close_reason":"Pod running stable 22h with 0 restarts - issue self-resolved","dependencies":[{"issue_id":"infra-36z.4","depends_on_id":"infra-36z","type":"parent-child","created_at":"2026-01-10T10:34:41.032069-05:00","created_by":"coneill"}]}
{"id":"infra-3bq","title":"Add flake.nix packages output for ansible-idempotency-test","description":"## File\nModify: `flake.nix`\n\n## Changes\nAdd `packages` output with `ansible-idempotency-test` wrapper. The flake already has ARA + websocket-client in the Python env, so we can wrap the script:\n\n```nix\npackages = forEachSupportedSystem ({ pkgs }:\n  let\n    pythonEnv = pkgs.python3.withPackages (ps: with ps; [\n      ansible websocket-client ara  # existing deps\n    ]);\n  in {\n    ansible-idempotency-test = pkgs.writeShellApplication {\n      name = \"ansible-idempotency-test\";\n      runtimeInputs = [ pythonEnv ];\n      text = '\n        exec python3 ${./scripts/ansible-idempotency-test} \"$@\"\n      ';\n    };\n  });\n```\n\n## Result\nThis enables: `nix run github:claytono/infra#ansible-idempotency-test`\n\n## Note\nThe existing flake already has the ARA package built as a custom buildPythonPackage (lines 45-78 in flake.nix). Reuse that Python environment.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:41:39.450349-05:00","created_by":"coneill","updated_at":"2026-01-08T11:40:13.73062-05:00","closed_at":"2026-01-08T11:40:13.73062-05:00","close_reason":"packages output already exists in flake.nix; also adding semaphore devShell as part of Phase 1 work","dependencies":[{"issue_id":"infra-3bq","depends_on_id":"infra-a0g","type":"blocks","created_at":"2026-01-07T08:44:49.701243-05:00","created_by":"coneill"}]}
{"id":"infra-3s7","title":"Enable IPv6 for homelab servers","description":"# Enable IPv6 for homelab servers\n\n## Research Workflow - READ THIS FIRST\n\n**Starting a New Research Session:**\n1. Run `bd show infra-3s7` to see all context and previous research (in comments)\n2. Check dependencies: identify which sub-bead is next in logical order\n3. Run `bd ready` to verify the sub-bead has no blockers\n4. Begin research using ALL information from parent bead comments\n\n**Research Process:**\nEach sub-bead (infra-8sx, infra-gaf, etc.) is researched in an independent Claude Code session:\n\n1. **Read parent bead:** `bd show infra-3s7` - ALL context is in this bead's comments\n2. **Research the specific sub-bead:** Answer its core questions using codebase exploration\n3. **Document findings:** Add research results as a comment to parent bead (infra-3s7)\n4. **Close sub-bead:** Mark the research task complete\n5. **Next bead:** Next session starts at step 1 with updated context\n\n**Why This Works:**\n- `bd show infra-3s7` provides complete context for each independent session\n- Comments accumulate research findings sequentially\n- No conversation history needed - everything in the parent bead\n- Each session can pick up where the previous one left off\n\n**Final Step:**\nAfter all sub-beads researched, synthesize findings into implementation plan.\n\n---\n\n## Network Context\n- /56 delegated from ISP (may change after long power outage)\n- Router assigns /64 per VLAN\n\n## VLAN to IPv6 Prefix Mapping\n| VLAN | IPv4 Subnet | IPv6 Prefix |\n|------|-------------|-------------|\n| Default | 172.19.74.0/24 | 2600:4040:2ece:7500::/64 |\n| Servers | 172.19.75.0/24 | 2600:4040:2ece:7503::/64 |\n| IOT | 172.20.4.0/22 | 2600:4040:2ece:7501::/64 |\n| Guest | 172.19.77.0/24 | 2600:4040:2ece:7502::/64 |\n\n## IPv6 Address Derivation\nDerive IPv6 from IPv4 to keep addresses human-readable:\n- Use /64 prefix for the VLAN\n- Third IPv4 octet as segment\n- Fourth octet split: hundreds : remainder\n\nExamples:\n- 172.19.74.120 -\u003e 2600:4040:2ece:7500::74:1:20\n- 172.19.74.31 -\u003e 2600:4040:2ece:7500::74:0:31\n- 172.20.4.100 -\u003e 2600:4040:2ece:7501::4:1:0\n\n## Work Required\n- Terraform: subnet-to-prefix mapping, derive IPv6 from IPv4, add AAAA records\n- Ansible: ensure hosts have stable IPv6 (static or disable privacy extensions), sysctl settings","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-11T21:30:12.877361-05:00","created_by":"coneill","updated_at":"2026-01-22T23:33:04.991996-05:00","closed_at":"2026-01-22T23:33:04.991996-05:00","close_reason":"Research phase complete. Implementation work tracked in epic infra-ec1 with 6 phase beads (infra-kt0 through infra-frb).","dependencies":[{"issue_id":"infra-3s7","depends_on_id":"infra-gaf","type":"blocks","created_at":"2026-01-20T14:07:54.08659-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-8sx","type":"blocks","created_at":"2026-01-20T14:07:54.177447-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-66l","type":"blocks","created_at":"2026-01-20T14:07:54.269135-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-29t","type":"blocks","created_at":"2026-01-20T14:07:54.360239-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-4ps","type":"blocks","created_at":"2026-01-20T14:07:54.472615-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-pof","type":"blocks","created_at":"2026-01-20T14:07:54.568192-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-tvy","type":"blocks","created_at":"2026-01-20T14:07:54.652402-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-ao8","type":"blocks","created_at":"2026-01-20T14:07:54.739228-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-gbn","type":"blocks","created_at":"2026-01-20T14:07:54.83129-05:00","created_by":"Clayton O'Neill"},{"issue_id":"infra-3s7","depends_on_id":"infra-1aa","type":"blocks","created_at":"2026-01-20T14:07:54.926133-05:00","created_by":"Clayton O'Neill"}],"comments":[{"id":7,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-4ps: Route53 AAAA Record Management ✓\n\n**Core Questions Answered:**\n\n1. **AAAA records for all hosts or only public_dns=true?**\n   - Answer: Use BOTH `public_dns` AND `enable_ipv6` filters\n   - Filter logic: `if v.public_dns \u0026\u0026 v.enable_ipv6`\n   - Excluded: luser (public_dns=false), p2/p3/p4-amt (enable_ipv6=false)\n   - Result: 16 of 19 hosts get AAAA records\n\n2. **TTL strategy?**\n   - Answer: Match A record TTL of 300 seconds\n   - Rationale: DNS best practice - matching TTLs prevent cache inconsistencies\n   - Infrastructure hosts change more frequently than nameservers\n\n3. **Validation to prevent conflicts?**\n   - Answer: No conflicts possible - IPv6 computed deterministically from IPv4\n   - Formula: `2600:4040:2ece:7500::74:\u003chundreds\u003e:\u003cremainder\u003e`\n   - Route53 auto-validates IPv6 format\n   - Examples: .41 → ::74:0:41, .134 → ::74:1:34, .120 → ::74:1:20\n\n4. **CNAMEs automatic AAAA support?**\n   - Answer: Yes, automatic - standard DNS chaining\n   - No special handling needed\n   - CNAMEs (nut, zwavejs, etc.) will resolve to target's AAAA once created\n\n**Implementation Design:**\n- Compute IPv6 INSIDE DNS module (not in root locals.tf)\n- Pass `infrastructure_ipv6_prefix` as module parameter\n- Add `enable_ipv6` optional field to infrastructure_hosts type\n- AMT interfaces get `enable_ipv6 = false` (IPv4-only management)\n\n**Files to modify (5 total):**\n1. `opentofu/locals.tf` - Add enable_ipv6=false to AMT hosts\n2. `opentofu/main.tf` - Pass IPv6 prefix to DNS module\n3. `modules/dns/variables.tf` - Add enable_ipv6 field + prefix var\n4. `modules/dns/main.tf` - Compute IPv6 in locals with format()\n5. `modules/dns/oneill.tf` - Update A record source, add AAAA records\n\n**Expected outcome:**\n- 16 new AAAA records created\n- 0 resources changed (A record change is in-place)\n- All CNAMEs automatically gain IPv6 via target resolution\n\n**Cascade effects:**\n- Unblocks: Implementation of AAAA records (this research answers all questions)\n- Informs: infra-tvy (Ansible can use same filtering), infra-ao8 (testing strategy)\n\n**Full research plan:** /Users/coneill/.claude/plans/shimmying-honking-scroll.md","created_at":"2026-01-20T23:40:53Z"},{"id":1,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-8sx: VLAN/Subnet Identification Strategy ✓\n\n**VLAN Distribution:**\n- ALL 15 Ansible inventory servers are on **172.19.74.0/24 (Default VLAN)**\n- Maps to IPv6 prefix: **2600:4040:2ece:7500::/64**\n- Other VLANs (75/Servers, 77/Guest, 20.x/IoT) are **OUT OF SCOPE**\n\n**Server Inventory (Ansible inventory only, 15 total):**\n- 5 Proxmox hosts: p1-p4, p9 (172.19.74.41-44, .155)\n- 4 Kubernetes nodes: k1-k4 (172.19.74.134, .112, .74, .75)\n- 3 VMs: infra1, luser, fs2 (172.19.74.31, .161, .139)\n- 3 Raspberry Pis: garagepi, pantrypi, pantrypi-wifi (172.19.74.216, .120, .118)\n\n**Excluded:**\n- AMT interfaces (p2-amt, p3-amt, p4-amt) - Do not support IPv6\n- Desktop (szamar) - Not in ansible inventory\n\n**Strategy Decision:** Single IPv6 prefix constant\n- All servers on one VLAN → simple constant, not a map\n- `infrastructure_ipv6_prefix = \"2600:4040:2ece:7500::/64\"`\n- Can extend to map structure if servers move to other VLANs later\n\n**Proposed Data Structure (opentofu/locals.tf):**\n```hcl\nlocals {\n  # IPv6 prefix for Default VLAN (all infrastructure servers)\n  infrastructure_ipv6_prefix = \"2600:4040:2ece:7500::/64\"\n}\n```\n\n**Edge Cases:**\n- Multi-interface hosts (pantrypi + pantrypi-wifi): Each derives IPv6 independently, no special handling\n- Future VLAN migration: Convert to map structure if needed\n\n**Pattern References:**\n- `opentofu/modules/dns/main.tf` lines 12-32: nameserver_ips local structure\n\n**Next Task:** infra-gaf (IPv6 Derivation Implementation) will use this prefix","created_at":"2026-01-20T19:56:27Z"},{"id":2,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-gaf: IPv6 Derivation Implementation in Terraform ✓\n\n**HCL Functions Are Sufficient:**\nThe IPv6 derivation formula can be implemented reliably using native HCL functions:\n- `split(\".\", ip)` - Split IPv4 into octets\n- `tonumber()` - Convert string to number\n- `floor()` - Get hundreds digit\n- `% (modulo)` - Get remainder\n- `format()` - Build IPv6 string with proper formatting\n\n**Working Formula:**\n```hcl\nformat(\n  \"%s::%s:%d:%d\",\n  infrastructure_ipv6_prefix,                   # \"2600:4040:2ece:7500\"\n  split(\".\", host.ip)[2],                       # Third octet (e.g., \"74\")\n  floor(tonumber(split(\".\", host.ip)[3]) / 100), # Hundreds digit\n  tonumber(split(\".\", host.ip)[3]) % 100        # Remainder\n)\n```\n\n**Verified Examples:**\n- 172.19.74.41 → 2600:4040:2ece:7500::74:0:41 ✓\n- 172.19.74.120 → 2600:4040:2ece:7500::74:1:20 ✓\n- 172.19.74.134 → 2600:4040:2ece:7500::74:1:34 ✓\n- 172.19.74.155 → 2600:4040:2ece:7500::74:1:55 ✓\n- 172.19.74.216 → 2600:4040:2ece:7500::74:2:16 ✓\n\n**Edge Cases Tested:**\n- Network address (.0) → ::74:0:0 ✓\n- Gateway (.1) → ::74:0:1 ✓\n- Broadcast (.255) → ::74:2:55 ✓\n- All produce valid IPv6 addresses with no collisions\n\n**Multi-Interface Hosts:**\nNo special handling needed. pantrypi/pantrypi-wifi each have unique IPs:\n- pantrypi: 172.19.74.120 → 2600:4040:2ece:7500::74:1:20\n- pantrypi-wifi: 172.19.74.118 → 2600:4040:2ece:7500::74:1:18\nDifferent last octets = different IPv6 addresses, no collision risk.\n\n**Implementation Pattern:**\n```hcl\nlocals {\n  infrastructure_ipv6_prefix = \"2600:4040:2ece:7500\"\n  \n  infrastructure_hosts_with_ipv6 = {\n    for name, host in local.infrastructure_hosts : name =\u003e merge(host, {\n      ipv6 = format(\n        \"%s::%s:%d:%d\",\n        local.infrastructure_ipv6_prefix,\n        split(\".\", host.ip)[2],\n        floor(tonumber(split(\".\", host.ip)[3]) / 100),\n        tonumber(split(\".\", host.ip)[3]) % 100\n      )\n    })\n  }\n}\n```\n\n**Validation:**\n- Collision prevention: Formula is deterministic (same input = same output)\n- IPv4 uniqueness guarantees IPv6 uniqueness within the /64 prefix\n- No manual collision checking needed\n\n**Integration Points:**\n1. **opentofu/locals.tf** (lines 17-140): Add `infrastructure_ipv6_prefix` constant and derive IPv6 for each host\n2. **DNS module variable**: Pass `infrastructure_hosts_with_ipv6` instead of `infrastructure_hosts`\n3. **AAAA records**: Follow pattern at oneill.tf:141-149 (infrastructure_hosts A records)\n\n**Test Files Created:**\n- `.tmp/ipv6-derivation-function.tf` - Production-ready example with sample hosts\n\n**Critical Decision:**\nStore VLAN-to-prefix mapping as simple constant (not variable):\n- All servers on one VLAN = no need for map structure\n- Constant in locals.tf keeps it close to infrastructure_hosts definition\n- Can extend to map if servers move to other VLANs later\n\n**Next Tasks:**\nThis unblocks infra-29t (Terraform Data Structure Changes) for implementation.","created_at":"2026-01-20T20:02:13Z"},{"id":3,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-66l: IPv6 Address Assignment Method ✓\n\n**Decision: Static IPv6 configuration managed via Ansible**\n\n**Address Derivation Formula:**\n```\nIPv4: 172.19.74.X → IPv6: 2600:4040:2ece:7500::74:X\n```\n\nExamples:\n- k1: 172.19.74.134 → 2600:4040:2ece:7500::74:134\n- p1: 172.19.74.41 → 2600:4040:2ece:7500::74:41\n- pantrypi: 172.19.74.120 → 2600:4040:2ece:7500::74:120\n\n**Rationale:**\n- Infrastructure already uses static IPv4 throughout\n- Need stable addresses for DNS (AAAA records)\n- Ansible manages all network config declaratively\n- No privacy extensions needed (servers, not clients)\n\n**Network Management:**\n- All hosts use ifupdown (Debian traditional networking)\n- Proxmox (p1-p4): ifupdown2 via `/etc/network/interfaces`\n- Other hosts: michaelrigart.interfaces role\n- michaelrigart.interfaces fully supports IPv6 via `ip6` parameter\n\n**Implementation Approach:**\n- Ansible: Add `ip6` config to host_vars files\n- Terraform: Add explicit `ipv6` field to infrastructure_hosts\n- DNS: Create AAAA records from ipv6 field\n\n**Cascade Effects:**\n- Unblocks infra-29t (Terraform Data Structure)\n- Unblocks infra-tvy (Ansible IPv6 Config)\n- Informs infra-4ps (Route53 AAAA Records)\n- Informs infra-pof (Sysctl Config - privacy extensions not needed)\n\n**Out of Scope:**\n- Kubernetes dual-stack (separate workstream: infra-2wn)","created_at":"2026-01-20T20:49:22Z"},{"id":4,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-29t: Terraform Data Structure Changes ✓\n\n**Decision: Compute IPv6 inside DNS module, add optional enable_ipv6 field**\n\n**Key Architectural Decisions:**\n\n1. **Computed IPv6 (not explicit)**: IPv6 addresses computed from IPv4 via `for` expression\n   - Single source of truth (IPv4 remains authoritative)\n   - No duplication risk, no manual maintenance\n   - Formula: `2600:4040:2ece:7500::74:X` (embedded third octet + final octet)\n\n2. **DNS module encapsulation**: IPv6 computation is implementation detail of DNS module\n   - Root `main.tf` passes IPv6 prefix as module parameter\n   - DNS module computes addresses internally using `merge()` + `format()`\n   - Better separation of concerns than root-level locals\n\n3. **Optional enable_ipv6 field**: Control which hosts get IPv6\n   - Defaults to `true` (most hosts get IPv6)\n   - Set `enable_ipv6 = false` for AMT interfaces (no IPv6 support)\n   - Follows existing pattern (`public_dns = optional(bool, true)`)\n\n4. **No VLAN field**: All infrastructure hosts on single VLAN (172.19.74.0/24)\n   - No need for map structure\n   - Can extend later if multi-VLAN needed\n\n**Implementation Files:**\n- `opentofu/locals.tf`: Add `enable_ipv6 = false` to p2-amt, p3-amt, p4-amt\n- `opentofu/main.tf`: Pass `infrastructure_ipv6_prefix` to DNS module\n- `opentofu/modules/dns/variables.tf`: Add prefix var + enable_ipv6 optional field\n- `opentofu/modules/dns/main.tf`: Compute IPv6 addresses in locals block\n- `opentofu/modules/dns/oneill.tf`: Create AAAA records (filtered for ipv6 != null)\n\n**Migration:**\n- Zero migration needed (computed fields don't affect state)\n- Purely additive changes\n- `tofu plan` will show only new AAAA records\n\n**Out of Scope:**\n- ESPHome IPv6 (different network, Phase 2)\n- UniFi provider IPv6 DHCP (not supported)\n\n**Cascade Effects:**\n- Unblocks infra-4ps (Route53 AAAA Records)\n- Informs infra-tvy (Ansible can use same derivation formula)\n- infra-gbn (No new tools needed - native HCL)","created_at":"2026-01-20T21:27:14Z"},{"id":8,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-gbn: External Tool Dependencies ✓\n\n**Core Questions Answered:**\n\n1. **Does UniFi Terraform provider support IPv6 DHCP reservations?**\n   - Answer: ❌ NO - filipowm/unifi v1.0.0 only supports `fixed_ip` (IPv4)\n   - No `ipv6_fixed_ip` or equivalent attribute exists\n   - Impact: Cannot manage IPv6 DHCP reservations via Terraform\n   - Workaround: Manual configuration in UniFi controller or rely on SLAAC\n\n2. **IPv6 support in community Ansible roles?**\n   - michaelrigart.interfaces v1.15.6: ✅ FULL IPv6 support via `ip6` parameter\n     * Supports address, prefix, gateway, routes, rules\n     * Works on ethernet, bond, bridge (Debian \u0026 RedHat)\n     * Example: `ip6: { address: 2600:4040:2ece:7500::74:0:41, prefix: 64, gateway: ... }`\n   - artis3n.tailscale v1.2.0: ✅ NATIVE IPv6 support\n     * Exposes `tailscale_node_ipv4` and `tailscale_node_ipv6` facts\n     * Already configured with IPv6 forwarding in site.yaml:171\n\n**Scope Impact:**\n- ✅ Infrastructure servers can proceed with IPv6 (Ansible + DNS)\n- ❌ UniFi DHCP IPv6 reservations out of scope (provider limitation)\n- Static IPv6 configuration via Ansible is the path forward\n\n**Files Investigated:**\n- `opentofu/main.tf` - Provider versions\n- `opentofu/unifi.tf` - DHCP reservations (IPv4 only)\n- `ansible/requirements.yaml` - Role dependencies\n- `ansible/site.yaml` - IPv6 forwarding config\n- michaelrigart.interfaces and artis3n.tailscale role documentation\n\n**Cascade Effects:**\n- Confirms: infra-tvy can use michaelrigart.interfaces `ip6` parameter\n- Informs: infra-1aa (no automated DHCP management, manual or SLAAC only)\n- Unblocks: Implementation can proceed with static IPv6 + AAAA records approach\n\n**Full research plan:** /Users/coneill/.claude/plans/glowing-jingling-glacier.md","created_at":"2026-01-21T00:09:48Z"},{"id":9,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-pof: Sysctl Configuration Strategy ✓\n\n**Core Findings:**\n\n1. **IP Forwarding Requirements (IPv4 AND IPv6 together):**\n   - **Enable forwarding:** k1-k4, infra1, flux, pantrypi (7 hosts)\n   - **No forwarding:** p1-p9, garagepi, luser, fs2 (Proxmox uses bridging)\n   - **Strategy:** Treat IPv4/IPv6 identically - if forwarding enabled, enable both protocols\n\n2. **Router Advertisement (accept_ra):**\n   - **Set accept_ra=0** for all hosts (static config, security)\n   - **Rationale:** Using static IPv6 via Ansible, prevent rogue RA attacks\n   - Explicit setting even on forwarding hosts (consistency vs. implicit)\n\n3. **Privacy Extensions:**\n   - **Set use_tempaddr=0** for all hosts (servers need stable addresses)\n   - **Rationale:** AAAA records require stable IPs, no privacy benefit for servers\n\n4. **Other IPv6 Sysctls:**\n   - accept_redirects=0 (security, matches IPv4)\n   - autoconf=0 (disable SLAAC, use static)\n\n5. **Implementation Strategy:**\n   - **Use existing `ip_forwarding_enabled` variable pattern**\n   - Add to kubernetes group_vars (covers k1-k4)\n   - Create pantrypi host_vars (Matter/Thread requirement)\n   - Consolidate all network sysctls in common role\n   - **Two new sysctl files:**\n     - `/etc/sysctl.d/99-network.conf` - Base IPv6 hardening (all hosts)\n     - `/etc/sysctl.d/99-forwarding.conf` - IPv4+IPv6 forwarding (conditional)\n   - **Cleanup old files manually before deployment:**\n     - Remove `/etc/sysctl.d/99-kubernetes.conf` (k1-k4)\n     - Remove `/etc/sysctl.d/99-kubernetes-cri.conf` (infra1, flux)\n   - **Remove from Ansible:**\n     - kubeadm role IPv4 forwarding task\n     - Tailscale site.yaml forwarding task + handler\n\n6. **Hosts Needing `ip_forwarding_enabled: true`:**\n   - infra1, flux: Already configured\n   - kubernetes group_vars: NEW (covers k1-k4)\n   - pantrypi: NEW (for Matter/Thread - infra-bre dependency)\n\n**Files to Modify:**\n- `ansible/roles/common/tasks/main.yaml` - Add network hardening + forwarding sysctls\n- `ansible/group_vars/kubernetes.yaml` - Add ip_forwarding_enabled\n- `ansible/host_vars/pantrypi.yaml` - Create with ip_forwarding_enabled\n- `ansible/roles/kubeadm/tasks/common.yaml` - Remove IPv4 forwarding\n- `ansible/site.yaml` - Remove Tailscale forwarding task + handler\n\n**Full research plan:** /Users/coneill/.claude/plans/binary-purring-candy.md","created_at":"2026-01-21T01:36:15Z"},{"id":10,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-tvy: Ansible Host-Level IPv6 Configuration ✓\n\n**Core Questions Answered:**\n\n1. **Configure at interface level or via sysctl?**\n   - Answer: Interface level using existing roles\n   - Proxmox: Modify our own interfaces.j2 template\n   - Other hosts: Use michaelrigart.interfaces ip6 parameter (already supported)\n   - Sysctl: Only for disabling SLAAC/autoconf (separate phase)\n\n2. **Variable strategy?**\n   - Answer: Derived IPv6 with global configuration\n   - Global vars in group_vars/all/ipv6.yaml: prefix + gateway\n   - Proxmox: Derive from bridge.address in template\n   - Others: Add host_ipv4 var, derive via Jinja2 in host_vars\n   - Formula: infrastructure_ipv6_prefix::74:last_octet\n\n3. **Dependency order?**\n   - Answer: Interleaved implementation and deployment\n   - Phase 0: Enable network management for Raspberry Pis (IPv4 only prep)\n   - Phase 1: Deploy sysctl to prevent SLAAC\n   - Phase 2: Proxmox hosts (one at a time, evacuate VMs first)\n   - Phase 3: Other hosts (cordon/drain k8s nodes individually)\n\n4. **Raspberry Pi strategy?**\n   - Answer: IN SCOPE (garagepi wifi, pantrypi ethernet)\n   - Enable manage_network: true in prep phase\n   - garagepi: wlan0 (wifi only), pantrypi: eth0\n   - pantrypi-wifi is just pantrypi's wifi interface, not separate host\n\n**Scope: 13 hosts total**\n- Proxmox: p1-p4, p9 (5 hosts)\n- Physical: infra1 (1 host)\n- VMs: luser, k1-k4 (5 hosts)\n- Raspberry Pis: garagepi, pantrypi (2 hosts)\n\n**Key Design Decisions:**\n- No galaxy role modifications needed\n- IPv6 gateway: 2600:4040:2ece:7500::1 (network-standard, not derived)\n- Both Ansible and Terraform have prefix (must update both if ISP changes)\n- Evacuate Proxmox VMs before network changes\n- Cordon/drain Kubernetes nodes individually\n\n**Files to modify:**\n- ansible/group_vars/all/ipv6.yaml - NEW: Global prefix + gateway\n- ansible/roles/proxmox/templates/interfaces.j2 - Add inet6 derivation\n- ansible/host_vars (8 files) - Add host_ipv4 + ip6\n- ansible/roles/common/tasks/main.yaml - Sysctl config (if not in infra-pof)\n\n**Full plan:** /Users/coneill/.claude/plans/moonlit-beaming-treasure.md","created_at":"2026-01-21T22:59:30Z"},{"id":11,"issue_id":"infra-3s7","author":"Clayton O'Neill","text":"## infra-1aa: Security and Backward Compatibility ✓\n\n**Core Questions Answered:**\n\n1. **IPv6-specific firewall rules at UniFi/network level?**\n   - Answer: IPv6 RA already enabled on network (other hosts using it)\n   - Verify existing configuration: prefix 2600:4040:2ece:7500::/64, gateway ::1\n   - Verify IPv6 firewall rules exist (or create based on IPv4 rules if restrictive)\n   - No Terraform changes needed (UniFi rules manual)\n\n2. **Does IPv6 change threat surface?**\n   - Answer: Minimal change for infrastructure servers on trusted LAN\n   - NDP attacks mitigated by trusted environment\n   - Sysctl hardening (accept_ra=0, autoconf=0, use_tempaddr=0) - planned in infra-pof\n   - No additional services exposed, same perimeter firewall protection\n\n3. **Dual-stack: can we enable IPv6 without breaking IPv4?**\n   - Answer: YES - static IPv6 + static IPv4 is safe\n   - Fail2ban already dual-stack ready (::1 in ignoreip)\n   - michaelrigart.interfaces supports both protocols simultaneously\n   - SSH, Tailscale, NFS already dual-stack capable\n   - No application-layer changes needed for infrastructure hosts\n\n4. **Hosts that can't support IPv6: how to exclude?**\n   - Answer: Already handled via enable_ipv6 field\n   - Excluded: p2-amt, p3-amt, p4-amt (enable_ipv6 = false)\n   - Included: 13 hosts (5 Proxmox, 4 K8s nodes, 2 VMs, 2 RPis)\n   - luser gets IPv6 but no AAAA record (public_dns=false)\n\n5. **Phased rollout vs. all-at-once deployment?**\n   - Answer: PHASED ROLLOUT by host tier\n   - Phase 1: Single test host (luser) - validate Ansible, connectivity\n   - Phase 2: Non-critical hosts (infra1, fs2, garagepi, pantrypi)\n   - Phase 3: Kubernetes nodes (k1-k4) - cordon/drain before changes\n   - Phase 4: Proxmox hosts (p1-p4, p9) - evacuate VMs, one at a time\n\n**External Security Validation Strategy:**\n\nTools identified for IPv6 scanning from internet:\n- nmap -6 from external IPv6 host\n- Online scanners: ipv6.chappell-family.com, subnetonline.com\n- test-ipv6.com for connectivity/reachability\n\nValidation approach:\n- Baseline scan of existing IPv6 hosts before infrastructure rollout\n- Per-phase external scans after each tier\n- Verify no unexpected ports open from WAN\n- Confirm ICMPv6 types 1, 2, 128, 129 allowed; 133-137 blocked from WAN\n- Test link-local addresses unreachable from WAN\n\n**Key Findings:**\n\n- **No Kubernetes app changes needed** (out of scope for infra-3s7)\n- **No IPv4-only assumptions in infrastructure layer** (SSH, Tailscale, NFS all dual-stack)\n- **UniFi IPv6 already operational** (verify, don't configure from scratch)\n- **Rollout risk manageable** with phased approach and per-host rollback\n- **External validation critical** before AAAA record creation\n\n**Cascade Effects:**\n- Unblocks: Implementation of infra-3s7 (all security research complete)\n- Informs: infra-ao8 (Testing and Validation) - phased strategy provides test framework\n- Dependencies: UniFi IPv6 verification before Phase 1\n\n**Full research plan:** /Users/coneill/.claude/plans/harmonic-swimming-ritchie.md","created_at":"2026-01-22T04:45:34Z"}]}
{"id":"infra-45y","title":"Evaluate PR #1137: Update Helm release crowdsec to v0.21.1","description":"Run the renovate-eval skill against PR #1137 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.10649-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.635981-05:00","closed_at":"2026-01-13T00:46:36.635981-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-45y","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.130227-05:00","created_by":"coneill"}]}
{"id":"infra-4bz","title":"Investigate why Docker digest PRs are not automerging","description":"Docker digest update PRs from Renovate that should have automerged are sitting open. Examples:\n- #1064: Update linuxserver/bazarr Docker digest\n- #1061: Update ghcr.io/autobrr/qui Docker digest\n\n## Questions to Investigate\n- What automerge config applies to digest updates?\n- Are required checks blocking automerge?\n- Is stability days requirement configured for digests?\n- Are branch protection rules interfering?","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T09:52:33.567472-05:00","created_by":"coneill","updated_at":"2026-01-06T08:45:46.824159-05:00","closed_at":"2026-01-06T08:45:46.824163-05:00"}
{"id":"infra-4jd","title":"Check artis3n.tailscale for deprecation fixes","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T17:54:11.045706-05:00","created_by":"coneill","updated_at":"2026-01-19T01:26:14.149992-05:00","closed_at":"2026-01-19T01:26:14.149992-05:00","close_reason":"Updated artis3n.tailscale to 1.2.0 - merged in PR #1190"}
{"id":"infra-4jg","title":"Remove all Vagrant remnants from codebase","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-18T10:43:49.660305-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-18T12:00:20.008014-05:00","closed_at":"2026-01-18T12:00:20.008014-05:00","close_reason":"PR #1186: Removed is_vagrant conditional from postfix role"}
{"id":"infra-4nj","title":"Evaluate PR #1150: Update linuxserver/radarr Docker tag to v6","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.506956-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-4o1","title":"Phase 3: Ansible IPv6 Configuration","description":"Read the full plan and all comments in infra-ec1, then implement Phase 3.","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:32:29.818904-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-29T13:17:59.800317-05:00","closed_at":"2026-01-29T13:17:59.800317-05:00","close_reason":"IPv6 config added to all 12 hosts. Commit 38b0a518 on ipv6-phase3-ansible-config branch.","dependencies":[{"issue_id":"infra-4o1","depends_on_id":"infra-dfe","type":"blocks","created_at":"2026-01-22T23:32:41.350144-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-4ps","title":"Route53 AAAA Record Management","description":"**Core Questions:**\n- Create AAAA records for all hosts, or only those with `public_dns=true`?\n- TTL strategy: match A record TTLs (300s) or different?\n- Validation: how to prevent conflicts with manually-configured IPv6?\n- CNAMEs (e.g., `nut.oneill.net → infra1.oneill.net`): automatic AAAA support?\n\n**Investigation Needed:**\n- Review Route53 AAAA record requirements\n- Check if Route53 validates IPv6 format\n- Understand DNS failover/health checks with IPv6\n\n**Critical Files:**\n- `opentofu/modules/dns/oneill.tf` (lines 141-149, 212-266) - A and AAAA patterns\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:11.674424-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.464411-05:00","closed_at":"2026-01-22T23:32:54.464411-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-4wk","title":"Evaluate PR #1141: Update traefik Docker tag to v3.6.5","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.172762-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-54d","title":"Generic Docker image build and push pipeline","description":"Build infrastructure for creating and publishing Docker images from Dockerfiles\n\n## Requirements\n- Build all changed images on each PR (CI validation)\n- Push to `latest` tag after merge to main\n- Local builds can push to `dev` tag for testing\n- Push to GHCR (ghcr.io/claytono/...)\n\n## Use Cases\n- ansible-idempotency-test image (immediate need)\n- Future containerized tools from this repo","status":"tombstone","priority":2,"issue_type":"epic","created_at":"2026-01-07T22:39:14.749069-05:00","created_by":"coneill","updated_at":"2026-01-08T09:39:14.400289-05:00","comments":[{"id":5,"issue_id":"infra-54d","author":"coneill","text":"Implementation started - all files created, pending local testing:\n- docker/semaphore/Dockerfile\n- docker/semaphore/entrypoint.sh  \n- docker/semaphore/ansible-wrapper.sh (copied from k8s)\n- docker/semaphore/setup-ansible-bins.sh\n- docker/semaphore/smoke-test.sh\n- .github/workflows/docker-build.yaml\n- scripts/docker-build\n\nNext step: Test locally with scripts/docker-build semaphore","created_at":"2026-01-08T05:30:22Z"}],"deleted_at":"2026-01-08T09:39:14.400289-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"epic"}
{"id":"infra-56d","title":"Exclude resticprofile kubernetes manifests from Renovate kubernetes manager","description":"Renovate's kubernetes manager is finding Docker images in generated resticprofile manifests and creating PRs for them. These manifests should only be managed via kustomize.\n\nAdd resticprofile kubernetes path to the kubernetes manager's ignorePaths in .renovaterc.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T15:49:17.126558-05:00","created_by":"coneill","updated_at":"2026-01-06T08:45:55.620634-05:00","closed_at":"2026-01-06T08:45:55.620651-05:00"}
{"id":"infra-578","title":"Update zigbee2mqtt role","description":"Update zigbee2mqtt role to use comment annotation format (keep as template, has other variables).\n\n## Replace `ansible/roles/zigbee2mqtt/defaults/main.yaml`:\n\n```yaml\n---\n# renovate: datasource=docker\nzigbee2mqtt_image: \"koenkk/zigbee2mqtt:2.7.1@sha256:163e7351430a95d550d5b1bb958527edc1eff115eb013ca627f3545a192e853f\"\nzigbee2mqtt_data_dir: /etc/zigbee2mqtt\nzigbee2mqtt_device: /dev/serial/by-id/usb-Nabu_Casa_SkyConnect_v1.0_621e38fda096ed118be4c498a7669f5d-if00-port0\nzigbee2mqtt_hostname: zigbee2mqtt.oneill.net\n```\n\n## Update `ansible/roles/zigbee2mqtt/templates/docker-compose.yaml.j2` (line 4):\n\nChange:\n```yaml\n    image: koenkk/zigbee2mqtt:{{ zigbee2mqtt_image_tag }}\n```\n\nTo:\n```yaml\n    image: {{ zigbee2mqtt_image }}\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:35:53.809733-05:00","created_by":"coneill","updated_at":"2026-01-06T12:19:09.485022-05:00","closed_at":"2026-01-06T12:19:09.485022-05:00","close_reason":"Closed"}
{"id":"infra-59o","title":"Research Vultr IPv6 assignment and DNS integration for flux","description":"## Context\n\nflux is a Vultr VPS (not in homelab) that needs IPv6 enabled before external IPv6 testing can proceed for infra-3s7.\n\n**Current State:**\n- flux is a Vultr VPS instance\n- Manages DNS via Route53 (ddns-route53 updates A record)\n- Need to understand Vultr IPv6 provisioning and integrate with existing DNS automation\n\n**Core Questions:**\n1. How is IPv6 assigned to Vultr instances? (SLAAC, DHCPv6, static via control panel?)\n2. Does Vultr provide stable /64 or /128 addresses?\n3. How to query IPv6 address from within the instance?\n4. Integration with ddns-route53: does it already support AAAA records?\n5. If not, what changes needed to ddns-route53 configuration?\n\n**Investigation Needed:**\n- Review Vultr documentation for IPv6 provisioning\n- Check ddns-route53 codebase/configuration for IPv6 support\n- Understand how flux instance is provisioned (Terraform? Manual?)\n- Determine if IPv6 is already enabled but not configured in DNS\n\n**Success Criteria:**\n- Document how to enable IPv6 on flux Vultr instance\n- Document how to integrate IPv6 address with Route53 AAAA record\n- Provide implementation steps for flux IPv6 enablement\n\n**Blocks:** infra-ao8 Phase 0 (external IPv6 testing requires flux IPv6)","status":"closed","priority":2,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T12:31:44.437528-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T18:49:36.038377-05:00","closed_at":"2026-01-22T18:49:36.038377-05:00","close_reason":"Closed"}
{"id":"infra-59z","title":"Create separate PR for ARA callback plugin","description":"Create focused PR with just ansible/callback_plugins/ara_url.py. Branch from main, copy plugin from semaphore-auto-deploy branch, push and merge.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T01:17:29.29643-05:00","created_by":"coneill","updated_at":"2026-01-04T02:06:54.889703-05:00","closed_at":"2026-01-04T02:06:54.889703-05:00","close_reason":"PR #1049 merged - ARA callback plugin added to main","dependencies":[{"issue_id":"infra-59z","depends_on_id":"infra-6cq","type":"parent-child","created_at":"2026-01-04T01:18:11.996248-05:00","created_by":"coneill"}]}
{"id":"infra-5i0","title":"Create ansible-idempotency-test script","description":"Create scripts/ansible-idempotency-test that:\n- Imports SemaphoreDeployer from lib.semaphore\n- Uses ara.clients.http.AraHttpClient to query per-host changed/failed counts\n- Implements check_idempotency() returning per-host status dict\n- Implements report_healthcheck() POSTing to healthchecks.io with status and ARA/Semaphore links\n- Retry logic: up to 3 attempts, stop early when all hosts idempotent (0 changed, 0 failed)","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-07T08:39:03.912096-05:00","created_by":"coneill","updated_at":"2026-01-07T08:39:24.915251-05:00","deleted_at":"2026-01-07T08:39:24.915251-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-5kq","title":"Test 2: Automatic Trigger - Single Host Change","description":"Test smart detection targets only the changed host.\n\n## Steps\n1. Create branch: `git checkout -b test-auto-deploy origin/main`\n2. Add harmless change to `ansible/host_vars/k1.yaml`:\n   ```yaml\n   # Test comment for auto-deploy verification\n   ```\n3. Commit, push, create PR, merge to main\n\n## Verification Checklist\n- [ ] Workflow triggers automatically on push to main\n- [ ] ansible-detect-changes outputs `hosts=k1`\n- [ ] semaphore-deploy runs with `--limit k1`\n- [ ] Task completes successfully\n- [ ] ARA URL appears\n- [ ] Verify via ARA that the deploy ran on k1 as expected","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T22:21:17.818035-05:00","created_by":"coneill","updated_at":"2026-01-05T00:59:45.826214-05:00","closed_at":"2026-01-05T00:59:45.826214-05:00","close_reason":"Test 2 passed - run 20706466178 correctly detected k1 host change, deployed with --limit k1, task 126 succeeded","dependencies":[{"issue_id":"infra-5kq","depends_on_id":"infra-oty","type":"blocks","created_at":"2026-01-04T22:21:29.946122-05:00","created_by":"coneill"}]}
{"id":"infra-5ln","title":"Evaluate PR #1149: Update mariadb Docker tag to v11.8","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.579032-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-5rr","title":"Deploy Shelfmark book downloader","description":"Deploy Shelfmark (calibrain/shelfmark) as replacement for deleted ephemera.\n\n**Context:**\n- Ephemera repo/package deleted - 'rage quit' per Reddit\n- Shelfmark is the community-recommended replacement\n- Repo: https://github.com/calibrain/shelfmark\n\n**Features:**\n- Full Anna's Archive support\n- Torrent/Usenet/IRC via Prowlarr\n- Audiobook support\n- Works with Calibre/CWA/Booklore ingest folders","status":"open","priority":2,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-19T10:33:12.834695-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-19T10:33:12.834695-05:00"}
{"id":"infra-5va","title":"Add Renovate management for flake.nix hardcoded versions","description":"Renovate should manage hardcoded versions in flake.nix including: unifi-network-mcp (0.2.0), ara (1.7.3), mcp-cli (0.1.4). Investigate using custom managers or regex managers to update these versions automatically.","status":"open","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-23T00:54:50.702619-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-23T00:54:50.702619-05:00"}
{"id":"infra-5wo","title":"Evaluate PR #1159: Update peter-evans/create-pull-request action to v8","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T18:49:24.729971-05:00","created_by":"coneill","updated_at":"2026-01-12T18:57:29.503456-05:00","closed_at":"2026-01-12T18:57:29.503456-05:00","close_reason":"PR #1159 merged","dependencies":[{"issue_id":"infra-5wo","depends_on_id":"infra-5z4","type":"blocks","created_at":"2026-01-12T18:53:43.745747-05:00","created_by":"coneill"}]}
{"id":"infra-5x8","title":"Auto-merge Ansible Docker image digest updates","description":"Add automerge rule for digest updates on Ansible-managed Docker images.\n\n## Current State\nRenovate manages these Docker images in Ansible roles via custom.regex:\n- `hertzg/rtl_433` (rtl433 role)\n- `zwavejs/zwave-js-ui` (zwavejs role)  \n- `koenkk/zigbee2mqtt` (zigbee2mqtt role)\n\n## Desired Behavior\nDigest-only updates (same tag, new sha256) should auto-merge like Kubernetes images do.\n\n## Implementation\nAdd a packageRule to .renovaterc:\n```json\n{\n  \"matchDatasources\": [\"docker\"],\n  \"matchManagers\": [\"custom.regex\"],\n  \"matchFileNames\": [\"ansible/roles/*/defaults/main.yaml\"],\n  \"matchUpdateTypes\": [\"digest\"],\n  \"minimumReleaseAge\": \"0 days\",\n  \"automerge\": true,\n  \"automergeType\": \"pr\",\n  \"addLabels\": [\"automerge\"],\n  \"description\": \"Auto-merge Ansible Docker image digest updates if CI passes\"\n}\n```\n\n## References\nSimilar to existing kustomize digest rule (lines 210-223) and CNPG digest rule (lines 224-244).","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T15:19:23.618153-05:00","created_by":"coneill","updated_at":"2026-01-06T11:28:40.385159-05:00","closed_at":"2026-01-06T11:28:40.385163-05:00"}
{"id":"infra-5yv","title":"Evaluate PR #1136: Update Helm release authentik to v2025.10.3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.536058-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-5z4","title":"Evaluate Renovate PRs","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-12T09:05:21.22066-05:00","created_by":"coneill","updated_at":"2026-01-13T00:45:47.049017-05:00","closed_at":"2026-01-13T00:45:47.049017-05:00","close_reason":"User decided to work on something else"}
{"id":"infra-5z4.1","title":"Evaluate PR #1165: Update debian:trixie-slim Docker digest to ef03cc5","description":"Run the renovate-eval skill against PR #1165 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T00:44:29.776016-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.630312-05:00","closed_at":"2026-01-13T00:46:36.630312-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-5z4.1","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-13T00:44:29.780679-05:00","created_by":"coneill"}]}
{"id":"infra-61v","title":"Refactor Semaphore to use ConfigMap-mounted flake","description":"## Goal\nEliminate GitHub runtime dependency by mounting flake.nix and scripts via ConfigMap instead of git clone.\n\n## Changes\n\n### flake.nix\nAdd `semaphore` devShell with pythonEnv + opentofu\n\n### kubernetes/semaphore/scripts/\n- `setup-nix.sh` - NEW: Install nix, set up PATH\n- `init-tools.sh` - REFACTORED: Use `nix print-dev-env .#semaphore` instead of git clone + direnv\n- `ansible-wrapper.sh` - MOVED from parent directory\n\n### kubernetes/semaphore/render\nCopy flake.nix and flake.lock to semaphore directory with \"DO NOT EDIT\" header\n\n### kubernetes/semaphore/kustomization.yaml\nAdd configMapGenerator for `semaphore-infra` ConfigMap with all scripts\n\n### kubernetes/semaphore/patch-deployment.yaml\n- Mount ConfigMap at `/infra`\n- Add emptyDir for `/nix`\n\n## Verification\n1. Run render script\n2. Deploy with kubectl apply -k\n3. Check pod logs for successful startup\n4. Test a Semaphore deployment job works","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T11:41:01.580025-05:00","created_by":"coneill","updated_at":"2026-01-08T21:42:43.750534-05:00","closed_at":"2026-01-08T21:42:43.750534-05:00","close_reason":"Merged PRs #1105 and #1106 - Semaphore now uses ConfigMap-mounted flake instead of git clone"}
{"id":"infra-620","title":"Evaluate PR #1148: Update Helm release velero to v11.3.1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.650324-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-66l","title":"IPv6 Address Assignment Method","description":"**Core Questions:**\n- Static IPv6 (derived from IPv4) vs. SLAAC (auto-configuration)?\n- If SLAAC: will addresses be stable, or randomized by privacy extensions?\n- How to disable privacy extensions: globally or per-interface?\n- DNS strategy: static AAAA records or dynamic?\n- Kubernetes IPv6 requirements: does kubeadm need specific configuration?\n\n**Investigation Needed:**\n- Research \"stable IPv6\" requirement: static config or SLAAC without privacy?\n- Check Kubernetes documentation for IPv6 address requirements\n- Examine UDMP DHCPv6 capabilities\n- Test whether systemd-networkd can use SLAAC reliably\n\n**Why It Matters:**\nThis cascades to Ansible configuration, DNS management, and sysctl settings. If using SLAAC, Terraform-derived addresses may be unnecessary.\n\n**Critical Files:**\n- `ansible/roles/kubeadm/` - Kubernetes network configuration\n- Current network interface configs in `ansible/host_vars/`\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:06.692655-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.466653-05:00","closed_at":"2026-01-22T23:32:54.466653-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-6cq","title":"Enable ARA URL Reporting in Semaphore","description":"Deploy callback plugin to enable ARA URL reporting. Infrastructure already exists, just need to merge callback plugin.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-04T01:17:23.425578-05:00","created_by":"coneill","updated_at":"2026-01-04T02:06:55.132645-05:00","closed_at":"2026-01-04T02:06:55.132645-05:00","close_reason":"ARA URL reporting enabled - callback plugin merged to main"}
{"id":"infra-6fm","title":"Fix INJECT_FACTS_AS_VARS in group_vars/all/docker.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:52:49.46681-05:00","created_by":"coneill","updated_at":"2026-01-13T19:24:55.621817-05:00","closed_at":"2026-01-13T19:24:55.621817-05:00","close_reason":"Fixed: changed ansible_* to ansible_facts['*'] syntax"}
{"id":"infra-6jw","title":"Evaluate PR #1135: Update dependency esphome to v2025.12.2","description":"Run the renovate-eval skill against PR #1135 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.250849-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:34.007167-05:00","closed_at":"2026-01-12T18:48:34.007167-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-6jw","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.126702-05:00","created_by":"coneill"}]}
{"id":"infra-6m4","title":"Add automerge rule (follow-up PR)","description":"**Note:** This is a SEPARATE PR, created after confirming Renovate properly detects all 8 images.\n\nAfter merging the main PR and validating that Renovate detects all images correctly, add this automerge rule to `.renovaterc` in the packageRules array:\n\n```json\n{\n  \"matchDatasources\": [\"docker\"],\n  \"matchManagers\": [\"custom.regex\"],\n  \"matchFileNames\": [\"ansible/**\"],\n  \"matchUpdateTypes\": [\"digest\"],\n  \"minimumReleaseAge\": \"0 days\",\n  \"automerge\": true,\n  \"automergeType\": \"pr\",\n  \"addLabels\": [\"automerge\"],\n  \"description\": \"Auto-merge Docker digest updates in Ansible if CI passes\"\n}\n```\n\n## Testing before creating this PR:\n\n1. Check Mend Renovate dashboard for detected dependencies\n2. Verify all 8 images appear with correct depName\n3. Confirm images without digests are also detected","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:44:23.977423-05:00","created_by":"coneill","updated_at":"2026-01-06T18:54:15.130882-05:00","closed_at":"2026-01-06T18:54:15.130882-05:00","close_reason":"Completed - recursive strategy fix merged, automerge rule in PR #1086","dependencies":[{"issue_id":"infra-6m4","depends_on_id":"infra-cza","type":"blocks","created_at":"2026-01-06T11:46:30.102534-05:00","created_by":"coneill"}]}
{"id":"infra-6o5","title":"Update kustomization to include Tailscale ingress","description":"**File:** /Users/coneill/src/infra2/kubernetes/semaphore/kustomization.yaml\n\n**Problem:** The new Tailscale ingress file needs to be added to the kustomization resources list.\n\n**Change Required:**\n\nAdd `semaphore-tailscale-ingress.yaml` to the resources list after `helm/serviceaccount.yaml` (line 20).\n\nCurrent resources block ends at line 20:\n```yaml\nresources:\n- namespace.yaml\n- semaphore-postgres-cluster.yaml\n- externalsecret.yaml\n- externalsecret-ansible-vault.yaml\n- externalsecret-ara.yaml\n- externalsecret-encryption.yaml\n- externalsecret-oidc.yaml\n- helm/configmap.yaml\n- helm/deployment.yaml\n- helm/ingress.yaml\n- helm/pvc.yaml\n- helm/service.yaml\n- helm/serviceaccount.yaml\n```\n\nAdd this line after `helm/serviceaccount.yaml`:\n```yaml\n- semaphore-tailscale-ingress.yaml\n```\n\n**Context:** Required to include the new Tailscale ingress in the Kubernetes deployment.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:49:25.768344-05:00","created_by":"coneill","updated_at":"2026-01-04T12:58:54.112849-05:00","closed_at":"2026-01-04T12:58:54.112852-05:00","dependencies":[{"issue_id":"infra-6o5","depends_on_id":"infra-ugw","type":"blocks","created_at":"2026-01-04T12:54:54.136211-05:00","created_by":"coneill"}]}
{"id":"infra-6ue","title":"Evaluate PR #1158: Update actions/setup-python action to v6","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T18:49:41.406144-05:00","created_by":"coneill","updated_at":"2026-01-12T19:23:08.004146-05:00","closed_at":"2026-01-12T19:23:08.004146-05:00","close_reason":"PR #1158 merged","dependencies":[{"issue_id":"infra-6ue","depends_on_id":"infra-5z4","type":"blocks","created_at":"2026-01-12T18:53:43.813309-05:00","created_by":"coneill"}]}
{"id":"infra-6wn","title":"Test 5: Manual with Tags","description":"Test custom tags parameter.\n\n## Steps\n1. Run workflow_dispatch with:\n   - `hosts: all`\n   - `tags: hostname,timezone`\n\n## Commands\n```bash\ngh workflow run \"Deploy Ansible to Production\" -f hosts=all -f tags=hostname,timezone\n```\n\n## Verification Checklist\n- [ ] semaphore-deploy passes `--tags hostname,timezone`\n- [ ] Only those tags run\n- [ ] ARA URL appears\n- [ ] Verify via ARA that only the hostname and timezone tags executed","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-04T22:28:44.339533-05:00","created_by":"coneill","updated_at":"2026-01-04T22:29:46.436731-05:00","deleted_at":"2026-01-04T22:29:46.436731-05:00","deleted_by":"daemon","delete_reason":"delete","original_type":"task"}
{"id":"infra-72l","title":"Undo Tailscale client version pin","description":"Remove the Tailscale client version pin that was previously set. Allow Tailscale to auto-update or use the default package version.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-07T09:43:17.741503-05:00","created_by":"coneill","updated_at":"2026-01-11T11:15:11.727775-05:00","closed_at":"2026-01-10T12:31:57.669641-05:00"}
{"id":"infra-77h","title":"Phase 4: Phased Deployment \u0026 External Validation","description":"Read the full plan and all comments in infra-ec1, then implement Phase 4.","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:32:29.92471-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-29T15:45:04.909111-05:00","closed_at":"2026-01-29T15:45:04.909111-05:00","close_reason":"All 12 hosts deployed and validated. External firewall confirmed blocking. PR #1315 created.","dependencies":[{"issue_id":"infra-77h","depends_on_id":"infra-4o1","type":"blocks","created_at":"2026-01-22T23:32:41.431362-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-7bj","title":"Deploy podcast transcription stack (PinePods + Speakr + WhisperX)","description":"## Overview\n\nSelf-hosted podcast downloading and transcription system with searchable transcripts, speaker diarization, and AI summarization.\n\n## Requirements\n\n- Import OPML file for podcast subscriptions\n- Automatically download new podcast episodes\n- Transcribe episodes with GPU acceleration\n- Speaker identification (diarization)\n- Searchable transcripts with semantic search\n- AI summarization and chat with transcripts\n- Chapterization (nice to have)\n\n## Architecture\n\n```\n┌─────────────┐    ┌──────────────┐    ┌─────────────┐\n│  PinePods   │───▶│   Watcher    │───▶│   Speakr    │\n│             │    │  (sidecar)   │    │             │\n│ /downloads  │    │  inotify +   │    │ /api/upload │\n│             │    │  curl POST   │    │             │\n└─────────────┘    └──────────────┘    └─────────────┘\n      ▲                                       │\n      │                               ┌───────▼───────┐\n   OPML                               │ WhisperX ASR  │\n   import                             │   (GPU)       │\n                                      └───────────────┘\n```\n\n## Components\n\n### 1. PinePods (Podcast Manager)\n- **GitHub**: https://github.com/madeofpendletonwool/PinePods\n- **Purpose**: OPML import, auto-download episodes\n- **Storage**: `/opt/pinepods/downloads` (flat structure for server downloads)\n- **Features**: ntfy/Gotify notifications, Kubernetes Helm charts, GPodder API\n- **Stars**: 773, actively maintained\n\n### 2. Speakr (Transcription + Search)\n- **GitHub**: https://github.com/murtaza-nasir/speakr\n- **Purpose**: Transcription, search, summarization, AI chat\n- **API**: REST v1 with Swagger at `/api/v1/docs`\n- **Upload endpoint**: `POST /api/v1/upload`\n- **Features**:\n  - Semantic search (\"Inquire Mode\")\n  - Speaker diarization\n  - Voice profiles (with WhisperX)\n  - AI chat with transcripts\n  - Smart tagging with AI prompts\n  - Auto-export to Obsidian/Logseq\n  - OIDC SSO support\n- **Stars**: 2.7k, actively maintained\n- **No chapter detection** - would need separate tool\n\n### 3. WhisperX ASR (GPU Transcription)\n- **GitHub**: https://github.com/murtaza-nasir/whisperx-asr-service\n- **Purpose**: GPU-accelerated transcription with diarization\n- **Requires**: NVIDIA GPU, HuggingFace token for pyannote models\n- **Same author as Speakr** - designed to work together\n\n### 4. File Watcher (Custom Sidecar)\n- Simple inotifywait + curl container\n- Watches PinePods download directory\n- POSTs new audio files to Speakr API\n- Could also use PinePods ntfy integration → n8n workflow\n\n## Transcription Backend Options\n\n| Backend | Diarization | Voice Profiles | GPU Required |\n|---------|-------------|----------------|--------------|\n| WhisperX ASR (recommended) | Best quality | ✅ | Yes |\n| OpenAI gpt-4o-transcribe-diarize | ✅ | ❌ | No |\n| Legacy OpenAI Whisper | ❌ | ❌ | No |\n\n## Summarization Options\n\nSpeakr supports multiple backends via `TEXT_MODEL_API_KEY`:\n- **OpenRouter** (recommended for Claude access)\n- **OpenAI** (GPT-4)\n- **Ollama** (fully self-hosted)\n\nClaude API has 200K token context - handles full podcast transcripts without chunking.\n\n## For Chapters (Optional)\n\nSpeakr doesn't have chapter detection. Options:\n- **chapterize-whisper**: https://github.com/jeeftor/chapterize-whisper (CLI tool)\n- **Claude API post-processing**: Generate chapter summaries from transcript\n- **AssemblyAI**: `auto_chapters=True` if willing to use external API\n\n## Alternatives Considered\n\n| Tool | Status | Notes |\n|------|--------|-------|\n| Audiobookshelf | ✅ Active | Overkill - audiobook-first, not podcast-focused |\n| Podgrab | ❌ Dead | Last commit 2022, avoid |\n| PodFetch | ✅ Active | Lighter than PinePods, no ntfy integration |\n| Scriberr | ✅ Active | General transcription, less search-focused than Speakr |\n\n## Implementation Tasks\n\n1. [ ] Deploy WhisperX ASR with GPU passthrough\n2. [ ] Deploy Speakr with WhisperX backend\n3. [ ] Deploy PinePods with shared volume\n4. [ ] Create file watcher sidecar container\n5. [ ] Configure Speakr summarization (OpenRouter/Claude)\n6. [ ] Import OPML and test end-to-end flow\n7. [ ] Optional: Add chapterization pipeline\n\n## References\n\n- Speakr docs: https://murtaza-nasir.github.io/speakr/\n- Speakr API: https://murtaza-nasir.github.io/speakr/user-guide/api-reference/\n- PinePods docs: https://www.pinepods.online/docs/intro\n- WhisperX: https://github.com/m-bain/whisperX","status":"in_progress","priority":3,"issue_type":"feature","owner":"clayton@oneill.net","created_at":"2026-01-22T18:32:06.712289-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-25T21:24:35.935428-05:00","comments":[{"id":13,"issue_id":"infra-7bj","author":"Clayton O'Neill","text":"## Status Update\n\n### Completed\n- ✅ Speakr deployed with WhisperX ASR backend (GPU)\n- ✅ PostgreSQL via CloudNativePG\n- ✅ Authentik SSO integration (password login disabled)\n- ✅ Semantic search enabled (Inquire Mode)\n- ✅ gpt-5-mini for summarization\n- ✅ PR #1265 created for Speakr deployment\n\n### In Progress\n- 🔄 Planning PinePods deployment\n\n### Remaining\n- [ ] Deploy PinePods with Helm chart\n- [ ] Create file watcher sidecar container\n- [ ] Configure shared volume between PinePods and watcher\n- [ ] Test end-to-end flow (podcast → download → transcription)\n- [ ] Import OPML and validate","created_at":"2026-01-26T02:24:45Z"}]}
{"id":"infra-7c9","title":"Annotate static docker-compose files","description":"Add `# renovate: datasource=docker` comment before each image line in static docker-compose files.\n\n## `ansible/roles/traefik/files/docker-compose.yml` (before image line):\n\n```yaml\n  traefik:\n    # renovate: datasource=docker\n    image: traefik:v3.6.4@sha256:c5bd185c41ba3dbb42cf8a1b9fbdc368bdc96f90c8e598134879935f64e7a7f1\n```\n\n## `ansible/roles/os-install/files/docker-compose.yaml` (3 images):\n\n```yaml\n  dnsmasq:\n    # renovate: datasource=docker\n    image: dockurr/dnsmasq:2.91@sha256:...\n  nginx:\n    # renovate: datasource=docker\n    image: nginx:1.29-alpine@sha256:...\n  proxmox-answer:\n    # renovate: datasource=docker\n    image: slothcroissant/proxmox-auto-installer-server:0.3.2@sha256:...\n```\n\n## `ansible/roles/nut/files/docker-compose.yaml` (before image line):\n\n```yaml\n  nut_webgui:\n    # renovate: datasource=docker\n    image: ghcr.io/superioone/nut_webgui:0.7.1@sha256:...\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:42:05.86562-05:00","created_by":"coneill","updated_at":"2026-01-06T12:19:09.488941-05:00","closed_at":"2026-01-06T12:19:09.488941-05:00","close_reason":"Closed"}
{"id":"infra-7jv","title":"Verify Renovate manages nix-installer version","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-08T00:34:53.927357-05:00","created_by":"coneill","updated_at":"2026-01-08T09:39:14.400289-05:00","deleted_at":"2026-01-08T09:39:14.400289-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-7qf","title":"Evaluate PR #1143: Update docker.io/getmeili/meilisearch Docker tag to v1.31.0","description":"Run the renovate-eval skill against PR #1143 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.661512-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.644893-05:00","closed_at":"2026-01-13T00:46:36.644893-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-7qf","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.14078-05:00","created_by":"coneill"}]}
{"id":"infra-8ar","title":"Evaluate PR #1143: Update docker.io/getmeili/meilisearch Docker tag to v1.31.0","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.030718-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-8eq","title":"Daily Ansible idempotency smoke test","description":"Implement a daily automated test to verify Ansible playbooks are idempotent.\n\n## Requirements\n- Run daily via Kubernetes CronJob\n- Trigger Semaphore deploy to all hosts\n- Run 2-3 attempts, stop when all hosts have no changed/failed tasks\n- Use ARA to determine success/failure (0 changed, 0 failed = idempotent)\n- Report to healthchecks.io with:\n  - Success/failure status\n  - Links to ARA playbook runs\n  - Links to Semaphore task runs","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-05T10:06:53.790357-05:00","created_by":"coneill","updated_at":"2026-01-09T16:13:03.9088-05:00","closed_at":"2026-01-09T16:13:03.9088-05:00","close_reason":"Epic complete - daily idempotency test CronJob deployed and operational","dependencies":[{"issue_id":"infra-8eq","depends_on_id":"infra-yks","type":"blocks","created_at":"2026-01-07T08:44:49.962053-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-a0g","type":"blocks","created_at":"2026-01-07T08:44:50.020508-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-3bq","type":"blocks","created_at":"2026-01-07T08:44:50.074904-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-a0y","type":"blocks","created_at":"2026-01-07T08:44:50.126671-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-d1y","type":"blocks","created_at":"2026-01-07T08:44:50.175998-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-08x","type":"blocks","created_at":"2026-01-07T08:44:50.233417-05:00","created_by":"coneill"},{"issue_id":"infra-8eq","depends_on_id":"infra-61v","type":"blocks","created_at":"2026-01-08T11:41:57.772222-05:00","created_by":"coneill"}]}
{"id":"infra-8o0","title":"Evaluate PR #1150: Update linuxserver/radarr Docker tag to v6","description":"Run the renovate-eval skill against PR #1150 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.133042-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:34.011115-05:00","closed_at":"2026-01-12T18:48:34.011115-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-8o0","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.154389-05:00","created_by":"coneill"}]}
{"id":"infra-8sx","title":"VLAN/Subnet Identification Strategy","description":"**Core Questions:**\n- Should VLAN be explicit in `infrastructure_hosts` or derived from third octet?\n- Is there 1:1 mapping between third octet and IPv6 prefix, or exceptions?\n- How to handle ESPHome spanning multiple subnets (172.20.4-7.x)?\n- Lookup table vs. inline derivation logic?\n\n**Investigation Needed:**\n- Map ALL current ESPHome IPs (full 172.20.0.0/22 range)\n- Verify no hosts exist in 172.19.75.x (Servers VLAN) or 172.19.77.x (Guest VLAN)\n- Document edge cases where subnet doesn't cleanly map to VLAN\n\n**Critical Files:**\n- `opentofu/esphome-hosts.tf` - auto-generated ESPHome hosts\n- `.beads/issues/infra-3s7.yaml` - VLAN mapping table\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:04.722996-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.468741-05:00","closed_at":"2026-01-22T23:32:54.468741-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-8uf","title":"Evaluate PR #1142: Update dependency community.proxmox to v1.5.0","description":"Run the renovate-eval skill against PR #1142 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.738594-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.643436-05:00","closed_at":"2026-01-13T00:46:36.643436-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-8uf","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.138888-05:00","created_by":"coneill"}]}
{"id":"infra-93h","title":"Proxmox loses console output after boot - only flashing cursor","description":"When Proxmox (p1-p4) boots, the console output disappears after the kernel starts and only shows a flashing cursor. This makes it difficult to troubleshoot boot issues via IPMI/console. Likely related to framebuffer/console configuration or kernel boot parameters. Need to investigate console= kernel parameters and ensure text mode console is preserved.","status":"open","priority":3,"issue_type":"bug","owner":"clayton@oneill.net","created_at":"2026-01-17T14:41:47.785569-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-17T14:41:47.785569-05:00"}
{"id":"infra-9d0","title":"Evaluate PR #1145: Update Helm release external-secrets to v1.2.0","description":"Run the renovate-eval skill against PR #1145 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.513679-05:00","created_by":"coneill","updated_at":"2026-01-13T00:43:51.864674-05:00","closed_at":"2026-01-13T00:43:51.864674-05:00","close_reason":"PR merged","dependencies":[{"issue_id":"infra-9d0","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.144386-05:00","created_by":"coneill"}]}
{"id":"infra-9fp","title":"Evaluate PR #1151: Update mariadb Docker tag to v12","description":"Run the renovate-eval skill against PR #1151 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.05614-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:34.012749-05:00","closed_at":"2026-01-12T18:48:34.012749-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-9fp","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.156315-05:00","created_by":"coneill"}]}
{"id":"infra-9uv","title":"Add Generic Regex Manager to .renovaterc","description":"Add a single generic regex manager for Docker images using comment annotations.\n\n## File: `.renovaterc`\n\nAdd this manager to the customManagers array:\n\n```json\n{\n  \"customType\": \"regex\",\n  \"description\": \"Docker images with comment annotations (generic)\",\n  \"managerFilePatterns\": [\n    \"**/*.yaml\",\n    \"**/*.yml\"\n  ],\n  \"matchStrings\": [\n    \"#\\\\s*renovate:\\\\s*datasource=docker\\\\s*\\\\n(?\u003cprefix\u003e\\\\s*[a-z_]+:\\\\s*\\\"?)(?\u003cdepName\u003e[^:\\\"\\\\s]+):(?\u003ccurrentValue\u003e[^@\\\"\\\\s]+)(?:@(?\u003ccurrentDigest\u003esha256:[a-f0-9]{64}))?(?\u003csuffix\u003e\\\"?)\"\n  ],\n  \"datasourceTemplate\": \"docker\",\n  \"autoReplaceStringTemplate\": \"# renovate: datasource=docker\\n{{{prefix}}}{{{depName}}}:{{{newValue}}}{{#if newDigest}}@{{{newDigest}}}{{/if}}{{{suffix}}}\"\n}\n```\n\nThis manager:\n- Matches any YAML file with `# renovate: datasource=docker` comment\n- Extracts depName from the actual image value\n- Handles images with or without digest\n- Works repo-wide (matches files containing the comment annotation)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:28:55.157332-05:00","created_by":"coneill","updated_at":"2026-01-06T12:10:27.40835-05:00","closed_at":"2026-01-06T12:10:27.40835-05:00","close_reason":"Closed"}
{"id":"infra-a0g","title":"Create ansible-idempotency-test script","description":"## File\nCreate: `scripts/ansible-idempotency-test`\n\n## Script Structure\n```python\n#!/usr/bin/env python3\n\"\"\"\nansible-idempotency-test — Verify Ansible playbooks are idempotent via ARA\n\nTriggers Semaphore deploy, checks ARA for 0 changed/0 failed per host,\nretries until idempotent, reports to healthchecks.io.\n\"\"\"\n\nfrom lib.semaphore import SemaphoreDeployer, ARA_URL\n# ... ARA querying logic\n# ... Healthcheck reporting\n```\n\n## Key Functions\n\n### ARA Querying\nUse existing `ara.clients.http.AraHttpClient` library (same as `ansible/callback_plugins/ara_url.py`)\n\n```python\ndef get_host_results(playbook_id: int) -\u003e dict[str, dict]:\n    \"\"\"Return {hostname: {changed: N, failed: N}} for all hosts in playbook\"\"\"\n    # Query: ara result list --playbook \u003cid\u003e -f json\n    # Group by host, count changed/failed status\n```\n\n### Idempotency Check Logic\n```python\ndef is_idempotent(host_results: dict) -\u003e tuple[bool, str]:\n    \"\"\"Check if all hosts have 0 changed and 0 failed\"\"\"\n    non_idempotent = []\n    for host, counts in host_results.items():\n        if counts[\"changed\"] \u003e 0 or counts[\"failed\"] \u003e 0:\n            non_idempotent.append(f\"{host}: {counts[\\\"changed\\\"]} changed, {counts[\\\"failed\\\"]} failed\")\n\n    if non_idempotent:\n        return False, \"\\n\".join(non_idempotent)\n    return True, \"All hosts idempotent\"\n```\n\n### Retry Logic\n- Run deploy via `SemaphoreDeployer.deploy_single_attempt()`\n- Check ARA for idempotency\n- If not idempotent, retry (max 3 attempts)\n- Stop early on success (all hosts idempotent)\n\n### Healthcheck Reporting\n```python\ndef report_healthcheck(success: bool, details: str, ara_urls: list[str], semaphore_urls: list[str]):\n    \"\"\"POST to healthchecks.io with status and links\"\"\"\n    ara_links = \"\\n\".join(f\"- {url}\" for url in ara_urls)\n    semaphore_links = \"\\n\".join(f\"- {url}\" for url in semaphore_urls)\n\n    body = f\"\"\"Status: {\"PASS\" if success else \"FAIL\"}\n\n{details}\n\nARA Reports:\n{ara_links}\n\nSemaphore Tasks:\n{semaphore_links}\"\"\"\n\n    endpoint = \"ansible-idempotency-test\" + (\"\" if success else \"/fail\")\n    # POST to https://hc.k.oneill.net/ping/${KEY}/{endpoint}\n```\n\n## Runtime Flow\n1. Trigger Semaphore deploy to all hosts (reuse SemaphoreDeployer)\n2. Extract playbook ID from ARA URL\n3. Query ARA for per-host changed/failed counts\n4. If any host has changed\u003e0 or failed\u003e0: retry (up to 3 attempts)\n5. Report to healthchecks.io with per-host status breakdown","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:41:04.801108-05:00","created_by":"coneill","updated_at":"2026-01-07T09:08:34.800196-05:00","closed_at":"2026-01-07T09:08:34.800196-05:00","close_reason":"Script created and tested - verified with real Semaphore/ARA flow","dependencies":[{"issue_id":"infra-a0g","depends_on_id":"infra-yks","type":"blocks","created_at":"2026-01-07T08:44:49.649713-05:00","created_by":"coneill"}]}
{"id":"infra-a0y","title":"Create Kubernetes CronJob for idempotency test","description":"## Goal\nUpdate CronJob to use ConfigMap-mounted flake instead of `nix run github:...`\n\n## Files\n`kubernetes/semaphore/idempotency-test/cronjob.yaml`\n\n## Changes\n- Switch from nixos/nix to alpine base image\n- Mount `semaphore-infra` ConfigMap at `/infra`\n- Add emptyDir for `/nix`\n- Use inline shell script that:\n  1. Sources `/infra/scripts/setup-nix.sh`\n  2. Runs `nix run .#ansible-idempotency-test`\n\n## ConfigMap additions (via render script)\nAdd Python scripts to ConfigMap:\n- `scripts/ansible-idempotency-test`\n- `scripts/lib/__init__.py`\n- `scripts/lib/semaphore.py`\n\n## Verification\n1. Update render script with Python script copying\n2. Run render script\n3. Deploy with kubectl apply -k\n4. Trigger test job manually\n5. Check job logs for successful execution","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:42:02.324684-05:00","created_by":"coneill","updated_at":"2026-01-09T15:59:35.072859-05:00","closed_at":"2026-01-09T15:59:35.072859-05:00","close_reason":"Phase 2 complete - CronJob uses ConfigMap-mounted scripts, healthcheck improvements added","dependencies":[{"issue_id":"infra-a0y","depends_on_id":"infra-3bq","type":"blocks","created_at":"2026-01-07T08:44:49.751449-05:00","created_by":"coneill"},{"issue_id":"infra-a0y","depends_on_id":"infra-54d","type":"blocks","created_at":"2026-01-07T22:39:25.846998-05:00","created_by":"coneill"},{"issue_id":"infra-a0y","depends_on_id":"infra-61v","type":"blocks","created_at":"2026-01-08T11:42:45.338979-05:00","created_by":"coneill"}]}
{"id":"infra-a5g","title":"Evaluate PR #1145: Update Helm release external-secrets to v1.2.0","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.886202-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-a7j","title":"Evaluate PR #1148: Update Helm release velero to v11.3.1","description":"Run the renovate-eval skill against PR #1148 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.284545-05:00","created_by":"coneill","updated_at":"2026-01-12T22:01:01.038457-05:00","closed_at":"2026-01-12T22:01:01.038457-05:00","close_reason":"PR #1148 merged","dependencies":[{"issue_id":"infra-a7j","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.150303-05:00","created_by":"coneill"}]}
{"id":"infra-ahy","title":"Test 4: No Ansible Changes (Skip)","description":"Verify workflow skips when no ansible changes.\n\n## Steps\n1. Create branch from main\n2. Change a non-ansible file (e.g., `README.md`)\n3. Merge to main\n\n## Verification Checklist\n- [ ] \"Deploy Ansible to Production\" workflow does NOT trigger\n- [ ] Verify via ARA that no unexpected deploys occurred","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T22:22:11.71352-05:00","created_by":"coneill","updated_at":"2026-01-05T09:53:39.822831-05:00","closed_at":"2026-01-05T09:53:39.822831-05:00","close_reason":"Test 4 passed - PR #1064 (kubernetes-only change) merged without triggering Ansible Production Deployment workflow","dependencies":[{"issue_id":"infra-ahy","depends_on_id":"infra-oty","type":"blocks","created_at":"2026-01-04T22:28:30.914761-05:00","created_by":"coneill"}]}
{"id":"infra-ao8","title":"Testing and Validation","description":"**Core Questions:**\n- How to validate derived IPv6 addresses before applying?\n- Pre-commit hook to validate IPv6 derivation matches IPv4?\n- Lab environment testing before production?\n- Success criteria: all hosts have IPv6? Kubernetes uses IPv6? Just DNS?\n- Phased rollout: by host group, subnet, or all at once?\n\n**Investigation Needed:**\n- Review `.pre-commit-config.yaml` for where to add validation\n- Check CI/CD (GitHub Actions) for IPv6 derivation testing\n- Define \"working IPv6\" for this infrastructure\n\n**Critical Files:**\n- `.pre-commit-config.yaml`\n- `.github/workflows/` - CI validation opportunities\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:20.543972-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.47107-05:00","closed_at":"2026-01-22T23:32:54.47107-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-auk","title":"Evaluate PR #1141: Update traefik Docker tag to v3.6.5","description":"Run the renovate-eval skill against PR #1141 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.822931-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.64211-05:00","closed_at":"2026-01-13T00:46:36.64211-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-auk","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.137098-05:00","created_by":"coneill"}]}
{"id":"infra-aye","title":"Migrate backups from Backblaze to Hetzner storage box","status":"open","priority":2,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T13:19:32.252771-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T13:19:32.252771-05:00"}
{"id":"infra-b7y","title":"Fix hass-check-config version drift","description":"scripts/hass-check-config:40 hardcodes ghcr.io/home-assistant/home-assistant:2025.7. Should extract version from kubernetes/hass rendered deployment or helm chart to stay in sync with deployed version.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T21:38:12.916712-05:00","created_by":"coneill","updated_at":"2026-01-06T22:55:42.731214-05:00","closed_at":"2026-01-06T22:55:42.731214-05:00","close_reason":"Closed"}
{"id":"infra-bd3","title":"Replace yannik.relaymail with maintained alternative","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T19:04:28.627177-05:00","created_by":"coneill","updated_at":"2026-01-18T11:45:15.138549-05:00","closed_at":"2026-01-18T11:45:15.138549-05:00","close_reason":"Replaced yannik.relaymail with oefenweb.postfix v3.8.1 - PR #1185"}
{"id":"infra-bem","title":"Check ntd.nut for deprecation fixes","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T17:54:22.269509-05:00","created_by":"coneill","updated_at":"2026-01-18T12:39:30.281324-05:00","closed_at":"2026-01-18T12:39:30.281324-05:00","close_reason":"Submitted upstream PR: https://github.com/ntd/ansible-role-nut/pull/21. Tracking update in https://github.com/claytono/infra/issues/1187"}
{"id":"infra-bre","title":"Matter + Thread Support for pantrypi","description":"# Matter + Thread Support for pantrypi\n\nDeploy Matter and Thread support using ZBT-2, following existing zigbee2mqtt/zwavejs patterns.\n\n## Architecture\n\n```\nThread devices (locks)     WiFi Matter devices\n        ↓                         ↓\n   Thread mesh                    │\n        ↓                         │\n┌─────────────────┐               │\n│  ZBT-2 (USB)    │               │\n└────────┬────────┘               │\n         ↓                        │\n┌─────────────────────────────────┴───┐\n│  pantrypi (host network)            │\n│  ┌─────────┐  ┌───────────────────┐ │\n│  │  OTBR   │→→│  Matter Server    │ │\n│  │ :8081   │  │  :5580            │ │\n│  └─────────┘  └───────────────────┘ │\n└─────────────────┬───────────────────┘\n                  │ Traefik (file provider)\n                  ↓\n┌─────────────────────────────────────┐\n│  matter.oneill.net (HTTPS/WSS)      │\n│  otbr.oneill.net (HTTPS)            │\n└─────────────────┬───────────────────┘\n                  ↓\n┌─────────────────────────────────────┐\n│  Home Assistant (Kubernetes)         │\n└─────────────────────────────────────┘\n```\n\n**Key differences from zigbee/zwave**:\n- Uses `network_mode: host` (required for mDNS/Thread routing)\n- Traefik routes via file provider config, not Docker labels\n\n## Dependencies\n\n- **infra-3s7**: IPv6 + forwarding sysctl settings needed for Thread\n\n## Security Note\n\nOTBR REST API and Matter Server WebSocket have **no protocol-level authentication**. This is a known limitation - IKEA even [closed their OTBR API entirely](https://www.matteralpha.com/explainer/ikea-patched-openthread-border-router-api-on-dirigera-hub) for security. We expose both via Traefik HTTPS for network-level security. Consider adding auth proxy for all home automation APIs in a future bead.\n\n## Existing Ansible Roles\n\n**Searched**: Ansible Galaxy, GitHub, Reddit, community forums. **No existing Ansible roles for Matter/OTBR found.** This plan is based on the reference docker-compose from [ownbee/hass-otbr-docker](https://github.com/ownbee/hass-otbr-docker).\n\n## Prerequisites\n\n### ZBT-2 Firmware\n\nThe ZBT-2 is already connected to pantrypi and detected:\n```\n/dev/serial/by-id/usb-Nabu_Casa_ZBT-2_DCB4D9122BA8-if00 -\u003e ../../ttyACM0\n```\n\nIf firmware needs updating, use [Device Toolbox](https://toolbox.openhomefoundation.org/home-assistant-connect-zbt-2/install/) or manually flash:\n```bash\npip install universal-silabs-flasher\n# Latest firmware: zbt2_openthread_rcp_2.4.4.0_GitHub-7074a43e4_gsdk_4.4.4.gbl\n# From: https://github.com/NabuCasa/silabs-firmware-builder/releases\nuniversal-silabs-flasher --device /dev/ttyACM0 flash --firmware \u003cfirmware.gbl\u003e\n```\n\n### Host Requirements (Verified)\n\n- **dbus**: Running (`systemctl is-active dbus` = active)\n- **Bluetooth**: Available (`hci0` UP RUNNING) - for BLE commissioning\n\n## Implementation\n\n### Phase 1: Create matter role\n\n**Create `ansible/roles/matter/defaults/main.yaml`**:\n```yaml\n---\n# renovate: datasource=docker\nmatter_otbr_image: \"ghcr.io/ownbee/hass-otbr-docker:v0.2.0@sha256:0b1fb50d2d41fba288736994c1773a3768dbef24efdf56780d37e28cb08e3b70\"\n# renovate: datasource=docker\nmatter_server_image: \"ghcr.io/home-assistant-libs/python-matter-server:8.1.0@sha256:170aa093ce91c76cde4cc390918307590f0f5558fcec93f913af3cb019e6562a\"\n\nmatter_data_dir: /etc/matter\nmatter_thread_data_dir: /var/lib/thread\n```\n\n**Create `ansible/roles/matter/tasks/main.yaml`**:\n```yaml\n---\n- name: Create matter directories\n  ansible.builtin.file:\n    path: \"{{ item }}\"\n    state: directory\n    mode: \"0755\"\n  loop:\n    - \"{{ matter_data_dir }}\"\n    - \"{{ matter_data_dir }}/server-data\"\n    - \"{{ matter_thread_data_dir }}\"\n\n- name: Deploy matter docker-compose file\n  ansible.builtin.template:\n    src: docker-compose.yaml.j2\n    dest: \"{{ matter_data_dir }}/docker-compose.yaml\"\n    mode: \"0644\"\n  notify: Restart matter\n\n- name: Start matter services\n  community.docker.docker_compose_v2:\n    project_src: \"{{ matter_data_dir }}\"\n    state: present\n    wait: true\n    wait_timeout: 120\n\n- name: Deploy Traefik file provider config\n  ansible.builtin.template:\n    src: traefik-matter.yaml.j2\n    dest: /etc/traefik/conf.d/matter.yaml\n    mode: \"0644\"\n  notify: Restart traefik\n```\n\n**Create `ansible/roles/matter/templates/docker-compose.yaml.j2`**:\n```yaml\n---\nservices:\n  otbr:\n    image: {{ matter_otbr_image }}\n    container_name: otbr\n    restart: unless-stopped\n    network_mode: host\n    privileged: true\n    devices:\n      - {{ matter_device }}:/dev/ttyUSB0\n    volumes:\n      - {{ matter_thread_data_dir }}:/var/lib/thread\n      - /run/udev:/run/udev:ro\n    environment:\n      DEVICE: /dev/ttyUSB0\n      BACKBONE_IF: {{ matter_backbone_interface }}\n      TZ: America/New_York\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8081/diagnostics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  matter-server:\n    image: {{ matter_server_image }}\n    container_name: matter-server\n    restart: unless-stopped\n    network_mode: host\n    security_opt:\n      - apparmor=unconfined  # Required for Bluetooth commissioning\n    volumes:\n      - {{ matter_data_dir }}/server-data:/data\n      - /run/dbus:/run/dbus:ro  # Required for Bluetooth commissioning\n    environment:\n      TZ: America/New_York\n    command: [\"--storage-path\", \"/data\", \"--bluetooth-adapter\", \"0\"]\n    depends_on:\n      otbr:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 5580 || exit 1\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n```\n\n**Create `ansible/roles/matter/templates/traefik-matter.yaml.j2`**:\n```yaml\n---\n# Traefik file provider config for Matter services\n# Uses host networking so can't use Docker labels\n# Route to ansible_host since Traefik is in bridge mode\n\nhttp:\n  routers:\n    matter-server:\n      rule: \"Host(`{{ matter_server_hostname }}`)\"\n      entryPoints:\n        - websecure\n      service: matter-server\n      tls:\n        certResolver: letsencrypt\n\n    otbr:\n      rule: \"Host(`{{ matter_otbr_hostname }}`)\"\n      entryPoints:\n        - websecure\n      service: otbr\n      tls:\n        certResolver: letsencrypt\n\n  services:\n    matter-server:\n      loadBalancer:\n        servers:\n          - url: \"http://{{ ansible_host }}:5580\"\n\n    otbr:\n      loadBalancer:\n        servers:\n          - url: \"http://{{ ansible_host }}:8081\"\n```\n\n**Create `ansible/roles/matter/handlers/main.yaml`**:\n```yaml\n---\n- name: Restart matter\n  community.docker.docker_compose_v2:\n    project_src: \"{{ matter_data_dir }}\"\n    state: present\n    recreate: always\n    wait: true\n    wait_timeout: 120\n\n- name: Restart traefik\n  community.docker.docker_compose_v2:\n    project_src: /etc/traefik\n    state: present\n    recreate: always\n```\n\n**Create `ansible/roles/matter/meta/main.yaml`**:\n```yaml\n---\ndependencies:\n  - role: geerlingguy.docker\n  - role: traefik\n```\n\n### Phase 2: Update inventory and playbook\n\n**Edit `ansible/inventory/default`** - add after `[zigbee2mqtt]` section:\n```ini\n[matter]\npantrypi\n```\n\n**Edit `ansible/site.yaml`** - add after Zigbee2MQTT play (~line 200):\n```yaml\n- name: Matter (OTBR + Matter Server)\n  hosts: matter\n  roles:\n    - role: matter\n      tags: matter\n```\n\n### Phase 3: Create host_vars and DNS\n\n**Create `ansible/host_vars/pantrypi.yaml`**:\n```yaml\n---\n# ZBT-2 Thread device\nmatter_device: /dev/serial/by-id/usb-Nabu_Casa_ZBT-2_DCB4D9122BA8-if00\nmatter_backbone_interface: eth0\n\n# Traefik hostnames\nmatter_server_hostname: matter.oneill.net\nmatter_otbr_hostname: otbr.oneill.net\n```\n\n**Edit `opentofu/modules/dns/oneill.tf`** - add CNAMEs:\n```hcl\n  # Matter services\n  \"matter\" = { type = \"CNAME\", value = \"pantrypi.oneill.net.\" }\n  \"otbr\"   = { type = \"CNAME\", value = \"pantrypi.oneill.net.\" }\n```\n\n### Phase 4: IPv6 sysctl (part of infra-3s7)\n\nRequired for Thread networking. Implement as part of **infra-3s7**:\n\n```yaml\n# /etc/sysctl.d/99-ipv6.conf\nnet.ipv6.conf.all.disable_ipv6=0\nnet.ipv4.conf.all.forwarding=1\nnet.ipv6.conf.all.forwarding=1\nnet.ipv6.conf.all.accept_ra_rt_info_max_plen=64\nnet.ipv6.conf.all.accept_ra=2\n```\n\n## Deployment\n\n```bash\ncd ~/src/infra/ansible\nansible-playbook site.yaml -l pantrypi --tags matter\n\ncd ~/src/infra/opentofu\ntofu apply -target=module.dns\n```\n\n## Home Assistant Integration\n\n### 1. Add OpenThread Border Router integration\n\n1. Settings \u003e Devices \u0026 Services \u003e Add Integration\n2. Search \"OpenThread Border Router\"\n3. URL: `https://otbr.oneill.net`\n\n### 2. Add Thread integration\n\nShould auto-configure from OTBR.\n\n### 3. Add Matter integration\n\n1. Settings \u003e Devices \u0026 Services \u003e Add Integration\n2. Search \"Matter\"\n3. URL: `wss://matter.oneill.net/ws`\n\n### 4. Commission Thread devices\n\n1. Open Home Assistant Companion app\n2. Settings \u003e Companion App \u003e Troubleshooting \u003e **Sync Thread credentials**\n3. Settings \u003e Devices \u0026 Services \u003e Devices \u003e **Add Device** \u003e **Add Matter device**\n4. Scan the Matter QR code on your lock\n5. Wait for commissioning (2-3 minutes for Thread devices)\n\n**Note**: BLE commissioning requires Bluetooth on your phone. Pantrypi has Bluetooth (hci0) for local commissioning if needed.\n\n## Verification\n\n```bash\n# Check via HTTPS\ncurl https://matter.oneill.net\ncurl https://otbr.oneill.net/diagnostics\n\n# Check container health\nssh pantrypi 'docker ps --format \"table {{.Names}}\\t{{.Status}}\"'\n\n# View logs\nssh pantrypi 'docker logs otbr --tail 50'\nssh pantrypi 'docker logs matter-server --tail 50'\n```\n\n## Files to create/modify\n\n| File | Action |\n|------|--------|\n| `ansible/roles/matter/defaults/main.yaml` | Create |\n| `ansible/roles/matter/tasks/main.yaml` | Create |\n| `ansible/roles/matter/templates/docker-compose.yaml.j2` | Create |\n| `ansible/roles/matter/templates/traefik-matter.yaml.j2` | Create |\n| `ansible/roles/matter/handlers/main.yaml` | Create |\n| `ansible/roles/matter/meta/main.yaml` | Create |\n| `ansible/inventory/default` | Edit - add `[matter]` group |\n| `ansible/site.yaml` | Edit - add matter play |\n| `ansible/host_vars/pantrypi.yaml` | Create |\n| `opentofu/modules/dns/oneill.tf` | Edit - add CNAMEs |\n\n## Known Limitations\n\n1. **Thread commissioning** requires IPv6 (depends on infra-3s7)\n2. **ZBT-2 is Thread-only** after flashing (SkyConnect handles Zigbee)\n3. **No protocol-level auth** - relying on network isolation + HTTPS\n\n## Sources\n\n- [Thread/Matter with Home Assistant Docker](https://blog.maxaller.name/engineering/2025/08/10/thread-matter-home-assistant-docker.html)\n- [hass-otbr-docker GitHub](https://github.com/ownbee/hass-otbr-docker)\n- [python-matter-server Docker docs](https://github.com/home-assistant-libs/python-matter-server/blob/main/docs/docker.md)\n- [Home Assistant Matter integration](https://www.home-assistant.io/integrations/matter/)\n- [IKEA OTBR security note](https://www.matteralpha.com/explainer/ikea-patched-openthread-border-router-api-on-dirigera-hub)\n","status":"open","priority":2,"issue_type":"feature","owner":"clayton@oneill.net","created_at":"2026-01-20T10:52:51.097506-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-20T10:52:51.097506-05:00","dependencies":[{"issue_id":"infra-bre","depends_on_id":"infra-77h","type":"blocks","created_at":"2026-01-22T23:35:00.999573-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-bx8","title":"Add WebSocket error handling to semaphore-deploy","description":"**File:** /Users/coneill/src/infra2/scripts/semaphore-deploy\n\n**Problem:** `stream_task_output_ws()` (lines 180-214) doesn't handle connection drops, invalid JSON, or timeouts gracefully:\n- `create_connection()` has no timeout parameter\n- `ws.recv()` blocks forever if server stops responding  \n- No handling for `WebSocketConnectionClosedException`\n\n**Changes Required:**\n\n1. **Add import for WebSocket exception** at top of file (after line 32):\n```python\nfrom websocket import create_connection, WebSocketConnectionClosedException\n```\n\n2. **Replace stream_task_output_ws method** (lines 180-214) with:\n```python\n    def stream_task_output_ws(self, task_id: int) -\u003e str:\n        \"\"\"Stream task output via WebSocket, return final status\"\"\"\n        ws_url = SEMAPHORE_URL.replace(\"https://\", \"wss://\") + \"/api/ws\"\n\n        ws = create_connection(\n            ws_url,\n            header={\"Authorization\": f\"Bearer {self.token}\"},\n            timeout=30\n        )\n\n        try:\n            ws.settimeout(60)  # 60s timeout for recv()\n            while True:\n                try:\n                    message = ws.recv()\n                except WebSocketConnectionClosedException:\n                    print(\"WebSocket connection closed unexpectedly\")\n                    return \"error\"\n                \n                try:\n                    data = json.loads(message)\n                except json.JSONDecodeError:\n                    continue  # Skip malformed messages\n\n                # Filter for our task\n                if data.get(\"task_id\") != task_id:\n                    continue\n\n                if data.get(\"type\") == \"log\":\n                    timestamp = data.get(\"time\", \"\")\n                    output = data.get(\"output\", \"\")\n                    if timestamp:\n                        try:\n                            dt = datetime.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n                            print(f\"  [{dt.strftime('%H:%M:%S.%f')[:-3]}] {output}\")\n                        except ValueError:\n                            print(f\"  \u003e {output}\")\n                    else:\n                        print(f\"  \u003e {output}\")\n                elif data.get(\"type\") == \"update\":\n                    status = data.get(\"status\")\n                    if status in (\"success\", \"error\", \"stopped\"):\n                        return status\n        finally:\n            ws.close()\n```\n\n**Context:** Addresses CodeRabbit (ID: 2659796658) and Copilot (ID: 2659796706, 2659796759, 2659796762) feedback about WebSocket robustness.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:50:07.920245-05:00","created_by":"coneill","updated_at":"2026-01-04T12:59:59.874555-05:00","closed_at":"2026-01-04T12:59:59.87457-05:00"}
{"id":"infra-c57","title":"Fix ARA callback race condition - restore UUID tracking","description":"**File:** /Users/coneill/src/infra2/ansible/callback_plugins/ara_url.py\n\n**Problem:** The callback queries for \"latest playbook by ID\" which causes a race condition if multiple playbooks run concurrently. Additionally, `v2_playbook_on_start` shows the *previous* run's URL since the current playbook hasn't been recorded yet.\n\n**Changes Required:**\n\n1. **Track play UUIDs** - Add instance variable to store play UUIDs\n2. **Remove v2_playbook_on_start ARA output** - This shows incorrect/stale data\n3. **Update v2_playbook_on_stats** - Use UUID-based lookup instead of order-by-id\n\n**Replace the entire file with:**\n```python\n\"\"\"Print ARA playbook URL at end of every playbook run.\"\"\"\n\nimport os\nfrom ansible.plugins.callback import CallbackBase\n\n\nclass CallbackModule(CallbackBase):\n    CALLBACK_VERSION = 2.0\n    CALLBACK_TYPE = \"notification\"\n    CALLBACK_NAME = \"ara_url\"\n    CALLBACK_NEEDS_ENABLED = False\n\n    def __init__(self):\n        super().__init__()\n        self._ara_enabled = False\n        self._ara_client = None\n        self._plays = []  # Track play UUIDs for accurate lookup\n\n    def v2_playbook_on_start(self, playbook):\n        \"\"\"Pre-flight check: validate ARA config and create client early.\"\"\"\n        api_server = os.environ.get(\"ARA_API_SERVER\", \"\")\n        if not api_server:\n            self._display.v(\"ara_url: ARA_API_SERVER not set, skipping URL reporting\")\n            return\n\n        try:\n            from ara.clients.http import AraHttpClient\n\n            # Validate we can create a client with required config\n            self._ara_client = AraHttpClient(\n                endpoint=api_server,\n                timeout=int(os.environ.get(\"ARA_API_TIMEOUT\", \"30\")),\n                auth=(\n                    os.environ.get(\"ARA_API_USERNAME\"),\n                    os.environ.get(\"ARA_API_PASSWORD\"),\n                ),\n            )\n            self._ara_enabled = True\n            # Don't print URL here - playbook not recorded yet, would show stale data\n\n        except ImportError:\n            self._display.warning(\"ara_url: ARA package not installed, URL reporting disabled\")\n        except Exception as e:\n            self._display.warning(f\"ara_url: Failed to initialize ARA client: {e}\")\n\n    def v2_playbook_on_play_start(self, play):\n        \"\"\"Track play UUIDs for accurate playbook lookup.\"\"\"\n        self._plays.append(play)\n\n    def v2_playbook_on_stats(self, stats):\n        \"\"\"Print ARA URL at end - fires regardless of success/failure.\"\"\"\n        if not self._ara_enabled or not self._ara_client:\n            return\n\n        api_server = os.environ.get(\"ARA_API_SERVER\", \"\")\n        try:\n            # Use play UUID to find the correct playbook (avoids race condition)\n            if self._plays:\n                response = self._ara_client.get(f\"/api/v1/plays?uuid={self._plays[0]._uuid}\")\n                if response.get(\"results\"):\n                    playbook_id = response[\"results\"][0][\"playbook\"]\n                    url = f\"{api_server}/playbooks/{playbook_id}.html\"\n                    self._display.display(f\"ARA Playbook ID: {playbook_id} URL: {url}\")\n                    return\n\n            # Fallback to order-by-id if UUID lookup fails\n            response = self._ara_client.get(\"/api/v1/playbooks\", order=\"-id\", limit=1)\n            if response.get(\"results\"):\n                playbook_id = response[\"results\"][0][\"id\"]\n                url = f\"{api_server}/playbooks/{playbook_id}.html\"\n                self._display.display(f\"ARA Playbook ID: {playbook_id} URL: {url}\")\n\n        except Exception as e:\n            self._display.warning(f\"ara_url: Failed to retrieve playbook URL: {e}\")\n```\n\n**Context:** This fixes race conditions identified by CodeRabbit (ID: 2659796655) and Copilot (ID: 2659796710, 2659796715). The UUID-based approach was in the original implementation (commit 93024dad).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:49:50.436916-05:00","created_by":"coneill","updated_at":"2026-01-04T12:58:54.15648-05:00","closed_at":"2026-01-04T12:58:54.156483-05:00"}
{"id":"infra-cow","title":"Change timezone for Semaphore container","description":"Configure timezone for the Semaphore container so task timestamps display in local time instead of UTC.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-07T09:32:54.065184-05:00","created_by":"coneill","updated_at":"2026-01-11T11:15:11.729275-05:00","closed_at":"2026-01-10T12:54:16.599165-05:00"}
{"id":"infra-cza","title":"Remove old per-image managers from .renovaterc","description":"Remove lines 73-102 from `.renovaterc`.\n\nThese are the 3 hardcoded per-image managers for rtl433, zwavejs, and zigbee2mqtt that will be replaced by the generic manager.\n\nThis step must be done AFTER Steps 2, 3, and 4 complete (after the image formats have been migrated).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:44:01.081174-05:00","created_by":"coneill","updated_at":"2026-01-06T12:20:43.277855-05:00","closed_at":"2026-01-06T12:20:43.277855-05:00","close_reason":"Closed","dependencies":[{"issue_id":"infra-cza","depends_on_id":"infra-e5p","type":"blocks","created_at":"2026-01-06T11:46:20.10581-05:00","created_by":"coneill"},{"issue_id":"infra-cza","depends_on_id":"infra-o3x","type":"blocks","created_at":"2026-01-06T11:46:23.7585-05:00","created_by":"coneill"},{"issue_id":"infra-cza","depends_on_id":"infra-578","type":"blocks","created_at":"2026-01-06T11:46:26.877301-05:00","created_by":"coneill"}]}
{"id":"infra-d1y","title":"Add healthcheck to opentofu/healthchecks.tf","description":"## File\nModify: `opentofu/healthchecks.tf`\n\n## Changes\nAdd new check: `ansible-idempotency-test` with appropriate timeout/grace period","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:44:03.38468-05:00","created_by":"coneill","updated_at":"2026-01-07T22:39:34.439805-05:00","closed_at":"2026-01-07T22:39:34.439805-05:00","close_reason":"Healthcheck created and applied via tofu apply","dependencies":[{"issue_id":"infra-d1y","depends_on_id":"infra-a0g","type":"blocks","created_at":"2026-01-07T08:44:49.805996-05:00","created_by":"coneill"}]}
{"id":"infra-dd1","title":"Evaluate PR #1142: Update dependency community.proxmox to v1.5.0","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.100479-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-dfe","title":"Phase 2: Ansible Sysctl Consolidation","description":"Read the full plan and all comments in infra-ec1, then implement Phase 2.","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:32:29.718022-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-29T13:00:58.828203-05:00","closed_at":"2026-01-29T13:00:58.828203-05:00","close_reason":"Code merged to main (a03f9fab). Sysctl consolidation deployed and validated.","dependencies":[{"issue_id":"infra-dfe","depends_on_id":"infra-s7d","type":"blocks","created_at":"2026-01-22T23:32:41.272344-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-dlq","title":"Investigate and address Ansible deprecation warnings","description":"Address deprecation warnings appearing in Ansible playbook runs. Analysis completed Jan 13, 2026.\n\n## Deprecation Types Found\n\n### 1. INJECT_FACTS_AS_VARS (ansible-core 2.24 removal)\nUsing `ansible_*` variables directly instead of `ansible_facts['*']` syntax.\n\n### 2. Module Utils Imports\n- `ansible.module_utils._text` (to_native, to_text)\n- `ansible.module_utils.common._collections_compat`\n\n## Action Items\n\n### Our Code (fix directly)\n- [ ] group_vars/all/docker.yaml\n- [ ] host_vars/flux.yaml\n- [ ] roles/kubeadm/tasks/master.yaml\n- [ ] roles/kubeadm/templates/kubeadm.conf.j2\n\n### Galaxy Roles (external)\n- [ ] artis3n.tailscale - Check for updates, submit PR if needed\n- [ ] ntd.nut - Check for updates\n- [ ] tbaczynski.chrony - Find replacement (unmaintained since 2021)\n- [ ] yannik.relaymail - Find replacement (unmaintained since 2019)\n\n## Timeline\n- Deprecated: ansible-core 2.19+\n- Removal: ansible-core 2.24 (expected late 2026)","status":"open","priority":3,"issue_type":"epic","created_at":"2026-01-07T09:14:21.334902-05:00","created_by":"coneill","updated_at":"2026-01-13T17:50:26.87904-05:00","dependencies":[{"issue_id":"infra-dlq","depends_on_id":"infra-6fm","type":"blocks","created_at":"2026-01-13T19:04:39.948523-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-23a","type":"blocks","created_at":"2026-01-13T19:04:40.009126-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-2jr","type":"blocks","created_at":"2026-01-13T19:04:40.070193-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-ts6","type":"blocks","created_at":"2026-01-13T19:04:40.12918-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-4jd","type":"blocks","created_at":"2026-01-13T19:04:40.187029-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-bem","type":"blocks","created_at":"2026-01-13T19:04:40.243758-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-op3","type":"blocks","created_at":"2026-01-13T19:04:40.302591-05:00","created_by":"coneill"},{"issue_id":"infra-dlq","depends_on_id":"infra-bd3","type":"blocks","created_at":"2026-01-13T19:04:40.361132-05:00","created_by":"coneill"}],"comments":[{"id":6,"issue_id":"infra-dlq","author":"coneill","text":"## Deprecation Analysis (Jan 13, 2026)\n\n### Summary\nAll deprecations are INJECT_FACTS_AS_VARS warnings (ansible-core 2.24 removal) and module_utils import warnings.\n\n### Our Code (4 files to fix)\n- group_vars/all/docker.yaml:6 - uses ansible_distribution\n- host_vars/flux.yaml:15 - template uses ansible facts\n- roles/kubeadm/tasks/master.yaml:30,37 - uses ansible_fqdn\n- roles/kubeadm/templates/kubeadm.conf.j2 - uses ansible facts\n\nFix: Change ansible_* to ansible_facts['*'] syntax\n\n### Galaxy Roles (need replacement or PR)\n- artis3n.tailscale - Active, check for updates/PR\n- ntd.nut - Active, check for updates\n- tbaczynski.chrony - Last updated 2021, REPLACE\n- yannik.relaymail - Last updated 2019, REPLACE\n\n### Module Utils Imports (external)\n- to_native, to_text from ansible.module_utils._text\n- ansible.module_utils.common._collections_compat\nThese come from external modules - will resolve with module updates.\n\n### Timeline\n- Deprecated in ansible-core 2.19+\n- Removal in ansible-core 2.24 (expected late 2026)","created_at":"2026-01-13T22:47:35Z"}]}
{"id":"infra-e21","title":"Improve Kubernetes node failure pod rescheduling","description":"When a node fails, pods take too long to reschedule. Need to review settings and optimize.\n\n**Tasks:**\n- [ ] Review current cluster settings (node eviction timeouts, tolerations, etc.)\n- [ ] Research best practices for fast pod rescheduling on node failure\n- [ ] Document current behavior with baseline test\n- [ ] Implement improvements\n- [ ] Test and validate faster rescheduling\n\n**Areas to investigate:**\n- kube-controller-manager: node-monitor-period, node-monitor-grace-period, pod-eviction-timeout\n- Default tolerations on pods (node.kubernetes.io/not-ready, node.kubernetes.io/unreachable)\n- PodDisruptionBudgets\n- Node problem detector settings\n- Taints and toleration seconds","status":"open","priority":2,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-19T10:48:36.643903-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-19T10:48:36.643903-05:00"}
{"id":"infra-e4z","title":"Make Semaphore URL configurable via environment variable","description":"**File:** /Users/coneill/src/infra2/scripts/semaphore-deploy\n\n**Problem:** Semaphore URL is hardcoded at line 34:\n```python\nSEMAPHORE_URL = \"https://semaphore.k.oneill.net\"\n```\n\nThis prevents using a different URL (e.g., Tailscale endpoint) without modifying the script.\n\n**Change Required:**\n\nReplace line 34:\n```python\nSEMAPHORE_URL = \"https://semaphore.k.oneill.net\"\n```\n\nWith:\n```python\nSEMAPHORE_URL = os.environ.get(\"SEMAPHORE_URL\", \"https://semaphore.k.oneill.net\")\n```\n\n**Context:** Allows the GitHub Actions workflow to use the Tailscale URL (`https://semaphore.tail39d37.ts.net`) while maintaining the public URL as default for local use.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:50:17.76191-05:00","created_by":"coneill","updated_at":"2026-01-04T12:59:59.930546-05:00","closed_at":"2026-01-04T12:59:59.930556-05:00"}
{"id":"infra-e5p","title":"Convert rtl433 to Static File","description":"Convert rtl433 role from template to static file (only templated value was the image).\n\n## Create `ansible/roles/rtl433/files/docker-compose.yaml`:\n\n```yaml\nservices:\n  rtl_433:\n    container_name: rtl_433\n    # renovate: datasource=docker\n    image: hertzg/rtl_433:25.12@sha256:c85d62f16205fdbdeb398ee464c66897854805f4aa6738876c9f2ba0874a70e0\n    restart: unless-stopped\n    devices:\n      - /dev/bus/usb:/dev/bus/usb\n    environment:\n      - MQTT_USERNAME=\\${MQTT_USER}\n      - MQTT_PASSWORD=\\${MQTT_PASS}\n      - MQTT_HOST=\\${MQTT_HOST}\n      - MQTT_PORT=\\${MQTT_PORT}\n      - TZ=America/New_York\n    command:\n      - \"-f\"\n      - \"915MHz\"\n      - \"-g\"\n      - \"0\"\n      - \"-C\"\n      - \"si\"\n      - \"-F\"\n      - \"json\"\n      - \"-F\"\n      - \"mqtt://\\${MQTT_HOST}:\\${MQTT_PORT}/retain=0,base=rtl_433,events=rtl_433/devices[/model][/id]\"\n      - \"-F\"\n      - \"log\"\n```\n\n## Update `ansible/roles/rtl433/tasks/main.yaml`:\n\nChange:\n- `ansible.builtin.template` → `ansible.builtin.copy`\n- `src: docker-compose.yaml.j2` → `src: docker-compose.yaml`\n\n## Delete:\n\n- `ansible/roles/rtl433/templates/docker-compose.yaml.j2`\n- `ansible/roles/rtl433/defaults/main.yaml`","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:33:58.992884-05:00","created_by":"coneill","updated_at":"2026-01-06T12:19:09.467645-05:00","closed_at":"2026-01-06T12:19:09.467645-05:00","close_reason":"Closed"}
{"id":"infra-ec1","title":"IPv6 Infrastructure Enablement - Implementation","description":"# IPv6 Infrastructure Enablement - Implementation Plan\n\n## Executive Summary\n\nEnable IPv6 for 12 homelab infrastructure servers on Default VLAN (172.19.74.0/24) with static addressing and AAAA DNS records.\n\n**Quick Stats:**\n- **Hosts receiving IPv6 via Ansible:** 12 (4 Proxmox, 4 K8s nodes, 1 physical server, 2 Raspberry Pis, 1 test VM)\n- **Hosts with existing IPv6:** 1 (flux - Vultr VPS, already configured, needs AAAA record only)\n- **AAAA DNS records to create:** 12 (11 Ansible-managed + flux, excludes luser with public_dns=false)\n- **Hosts excluded:** 5 (3 AMT interfaces, fs2 NAS, szamar desktop)\n- **Terraform files modified:** 5\n- **Ansible files modified:** 16\n- **Implementation phases:** 6 (0=Prep \u0026 k4 Restoration, 1=RPi IPv4, 2=Sysctl, 3=IPv6 Config, 4=Deploy, 5=DNS)\n- **Pull requests:** 2 (k4 restoration PR, then main IPv6 enablement PR)\n\n---\n\n## IPv6 Address Design\n\n**Network:**\n- VLAN: Default (172.19.74.0/24)\n- IPv6 Prefix: 2600:4040:2ece:7500::/64\n- Gateway: 2600:4040:2ece:7500::1\n\n**Derivation Formula (Simple):**\n```\nIPv4: 172.19.74.X → IPv6: 2600:4040:2ece:7500::74:X\n\nExamples:\n- 172.19.74.41  → 2600:4040:2ece:7500::74:41\n- 172.19.74.120 → 2600:4040:2ece:7500::74:120\n- 172.19.74.134 → 2600:4040:2ece:7500::74:134\n```\n\n**Implementation:**\n- **Terraform:** Compute IPv6 on-the-fly in DNS module from IPv4 address\n- **Ansible:** Derive IPv6 using Jinja2 in host_vars from global prefix + IPv4\n\n---\n\n## Host Inventory\n\n### Hosts Receiving IPv6 Configuration (12)\n\n| Host | IPv4 | IPv6 | Type | AAAA Record |\n|------|------|------|------|-------------|\n| p1 | 172.19.74.41 | ::74:41 | Proxmox | ✓ |\n| p2 | 172.19.74.42 | ::74:42 | Proxmox | ✓ |\n| p3 | 172.19.74.43 | ::74:43 | Proxmox | ✓ |\n| p4 | 172.19.74.44 | ::74:44 | Proxmox | ✓ |\n| infra1 | 172.19.74.31 | ::74:31 | Physical | ✓ |\n| k1 | 172.19.74.134 | ::74:134 | K8s node | ✓ |\n| k2 | 172.19.74.112 | ::74:112 | K8s node | ✓ |\n| k3 | 172.19.74.74 | ::74:74 | K8s node | ✓ |\n| k4 | 172.19.74.75 | ::74:75 | K8s node | ✓ |\n| garagepi | 172.19.74.216 | ::74:216 | RPi (WiFi) | ✓ |\n| pantrypi | 172.19.74.120 | ::74:120 | RPi (Ethernet) | ✓ |\n| luser | 172.19.74.161 | ::74:161 | VM (test) | ✗ (public_dns=false) |\n\n### Hosts with Existing IPv6 (1)\n\n| Host | IPv4 | IPv6 Status | AAAA Record |\n|------|------|-------------|-------------|\n| flux | Vultr VPS | Already configured | ✓ (add in Terraform) |\n\n### Hosts Excluded from IPv6 (6)\n\n| Host | IPv4 | Reason |\n|------|------|--------|\n| p2-amt | 172.19.74.201 | AMT interface (no IPv6 hardware support) |\n| p3-amt | 172.19.74.82 | AMT interface (no IPv6 hardware support) |\n| p4-amt | 172.19.74.83 | AMT interface (no IPv6 hardware support) |\n| fs2 | 172.19.74.139 | Synology NAS (not in Ansible inventory, Phase 2) |\n| szamar | 172.19.74.50 | Desktop PC (not managed by Ansible) |\n| pantrypi-wifi | 172.19.74.118 | **REMOVE** - Duplicate of pantrypi (same device) |\n\n---\n\n## Critical Implementation Requirements\n\n1. **Phase Ordering:** Ansible configuration MUST be deployed and validated BEFORE creating Terraform AAAA records\n2. **Why:** IPv6 is already active on the network for client devices. Creating AAAA records before servers have IPv6 will cause connection failures.\n3. **IPv6 Hardening:** Disable SLAAC, Router Advertisements, and privacy extensions (servers need stable addresses)\n4. **IP Forwarding:** Enable both IPv4 AND IPv6 forwarding together for: k1-k4, infra1, pantrypi\n5. **Validation:** Test at each phase before proceeding\n\n---\n\n## Implementation Phases\n\n### Phase 0: Preparation \u0026 Validation\n- Add k4 back to Ansible inventory and deploy baseline configuration (separate PR)\n- Create IPv6 feature branch for main work\n- Verify UniFi IPv6 configuration and firewall rules\n- Review current Terraform and Ansible state\n- Identify test host for external validation\n- Review ICMPv6 blocking on UDMP\n\n### Phase 1: Ansible IPv4 Network Management for RPis\n- Put garagepi and pantrypi under Ansible management for IPv4\n- Configure static networking before adding IPv6\n- Create vault variable for WiFi PSK\n- Deploy and validate\n\n### Phase 2: Ansible Sysctl Consolidation\n- Consolidate IPv4/IPv6 forwarding into common role\n- Add IPv6 network hardening sysctls\n- Remove duplicate forwarding configs from other roles\n- Deploy and validate on test host\n\n### Phase 3: Ansible IPv6 Configuration\n- Create global IPv6 variables (prefix, gateway)\n- Update Proxmox network template for IPv6\n- Add IPv6 config to host_vars for all hosts\n- Update RPi host_vars to add IPv6\n\n### Phase 4: Phased Deployment \u0026 External Validation\n- Deploy incrementally: test → infra → Proxmox → RPis → Kubernetes\n- Validate connectivity at each step\n- Test external IPv6 access with nmap from flux\n- Verify UDMP firewall blocking external access\n- Commit after successful deployment\n\n### Phase 5: Terraform DNS Changes\n- Add IPv6 computation to DNS module\n- Create AAAA records for 12 hosts (11 Ansible-managed + flux, excludes luser)\n- Remove pantrypi-wifi from Terraform\n- Verify DNS resolution\n\n---\n\n## Phase 0: Preparation \u0026 Validation\n\n**⚠️ CRITICAL SSH RESTRICTION ⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**You MUST NOT make any changes via SSH without explicit user approval**\n**Read-only commands only: cat, grep, ip addr, sysctl, systemctl status, etc.**\n**NEVER: systemctl restart, editing files via SSH, running deployment commands without permission**\n\n### Task 0.1: Add k4 Back to Inventory and Deploy\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval before running ansible-playbook\n\n**Background:** k4 needs to be added back to the Ansible inventory and have a baseline deployment before starting IPv6 work.\n\n**Step 1: Check current inventory status**\n\n```bash\n# Check if k4 is in inventory\ngrep -r \"k4\" ansible/inventory/\n\n# Check if k4 host_vars exists\nls -la ansible/host_vars/k4.yaml\n```\n\n**Step 2: Add k4 to inventory** (if missing)\n\n**File:** `ansible/inventory/default`\n\nAdd k4 to the kubernetes group with k1, k2, k3.\n\n**Step 3: Create k4 host_vars** (if missing)\n\n**File:** `ansible/host_vars/k4.yaml`\n\nBased on pattern from other K8s nodes:\n```yaml\n---\nmanage_network: true\nhost_ipv4: \"172.19.74.75\"\n\ninterfaces_ether_interfaces:\n  - device: ens18\n    bootproto: static\n    address: \"172.19.74.75\"\n    netmask: \"255.255.255.0\"\n    gateway: 172.19.74.1\n    dnsnameservers: 172.19.74.1\n    dnssearch: oneill.net\n```\n\n**Note:** IPv4 address 172.19.74.75 based on plan's host inventory table.\n\n**Step 4: Deploy to k4**\n\n```bash\ncd ansible\nansible-playbook -l k4 site.yaml\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh k4.oneill.net \"hostname; ip addr show; systemctl status kubelet\"\n```\n\n**Expected:**\n- k4 responds to SSH\n- Network configuration matches host_vars\n- Kubelet service is running\n- Node is in Ready state\n\n**Step 5: Verify Kubernetes cluster status**\n\n```bash\nkubectl get nodes\n# Expected: k4 shows as Ready\n```\n\n**Step 6: Commit k4 restoration**\n\n```bash\ngit add ansible/inventory/default ansible/host_vars/k4.yaml\ngit commit -m \"Add k4 back to Ansible inventory\n\n- Add k4 to kubernetes group in inventory\n- Create host_vars with IPv4 configuration (172.19.74.75)\n- Deploy baseline configuration\n- Verify k4 is Ready in Kubernetes cluster\"\n```\n\n**Step 7: Create PR for k4 restoration**\n\n```bash\ngit push -u origin HEAD\ngh pr create --fill\n```\n\n**Step 8: Wait for PR merge**\n\n**⚠️ CRITICAL BLOCKING STEP ⚠️**\n\n**DO NOT PROCEED** with the rest of the plan until this PR is merged. k4 must be in the main branch inventory before continuing with IPv6 work.\n\nAfter PR is merged:\n```bash\ngit checkout main\ngit pull\ngit branch -d \u003ck4-branch\u003e\n```\n\n### Task 0.2: Create IPv6 Feature Branch\n\nCreate a new feature branch for the main IPv6 enablement work:\n\n```bash\ngit checkout -b ipv6-infrastructure-enablement origin/main\n```\n\n**Note:** All subsequent work (Phases 1-5) will be committed to this branch and merged in a single PR at the end.\n\n### Task 0.3: Verify UniFi IPv6 Configuration\n\nCheck that IPv6 RA is enabled and configured correctly:\n\n```bash\n# SSH to any host with IPv6 already\nssh \u003chost\u003e 'ip -6 addr show | grep 2600:4040:2ece:7500'\nssh \u003chost\u003e 'ip -6 route show default'\n```\n\n**Expected:**\n- Prefix: 2600:4040:2ece:7500::/64\n- Gateway: 2600:4040:2ece:7500::1\n\n### Task 0.4: Review UniFi Firewall Rules for ICMPv6\n\nCheck UniFi firewall rules - currently blocking ICMPv6 to WAN interface on UDMP:\n\n```bash\n# Review firewall rules in UniFi Controller\n# Look for rules blocking ICMPv6\n# Consider whether this is intentional for security\n```\n\n**Action:** Document current ICMPv6 blocking behavior. Determine if this should be changed.\n\n### Task 0.5: Review Current Terraform State\n\n```bash\ncd opentofu\ntofu state list | grep -E '(infrastructure_hosts|route53_record)'\n```\n\n**Expected:** Existing A records, no AAAA records\n\n### Task 0.6: Review Current Ansible State\n\nCheck current network and sysctl configuration:\n\n```bash\n# Check for existing sysctl files\nansible all -m shell -a \"ls -la /etc/sysctl.d/99-*.conf\" | grep -E \"(kubernetes|cri)\"\n\n# Check current IP forwarding hosts\ngrep -r \"ip_forwarding_enabled\" ansible/host_vars/ ansible/group_vars/\n```\n\n**Expected:** Forwarding currently configured for infra1, flux only\n\n### Task 0.7: Identify Test Host with Open Ports\n\nChoose a single host with exposed services for external validation testing:\n\n**Candidates:**\n- infra1 (has NFS, potentially other services)\n- Any host with known externally-accessible services\n\n**Action:** Document which host will be used for external IPv6 validation\n\n---\n\n## Phase 1: Ansible IPv4 Network Management for RPis\n\n**⚠️ CRITICAL SSH RESTRICTION ⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**You MUST NOT make any changes via SSH without explicit user approval**\n**MUST obtain explicit permission before running ANY Ansible deployment commands**\n\n**Goal:** Put garagepi and pantrypi under Ansible network management for IPv4 before adding IPv6\n\n**Background:** Both RPis are in Ansible inventory but don't have host_vars yet. They're currently using DHCP/NetworkManager. We need to configure static IPv4 first, then add IPv6 in Phase 3.\n\n### Task 1.1: Create Encrypted WiFi PSK Variable\n\n**CRITICAL - READ-ONLY SSH**: Only reading WiFi PSK, not making changes\n\nGet the current WiFi password from garagepi and add encrypted variable to main.yaml:\n\n```bash\n# SSH to garagepi to get current WiFi PSK (READ-ONLY)\nssh garagepi.oneill.net \"sudo nmcli --show-secrets connection show 'netplan-wlan0-bleh' | grep 'psk:'\"\n\n# Encrypt the PSK for group_vars\ncd ansible\nansible-vault encrypt_string '\u003cpassword-from-above\u003e' --name 'vault_wifi_psk_bleh'\n```\n\n**File:** `ansible/group_vars/all/main.yaml`\n\nAdd the encrypted variable at the end of the file:\n\n```yaml\n# WiFi PSK for garagepi (SSID: bleh)\nvault_wifi_psk_bleh: !vault |\n      $ANSIBLE_VAULT;1.1;AES256\n      \u003cencrypted-string-from-above\u003e\n```\n\n### Task 1.2: Create garagepi IPv4 Host Variables\n\n**File:** `ansible/host_vars/garagepi.yaml` (NEW)\n\n```yaml\n---\nmanage_network: true\nhost_ipv4: \"172.19.74.216\"\n\n# WiFi configuration (layereight.wifi role)\nwifi_ssid: \"bleh\"\nwifi_psk: \"{{ vault_wifi_psk_bleh }}\"\n\n# Static IP configuration (michaelrigart.interfaces role)\ninterfaces_ether_interfaces:\n  - device: wlan0\n    allowclass: allow-hotplug\n    bootproto: static\n    address: \"172.19.74.216\"\n    netmask: \"255.255.255.0\"\n    gateway: 172.19.74.1\n    dnsnameservers: 172.19.74.1\n    dnssearch: oneill.net\n```\n\n**Note:** Uses both layereight.wifi (for WPA configuration) and michaelrigart.interfaces (for static IP). References WiFi PSK variable from main.yaml.\n\n### Task 1.3: Create pantrypi IPv4 Host Variables\n\n**File:** `ansible/host_vars/pantrypi.yaml` (NEW)\n\n```yaml\n---\n# Enable IP forwarding for Matter/Thread border router (infra-bre)\nip_forwarding_enabled: true\n\n# Network configuration\nmanage_network: true\nhost_ipv4: \"172.19.74.120\"\n\ninterfaces_ether_interfaces:\n  - device: eth0\n    bootproto: static\n    address: \"172.19.74.120\"\n    netmask: \"255.255.255.0\"\n    gateway: 172.19.74.1\n    dnsnameservers: 172.19.74.1\n    dnssearch: oneill.net\n    allowclass: allow-hotplug\n```\n\n### Task 1.4: Evaluate garagepi Deployment with Multiple Subagents\n\n**⚠️⚠️⚠️ CRITICAL - WiFi-ONLY DEVICE ⚠️⚠️⚠️**\n**If we break the network configuration, garagepi requires physical SD card access to fix**\n\n**DO NOT PROCEED** to Task 1.6 (garagepi deployment) until this evaluation is complete and approved.\n\nUse MULTIPLE Plan subagents (minimum 3) to evaluate the garagepi deployment approach in parallel. Each subagent should analyze:\n\n1. **Risk Assessment**:\n   - What are the specific risks of this WiFi configuration change?\n   - What could go wrong during the Ansible deployment?\n   - How likely is each failure mode?\n\n2. **Rollback Strategy**:\n   - If deployment fails and SSH is lost, what are our recovery options?\n   - Do we need to take a backup before proceeding?\n   - What's the fastest way to recover if we lose connectivity?\n\n3. **Safety Improvements**:\n   - Is there a safer way to test this configuration first?\n   - Should we test the config on a different device first?\n   - Can we validate the configuration without applying it?\n\n4. **Alternative Approaches**:\n   - Should we manually configure and test before Ansible?\n   - Is there a way to test the Ansible config in dry-run mode?\n\n**Required:** Review output from ALL subagents, synthesize their recommendations, and get user approval before proceeding.\n\n### Task 1.5: Evaluate pantrypi Deployment with Multiple Subagents\n\n**⚠️ CRITICAL - PRODUCTION DEVICE ⚠️**\n**Same risk as garagepi - requires physical SD card access if network breaks**\n\n**DO NOT PROCEED** to Task 1.7 (pantrypi deployment) until this evaluation is complete and approved.\n\nUse same multiple-subagent approach as Task 1.4 to evaluate pantrypi deployment risks and safety measures.\n\n### Task 1.6: Deploy IPv4 Configuration to garagepi\n\n**⚠️ ONLY PROCEED AFTER TASK 1.5 EVALUATION IS APPROVED ⚠️**\n**⚠️ REQUIRES EXPLICIT USER PERMISSION ⚠️**\n\n```bash\ncd ansible\nansible-playbook -l garagepi site.yaml --tags network\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh garagepi.oneill.net \"ip addr show wlan0 | grep 'inet '\"\n# Expected: 172.19.74.216/24\nssh garagepi.oneill.net \"iwconfig wlan0 | grep ESSID\"\n# Expected: ESSID:\"bleh\"\n```\n\n### Task 1.7: Deploy IPv4 Configuration to pantrypi\n\n**⚠️ ONLY PROCEED AFTER TASK 1.6 EVALUATION IS APPROVED ⚠️**\n**⚠️ REQUIRES EXPLICIT USER PERMISSION ⚠️**\n\n```bash\ncd ansible\nansible-playbook -l pantrypi site.yaml --tags network\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh pantrypi.oneill.net \"ip addr show eth0 | grep 'inet '\"\n# Expected: 172.19.74.120/24\n```\n\n### Task 1.8: Commit IPv4 Network Management\n\n```bash\ngit add ansible/host_vars/garagepi.yaml\ngit add ansible/host_vars/pantrypi.yaml\ngit add ansible/site.yaml\n\ngit commit -m \"Add IPv4 network management for RPis\n\n- Configure garagepi with static WiFi (SSID: bleh, encrypted PSK)\n- Configure pantrypi with static Ethernet\n- Both hosts now fully managed by Ansible\n- garagepi uses layereight.wifi + michaelrigart.interfaces together\"\n```\n\n---\n\n## Phase 2: Ansible Sysctl Consolidation\n\n**⚠️ CRITICAL SSH RESTRICTION ⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**You MUST NOT make any changes via SSH without explicit user approval**\n**MUST obtain explicit permission before running ANY Ansible deployment commands**\n\n**Goal:** Consolidate all IP forwarding and IPv6 hardening into the common role\n\n### Task 2.1: Add IPv6 Network Hardening Sysctls\n\n**File:** `ansible/roles/common/tasks/main.yaml`\n\nAdd after existing sysctl tasks (around line 123):\n\n```yaml\n- name: Configure IPv6 network hardening sysctls\n  ansible.builtin.copy:\n    dest: /etc/sysctl.d/99-network.conf\n    owner: root\n    group: root\n    mode: \"0644\"\n    content: |\n      # IPv6 network hardening\n      # Disable SLAAC (using static configuration)\n      net.ipv6.conf.all.autoconf=0\n      net.ipv6.conf.default.autoconf=0\n\n      # Disable Router Advertisements (prevent rogue RA attacks)\n      net.ipv6.conf.all.accept_ra=0\n      net.ipv6.conf.default.accept_ra=0\n\n      # Disable IPv6 redirects (security)\n      net.ipv6.conf.all.accept_redirects=0\n      net.ipv6.conf.default.accept_redirects=0\n  notify: reload sysctl\n\n- name: Configure IP forwarding sysctls\n  ansible.builtin.copy:\n    dest: /etc/sysctl.d/99-forwarding.conf\n    owner: root\n    group: root\n    mode: \"0644\"\n    content: |\n      # Enable IPv4 and IPv6 forwarding\n      net.ipv4.ip_forward=1\n      net.ipv6.conf.all.forwarding=1\n  when: ip_forwarding_enabled | default(false) | bool\n  notify: reload sysctl\n```\n\n### Task 2.2: Add ip_forwarding_enabled to Kubernetes Group\n\n**File:** `ansible/group_vars/kubernetes.yaml`\n\nAdd at end of file:\n\n```yaml\n# Enable IP forwarding for Kubernetes nodes (CNI requirement)\nip_forwarding_enabled: true\n```\n\n**Note:**\n- pantrypi already has `ip_forwarding_enabled: true` set in Phase 1\n- flux already has it set in existing host_vars (line 6)\n\n### Task 2.3: Remove IPv4 Forwarding from kubeadm Role\n\n**File:** `ansible/roles/kubeadm/tasks/common.yaml`\n\nRemove the sysctl task (around lines 7-18) that creates `/etc/sysctl.d/99-kubernetes.conf`\n\n### Task 2.4: Remove Tailscale Forwarding from site.yaml\n\n**File:** `ansible/site.yaml`\n\nRemove the forwarding task and handler:\n- Task: \"Configure sysctl for tailscale ip forwarding\"\n- Handler: \"Reload sysctl for tailscale ip forwarding\"\n\n### Task 2.5: Test Common Role on luser\n\n```bash\ncd ansible\nansible-playbook -l luser site.yaml --check --diff\n```\n\n**Expected:** Create `/etc/sysctl.d/99-network.conf`, no forwarding file (luser doesn't forward)\n\n### Task 2.6: Deploy Common Role to luser\n\n```bash\nansible-playbook -l luser site.yaml\n```\n\n**Verification:**\n```bash\nssh luser \"sysctl net.ipv6.conf.all.accept_ra net.ipv6.conf.all.use_tempaddr net.ipv6.conf.all.autoconf\"\n```\n\n**Expected:**\n```\nnet.ipv6.conf.all.accept_ra = 0\nnet.ipv6.conf.all.use_tempaddr = 0\nnet.ipv6.conf.all.autoconf = 0\n```\n\n### Task 2.7: Deploy to Additional Test Hosts\n\nDeploy to infra1 as additional validation before deploying to all hosts:\n\n```bash\ncd ansible\nansible-playbook -l infra1 site.yaml\n```\n\n**Verification:**\n```bash\n# Verify forwarding is enabled on infra1\nssh infra1 \"sysctl net.ipv4.ip_forward net.ipv6.conf.all.forwarding\"\n# Expected: Both show = 1\n\n# Verify IPv6 hardening is applied\nssh infra1 \"sysctl net.ipv6.conf.all.accept_ra net.ipv6.conf.all.autoconf\"\n# Expected: Both show = 0\n```\n\n### Task 2.8: Deploy to All Remaining Hosts\n\nAfter validating on luser and infra1, deploy to all remaining hosts:\n\n```bash\ncd ansible\n# Deploy to all hosts except luser and infra1 (already deployed)\nansible-playbook site.yaml --limit '!luser:!infra1'\n```\n\n**Affected hosts:**\n- **With forwarding enabled**: k1-k4, pantrypi (5 hosts receiving IPv6) + flux (already configured, not receiving IPv6)\n- **Without forwarding**: p1-p4, garagepi, gasherbrum, voron2 (~8 hosts)\n\n**Verification - Sample hosts:**\n```bash\n# Check forwarding hosts\nfor host in k1 pantrypi flux; do\n  echo \"=== $host ===\"\n  ssh $host \"sysctl net.ipv4.ip_forward net.ipv6.conf.all.forwarding\"\ndone\n# Expected: All show = 1\n\n# Check non-forwarding hosts\nfor host in p1 garagepi; do\n  echo \"=== $host ===\"\n  ssh $host \"sysctl net.ipv6.conf.all.accept_ra\"\ndone\n# Expected: All show = 0\n```\n\n### Task 2.9: Cleanup Old Sysctl Files\n\nRemove old sysctl configuration files from all hosts:\n\n```bash\ncd ansible\n\n# Remove old kubernetes sysctl files\nansible all -m file -a \"path=/etc/sysctl.d/99-kubernetes.conf state=absent\"\nansible all -m file -a \"path=/etc/sysctl.d/99-kubernetes-cri.conf state=absent\"\n\n# Reload sysctl on all hosts\nansible all -m command -a \"sysctl --system\"\n```\n\n### Task 2.10: Commit Ansible Sysctl Changes\n\n```bash\ngit add ansible/roles/common/tasks/main.yaml\ngit add ansible/group_vars/kubernetes.yaml\ngit add ansible/roles/kubeadm/tasks/common.yaml\ngit add ansible/site.yaml\n\ngit commit -m \"Consolidate IP forwarding and IPv6 hardening sysctls\n\n- Move all network sysctls to common role\n- Add unified IPv6 hardening (disable SLAAC, RA, privacy extensions)\n- Use ip_forwarding_enabled variable for both IPv4+IPv6\n- Remove duplicate forwarding from kubeadm role and site.yaml\n- Add ip_forwarding_enabled to kubernetes group\"\n```\n\n---\n\n## Phase 3: Ansible IPv6 Configuration\n\n**⚠️ CRITICAL SSH RESTRICTION ⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**You MUST NOT make any changes via SSH without explicit user approval**\n**This phase only modifies Ansible configuration files - NO deployment yet**\n\n**Goal:** Configure static IPv6 addresses on all infrastructure hosts\n\n### Task 3.1: Create Global IPv6 Variables\n\n**File:** `ansible/group_vars/all/ipv6.yaml` (NEW)\n\n```yaml\n---\n# IPv6 configuration for Default VLAN (172.19.74.0/24)\n\ninfrastructure_ipv6_prefix: \"2600:4040:2ece:7500\"\ninfrastructure_ipv6_gateway: \"2600:4040:2ece:7500::1\"\ninfrastructure_ipv6_netmask: \"64\"\n```\n\n### Task 3.2: Update Proxmox Network Template\n\n**File:** `ansible/roles/proxmox/templates/interfaces.j2`\n\nAdd inet6 stanza after each bridge's inet block (around line 26):\n\n```jinja2\n{% for bridge in proxmox_bridges %}\nauto {{ bridge.name }}\niface {{ bridge.name }} inet static\n{% if bridge.address is defined %}\taddress {{ bridge.address }}\n{% endif %}\n{% if bridge.gateway is defined %}\tgateway {{ bridge.gateway }}\n{% endif %}\n\tbridge-ports {{ bridge.bridge_ports }}\n\tbridge-stp {{ bridge.bridge_stp }}\n\tbridge-fd {{ bridge.bridge_fd }}\n{% if bridge.comment is defined %}\t# {{ bridge.comment }}\n{% endif %}\n\n# IPv6 configuration (derived from IPv4)\n{% if bridge.address is defined %}\n{% set ipv4 = bridge.address.split('/')[0] %}\niface {{ bridge.name }} inet6 static\n\taddress {{ infrastructure_ipv6_prefix }}::74:{{ ipv4.split('.')[3] }}/{{ infrastructure_ipv6_netmask }}\n\tgateway {{ infrastructure_ipv6_gateway }}\n{% endif %}\n\n{% endfor %}\n```\n\n### Task 3.3: Add IPv6 to infra1\n\n**File:** `ansible/host_vars/infra1.yaml`\n\nAdd at top:\n\n```yaml\nhost_ipv4: \"172.19.74.31\"\n```\n\nAdd to interfaces_ether_interfaces device:\n\n```yaml\n    ip6:\n      address: \"{{ infrastructure_ipv6_prefix }}::74:{{ host_ipv4.split('.')[3] }}\"\n      prefix: \"{{ infrastructure_ipv6_netmask }}\"\n      gateway: \"{{ infrastructure_ipv6_gateway }}\"\n```\n\n### Task 3.4: Add IPv6 to luser\n\n**File:** `ansible/host_vars/luser.yaml`\n\nSame pattern as infra1, with `host_ipv4: \"172.19.74.161\"`\n\n### Task 3.5: Add IPv6 to k1\n\n**File:** `ansible/host_vars/k1.yaml`\n\nSame pattern, with `host_ipv4: \"172.19.74.134\"`\n\n### Task 3.6: Add IPv6 to k2\n\n**File:** `ansible/host_vars/k2.yaml`\n\nSame pattern, with `host_ipv4: \"172.19.74.112\"`\n\n### Task 3.7: Add IPv6 to k3\n\n**File:** `ansible/host_vars/k3.yaml`\n\nSame pattern, with `host_ipv4: \"172.19.74.74\"`\n\n### Task 3.8: Add IPv6 to k4\n\n**File:** `ansible/host_vars/k4.yaml`\n\nSame pattern, with `host_ipv4: \"172.19.74.75\"`\n\n### Task 3.9: Add IPv6 to garagepi\n\n**File:** `ansible/host_vars/garagepi.yaml`\n\nAdd ip6 configuration to the existing interfaces_ether_interfaces device:\n\n```yaml\n---\nmanage_network: true\nhost_ipv4: \"172.19.74.216\"\n\n# WiFi configuration (layereight.wifi role)\nwifi_ssid: \"bleh\"\nwifi_psk: \"{{ vault_wifi_psk_bleh }}\"\n\n# Static IP configuration (michaelrigart.interfaces role)\ninterfaces_ether_interfaces:\n  - device: wlan0\n    allowclass: allow-hotplug\n    bootproto: static\n    address: \"172.19.74.216\"\n    netmask: \"255.255.255.0\"\n    gateway: 172.19.74.1\n    dnsnameservers: 172.19.74.1\n    dnssearch: oneill.net\n    ip6:\n      address: \"{{ infrastructure_ipv6_prefix }}::74:{{ host_ipv4.split('.')[3] }}\"\n      prefix: \"{{ infrastructure_ipv6_netmask }}\"\n      gateway: \"{{ infrastructure_ipv6_gateway }}\"\n```\n\n### Task 3.10: Add IPv6 to pantrypi\n\n**File:** `ansible/host_vars/pantrypi.yaml`\n\nAdd ip6 configuration to the existing interfaces_ether_interfaces device:\n\n```yaml\ninterfaces_ether_interfaces:\n  - device: eth0\n    bootproto: static\n    address: \"172.19.74.120\"\n    netmask: \"255.255.255.0\"\n    gateway: 172.19.74.1\n    dnsnameservers: 172.19.74.1\n    dnssearch: oneill.net\n    allowclass: allow-hotplug\n    ip6:\n      address: \"{{ infrastructure_ipv6_prefix }}::74:{{ host_ipv4.split('.')[3] }}\"\n      prefix: \"{{ infrastructure_ipv6_netmask }}\"\n      gateway: \"{{ infrastructure_ipv6_gateway }}\"\n```\n\n### Task 3.11: Validate Ansible Configuration\n\n```bash\ncd ansible\nansible-playbook --syntax-check site.yaml\n```\n\n**Note:** Do not commit yet - commit will happen after successful deployment in Phase 4.\n\n---\n\n## Phase 4: Phased Deployment \u0026 External Validation\n\n**⚠️⚠️⚠️ CRITICAL SSH AND DEPLOYMENT RESTRICTION ⚠️⚠️⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**DEPLOYMENT COMMANDS REQUIRE EXPLICIT USER PERMISSION FOR EACH HOST**\n**YOU MUST ASK FOR PERMISSION BEFORE EVERY ansible-playbook COMMAND**\n**YOU MUST ASK FOR PERMISSION BEFORE EVERY kubectl COMMAND**\n**NEVER assume permission carries over from one host to another**\n**Read-only SSH validation commands are allowed (ip addr, ping, sysctl)**\n\n**Goal:** Deploy IPv6 incrementally with validation at each step, including external security testing\n\n**Deployment Order:**\n1. infra1 (physical server) - FIRST for external validation\n2. External validation from flux - IMMEDIATELY after infra1\n3. luser (test VM)\n4. p4, p1, p2, p3 (Proxmox hosts, one at a time)\n5. garagepi, pantrypi (Raspberry Pis)\n6. k1, k2, k3, k4 (Kubernetes nodes, cordon/drain each)\n\n### Task 4.1: Deploy to infra1 (FIRST)\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval before running ansible-playbook\n\n```bash\ncd ansible\nansible-playbook -l infra1 site.yaml\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh infra1 \"ip -6 addr show\"\n# Expected: 2600:4040:2ece:7500::74:31/64\n\nssh infra1 \"ping6 -c 3 2600:4040:2ece:7500::1\"\n# Expected: Success\n\nssh infra1 \"ping6 -c 3 google.com\"\n# Expected: Success\n\n# Verify IPv4 still works\nping -c 3 infra1.oneill.net\n# Expected: Success\n```\n\n**CRITICAL:** Proceed IMMEDIATELY to Task 4.2 for external validation before deploying to any other hosts.\n\n### Task 4.2: External IPv6 Validation from flux (IMMEDIATE)\n\n**⚠️ CRITICAL - READ-ONLY SSH**: Testing firewall from external host\n\nTest external IPv6 access from flux to verify UDMP firewall is blocking:\n\n```bash\n# SSH to flux (external VPS with IPv6) - READ-ONLY\nssh flux.oneill.net\n\n# Test infra1 (first deployed host)\nping6 -c 3 infra1.oneill.net\n\n# Scan for open ports\nnmap -6 infra1.oneill.net -p 22,111,2049,8080\n```\n\n**Expected:**\n- **ICMP**: Should be BLOCKED except on WAN interface of UDMP\n- **SSH and services**: Should be BLOCKED by UDMP firewall for all interfaces\n- **If services are accessible from external**: STOP immediately and review UniFi firewall rules\n\n**Action:** Do NOT proceed to Task 4.3 until firewall blocking is confirmed working correctly.\n\n### Task 4.3: Deploy to luser\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval before running ansible-playbook\n\n```bash\ncd ansible\nansible-playbook -l luser site.yaml\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh luser \"ip -6 addr show dev ens18\"\n# Expected: 2600:4040:2ece:7500::74:161/64\n\nssh luser \"ping6 -c 3 2600:4040:2ece:7500::1\"\n# Expected: Success\n\nssh luser \"ping6 -c 3 google.com\"\n# Expected: Success\n\n# Verify IPv4 still works\nping -c 3 luser.oneill.net\n# Expected: Success\n```\n\n### Task 4.4: Deploy to Proxmox (p4 first)\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval before running ansible-playbook\n\n```bash\n# Evacuate VMs from p4 if critical\nssh p4 \"qm list\"\n# Migrate if needed\n\nansible-playbook -l p4 site.yaml\n```\n\n**Validation:**\n```bash\nssh p4 \"ip -6 addr show dev vmbr0; ping6 -c 3 google.com\"\n```\n\n### Task 4.5: Deploy to Remaining Proxmox Hosts\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval for EACH host before deploying\n\n```bash\nfor host in p1 p2 p3; do\n  echo \"=== REQUEST PERMISSION TO DEPLOY TO $host ===\"\n  echo \"Press Enter after getting user permission...\"\n  read\n\n  echo \"=== Deploying to $host ===\"\n  ansible-playbook -l $host site.yaml\n\n  echo \"=== Validating $host (READ-ONLY SSH) ===\"\n  ssh $host \"ip -6 addr show; ping6 -c 2 google.com\"\n\n  echo \"Press Enter to continue to next host...\"\n  read\ndone\n```\n\n### Task 4.6: Deploy to Raspberry Pis\n\n**⚠️⚠️⚠️ EXTREMELY CRITICAL - WiFi-ONLY CONNECTIVITY ⚠️⚠️⚠️**\n**Breaking network config requires physical SD card access to fix**\n**MUST get explicit user permission before EACH deployment**\n\n**Note:** Multiple subagent evaluation completed in Phase 1 Tasks 1.5-1.6 before proceeding here.\n\n```bash\n# garagepi (WiFi-only) - GET EXPLICIT PERMISSION FIRST\necho \"=== REQUEST PERMISSION TO DEPLOY TO garagepi ===\"\necho \"Press Enter after getting user permission...\"\nread\nansible-playbook -l garagepi site.yaml\n\n# pantrypi (Ethernet) - GET EXPLICIT PERMISSION FIRST\necho \"=== REQUEST PERMISSION TO DEPLOY TO pantrypi ===\"\necho \"Press Enter after getting user permission...\"\nread\nansible-playbook -l pantrypi site.yaml\n```\n\n**Validation (READ-ONLY SSH):**\n```bash\nssh garagepi \"ip -6 addr show dev wlan0; ping6 -c 3 google.com\"\nssh pantrypi \"ip -6 addr show dev eth0; ping6 -c 3 google.com\"\n```\n\n### Task 4.7: Deploy to Kubernetes Nodes\n\n**⚠️ REQUIRES EXPLICIT PERMISSION**: Must get user approval for kubectl commands AND ansible deployment for EACH node\n\n```bash\nfor node in k1 k2 k3 k4; do\n  echo \"=== REQUEST PERMISSION FOR $node ===\"\n  echo \"Press Enter after getting user permission for kubectl + ansible...\"\n  read\n\n  echo \"=== $node: Cordon and drain ===\"\n  kubectl cordon $node\n  kubectl drain $node --ignore-daemonsets --delete-emptydir-data\n\n  echo \"=== $node: Deploy IPv6 ===\"\n  ansible-playbook -l $node site.yaml\n\n  echo \"=== $node: Validate (READ-ONLY SSH) ===\"\n  ssh $node \"ip -6 addr show; ping6 -c 2 google.com\"\n\n  echo \"=== $node: Uncordon ===\"\n  kubectl uncordon $node\n  kubectl wait --for=condition=Ready node/$node --timeout=120s\n\n  echo \"Press Enter to continue to next node...\"\n  read\ndone\n```\n\n### Task 4.8: External IPv6 Validation from flux\n\nTest external IPv6 access from flux to verify UDMP firewall is blocking:\n\n```bash\n# SSH to flux (external VPS with IPv6)\nssh flux.oneill.net\n\n# Use luser as test host (first deployed, has IPv6)\n# Test connectivity\nping6 -c 3 luser.oneill.net\n\n# Scan for open ports\nnmap -6 luser.oneill.net -p 22,80,443,8006\n\n# Also test other hosts\nnmap -6 infra1.oneill.net -p 22,111,2049\nnmap -6 p1.oneill.net -p 22,8006\n```\n\n**Expected:**\n- ICMP should work (or be blocked by firewall - document which)\n- SSH and other services should be BLOCKED by UDMP firewall\n- If services are accessible, STOP and review firewall rules\n\n**Action:** Document results. If firewall not blocking as expected, update UniFi rules before proceeding.\n\n### Task 4.9: Commit Ansible IPv6 Configuration and Deployment\n\nAfter all hosts deployed and external validation complete:\n\n```bash\ngit add ansible/group_vars/all/main.yaml\ngit add ansible/group_vars/all/ipv6.yaml\ngit add ansible/roles/proxmox/templates/interfaces.j2\ngit add ansible/host_vars/infra1.yaml\ngit add ansible/host_vars/luser.yaml\ngit add ansible/host_vars/k{1,2,3,4}.yaml\ngit add ansible/host_vars/garagepi.yaml\ngit add ansible/host_vars/pantrypi.yaml\n\ngit commit -m \"Add IPv6 static configuration to all infrastructure hosts\n\nConfiguration changes:\n- Global IPv6 variables (prefix, gateway, netmask)\n- Proxmox template with IPv6 derivation from IPv4\n- Static IPv6 for 12 hosts using formula ::74:last_octet\n- WiFi PSK encrypted variable for garagepi\n\nDeployment validated:\n- 4 Proxmox: p1-p4\n- 4 Kubernetes: k1-k4\n- 2 RPis: garagepi (WiFi), pantrypi (Ethernet)\n- 2 servers: infra1, luser\n\nAll hosts verified:\n- IPv6 connectivity working (gateway + internet)\n- IPv4 maintained\n- External firewall blocking confirmed via nmap from flux\"\n```\n\n---\n\n## Phase 5: Terraform DNS Changes\n\n**⚠️ CRITICAL SSH RESTRICTION ⚠️**\n**SSH ACCESS IS READ-ONLY UNLESS EXPLICITLY PERMITTED BY USER**\n**MUST obtain explicit permission before running tofu apply commands**\n**Read-only validation commands are allowed (dig, tofu plan)**\n\n**Goal:** Add AAAA records for all 12 hosts (11 Ansible + flux)\n\n**CRITICAL:** Execute this phase LAST, after all hosts have working IPv6\n\n### Task 5.1: Add IPv6 Prefix to Terraform\n\n**File:** `opentofu/locals.tf`\n\nAdd after line 16:\n\n```hcl\nlocals {\n  # IPv6 prefix for Default VLAN (172.19.74.0/24)\n  infrastructure_ipv6_prefix = \"2600:4040:2ece:7500\"\n\n  infrastructure_hosts = {\n```\n\n### Task 5.2: Add enable_ipv6 to Excluded Hosts\n\n**File:** `opentofu/locals.tf`\n\nAdd `enable_ipv6 = false` to:\n- p2-amt, p3-amt, p4-amt (AMT interfaces)\n- fs2 (Synology NAS)\n- szamar (Desktop)\n\n### Task 5.3: Remove pantrypi-wifi\n\n**File:** `opentofu/locals.tf`\n\nDelete the entire pantrypi-wifi entry (~lines 126-131)\n\n### Task 5.4: Pass IPv6 Prefix to DNS Module\n\n**File:** `opentofu/main.tf`\n\nUpdate module call (around line 124):\n\n```hcl\nmodule \"dns\" {\n  source                      = \"./modules/dns\"\n  infrastructure_hosts        = local.infrastructure_hosts\n  infrastructure_ipv6_prefix  = local.infrastructure_ipv6_prefix\n}\n```\n\n### Task 5.5: Update DNS Module Variables\n\n**File:** `opentofu/modules/dns/variables.tf`\n\nAdd enable_ipv6 field and prefix variable:\n\n```hcl\nvariable \"infrastructure_hosts\" {\n  type = map(object({\n    mac         = string\n    ip          = string\n    hostname    = string\n    note        = string\n    public_dns  = optional(bool, true)\n    enable_ipv6 = optional(bool, true)\n  }))\n}\n\nvariable \"infrastructure_ipv6_prefix\" {\n  description = \"IPv6 /64 prefix for infrastructure hosts\"\n  type        = string\n}\n```\n\n### Task 5.6: Compute IPv6 in DNS Module\n\n**File:** `opentofu/modules/dns/main.tf`\n\nAdd to locals (after nameserver_ips):\n\n```hcl\nlocals {\n  nameserver_ips = {\n    # ... existing\n  }\n\n  # Compute IPv6 addresses\n  infrastructure_hosts_with_ipv6 = {\n    for name, host in var.infrastructure_hosts : name =\u003e merge(host, {\n      ipv6 = host.enable_ipv6 ? format(\n        \"%s::74:%s\",\n        var.infrastructure_ipv6_prefix,\n        split(\".\", host.ip)[3]\n      ) : null\n    })\n  }\n}\n```\n\n### Task 5.7: Create AAAA Records\n\n**File:** `opentofu/modules/dns/oneill.tf`\n\nUpdate A record source (around line 141):\n\n```hcl\nresource \"aws_route53_record\" \"infrastructure_hosts\" {\n  for_each = { for k, v in local.infrastructure_hosts_with_ipv6 : k =\u003e v if v.public_dns }\n\n  zone_id = aws_route53_zone.oneill_net.zone_id\n  name    = each.value.hostname\n  type    = \"A\"\n  ttl     = 300\n  records = [each.value.ip]\n}\n```\n\nAdd AAAA records after A records:\n\n```hcl\nresource \"aws_route53_record\" \"infrastructure_hosts_aaaa\" {\n  for_each = {\n    for k, v in local.infrastructure_hosts_with_ipv6 : k =\u003e v\n    if v.public_dns \u0026\u0026 v.ipv6 != null\n  }\n\n  zone_id = aws_route53_zone.oneill_net.zone_id\n  name    = each.value.hostname\n  type    = \"AAAA\"\n  ttl     = 300\n  records = [each.value.ipv6]\n}\n```\n\n### Task 5.8: Plan Terraform Changes\n\n```bash\ncd opentofu\ntofu plan -out=ipv6.tfplan\n```\n\n**Expected:**\n- 12 AAAA records to add (11 Ansible + flux)\n- 2 resources to destroy (pantrypi-wifi: unifi_user, aws_route53_record)\n\n### Task 5.9: Apply Terraform Changes\n\n```bash\ntofu apply ipv6.tfplan\n```\n\n**Verification:**\n```bash\n# Check AAAA records\ndig AAAA p1.oneill.net @172.19.74.1\ndig AAAA k1.oneill.net @172.19.74.1\ndig AAAA flux.oneill.net @172.19.74.1\n\n# Verify luser has NO AAAA (public_dns=false)\ndig AAAA luser.oneill.net @172.19.74.1\n# Expected: NXDOMAIN or NOERROR with no AAAA section\n\n# Verify pantrypi-wifi is gone\ndig A pantrypi-wifi.oneill.net @172.19.74.1\n# Expected: NXDOMAIN\n```\n\n### Task 5.10: Commit Terraform Changes\n\n```bash\ngit add opentofu/locals.tf opentofu/main.tf opentofu/modules/dns/\ngit commit -m \"Add IPv6 AAAA records and remove pantrypi-wifi\n\n- Add enable_ipv6 optional field (defaults to true)\n- Compute IPv6 from IPv4 in DNS module (formula: ::74:last_octet)\n- Create 12 AAAA records (11 Ansible-managed + flux)\n- Exclude 5 hosts with enable_ipv6=false\n- Remove pantrypi-wifi (duplicate interface)\n- TTL matches A records (300 seconds)\"\n```\n\n### Task 5.11: Create Pull Request\n\nPush the IPv6 feature branch and create a PR:\n\n```bash\ngit push -u origin ipv6-infrastructure-enablement\ngh pr create --fill\n```\n\n**Note:** This PR includes all changes from Phases 1-5:\n- RPi network management (Phase 1)\n- Sysctl consolidation (Phase 2)\n- Ansible IPv6 configuration (Phase 3)\n- Deployment to all 12 hosts (Phase 4)\n- Terraform DNS changes (Phase 5)\n\n---\n\n## Success Criteria\n\n**Infrastructure Level (IN SCOPE):**\n- ✅ k4 restored to Ansible inventory and brought up to baseline (Phase 0, separate PR)\n- ✅ 12 hosts have static IPv6 addresses configured via Ansible (p1-p4, k1-k4, infra1, luser, garagepi, pantrypi)\n- ✅ flux has existing IPv6 (Vultr VPS)\n- ✅ 12 AAAA records created in Route53 (11 Ansible-managed + flux, excludes luser)\n- ✅ IPv6 connectivity verified (gateway + internet)\n- ✅ IPv4 connectivity maintained\n- ✅ Sysctl hardening applied (SLAAC/RA disabled)\n- ✅ IP forwarding enabled for k1-k4, infra1, pantrypi\n\n**Out of Scope:**\n- ❌ Kubernetes pod dual-stack (infra-2wn)\n- ❌ ESPHome devices IPv6 (different VLAN)\n- ❌ fs2, szamar IPv6 (not managed by Ansible)\n\n---\n\n## Files Modified Summary\n\n**Terraform (5 files):**\n1. `opentofu/locals.tf` - Add IPv6 prefix, enable_ipv6 field, remove pantrypi-wifi\n2. `opentofu/main.tf` - Pass prefix to DNS module\n3. `opentofu/modules/dns/variables.tf` - Add IPv6 variables\n4. `opentofu/modules/dns/main.tf` - Compute IPv6 addresses\n5. `opentofu/modules/dns/oneill.tf` - Create AAAA records\n\n**Ansible (16 files):**\n1. `ansible/site.yaml` - Remove Tailscale forwarding (keep layereight.wifi role for garagepi)\n2. `ansible/group_vars/all/main.yaml` - Add vault_wifi_psk_bleh encrypted variable\n3. `ansible/group_vars/all/ipv6.yaml` - NEW: Global IPv6 variables\n4. `ansible/group_vars/kubernetes.yaml` - Add ip_forwarding_enabled\n5. `ansible/roles/common/tasks/main.yaml` - Add network sysctl tasks (no tags, removed use_tempaddr default)\n6. `ansible/roles/proxmox/templates/interfaces.j2` - Add inet6 stanza\n7. `ansible/roles/kubeadm/tasks/common.yaml` - Remove IPv4 forwarding\n8. `ansible/host_vars/infra1.yaml` - Add host_ipv4, ip6\n9. `ansible/host_vars/luser.yaml` - Add host_ipv4, ip6\n10. `ansible/host_vars/k1.yaml` - Add host_ipv4, ip6\n11. `ansible/host_vars/k2.yaml` - Add host_ipv4, ip6\n12. `ansible/host_vars/k3.yaml` - Add host_ipv4, ip6\n13. `ansible/host_vars/k4.yaml` - Add host_ipv4, ip6\n14. `ansible/host_vars/garagepi.yaml` - NEW: IPv4 WiFi static with PSK variable reference (Phase 1), add IPv6 (Phase 3)\n15. `ansible/host_vars/pantrypi.yaml` - NEW: IPv4 Ethernet static + IP forwarding (Phase 1), add IPv6 (Phase 3)\n16. **Note:** flux already has ip_forwarding_enabled, no changes needed\n\n---\n\n## Rollback Procedures\n\n**Per-Host Rollback:**\n```bash\n# Remove ip6 section from host_vars, redeploy\nansible-playbook -l \u003chost\u003e site.yaml\n```\n\n**Terraform Rollback:**\n```bash\ncd opentofu\ngit revert \u003ccommit-hash\u003e\ntofu apply\n```\n\n---\n\n**Plan complete. Ready for review and approval.**","status":"open","priority":3,"issue_type":"epic","owner":"clayton@oneill.net","created_at":"2026-01-22T23:28:25.225645-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:28:25.225645-05:00"}
{"id":"infra-ecp","title":"Generic Renovate Comment Annotations for Docker Images","description":"Create a single generic regex manager for Docker images using comment annotations. All images use full `name:tag` or `name:tag@digest` format (digest optional), enabling uniform handling across YAML variables and docker-compose files.\n\n## Scope\n\n**This PR:** Generic regex manager + migrate all 8 images\n**Follow-up PR:** Add automerge rule after confirming Renovate detection works\n\n## New Format\n\n```yaml\n# renovate: datasource=docker\nvariable_or_image: \"imagename:tag@sha256:digest\"\n```\n\n- The `depName` is extracted from the value, not the comment\n- Works with or without digest (digest is optional)\n- Works for any YAML file repo-wide (only matches files with the comment)\n- Kubernetes images already have native Renovate support (kustomize, helm managers)\n- Dockerfiles already have native Renovate support (dockerfile manager)\n- Third-party code in `ansible/galaxy/` and `.tmp/` excluded from hooks\n\nReferences: infra-5x8","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-06T11:28:16.478396-05:00","created_by":"coneill","updated_at":"2026-01-06T18:54:15.144826-05:00","closed_at":"2026-01-06T18:54:15.144826-05:00","close_reason":"Completed - recursive strategy fix merged, automerge rule in PR #1086","dependencies":[{"issue_id":"infra-ecp","depends_on_id":"infra-9uv","type":"blocks","created_at":"2026-01-06T11:46:39.217509-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-e5p","type":"blocks","created_at":"2026-01-06T11:46:39.268461-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-o3x","type":"blocks","created_at":"2026-01-06T11:46:39.318463-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-578","type":"blocks","created_at":"2026-01-06T11:46:39.369609-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-7c9","type":"blocks","created_at":"2026-01-06T11:46:39.419233-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-cza","type":"blocks","created_at":"2026-01-06T11:46:39.469617-05:00","created_by":"coneill"},{"issue_id":"infra-ecp","depends_on_id":"infra-6m4","type":"blocks","created_at":"2026-01-06T11:46:39.520816-05:00","created_by":"coneill"}]}
{"id":"infra-eew","title":"Evaluate PR #1132: Update nginx:1.29-alpine Docker digest to c083c37","description":"Run the renovate-eval skill against PR #1132 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.425035-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:34.005386-05:00","closed_at":"2026-01-12T18:48:34.005386-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-eew","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.12304-05:00","created_by":"coneill"}]}
{"id":"infra-egs","title":"Test 3: Automatic Trigger - All Hosts","description":"Test that non-host_vars changes target all hosts.\n\n## Steps\n1. Create branch from main\n2. Add harmless change to `ansible/roles/common/defaults/main.yaml`:\n   ```yaml\n   # Test comment for auto-deploy verification\n   ```\n3. Merge to main\n\n## Verification Checklist\n- [ ] ansible-detect-changes outputs `hosts=all`\n- [ ] semaphore-deploy runs without `--limit` (all hosts)\n- [ ] Task completes successfully\n- [ ] ARA URL appears\n- [ ] Verify via ARA that the deploy ran on all hosts as expected","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T22:21:40.938579-05:00","created_by":"coneill","updated_at":"2026-01-05T09:49:50.363022-05:00","closed_at":"2026-01-05T09:49:50.363022-05:00","close_reason":"Test 3 passed - role change correctly detected as requiring all hosts. Deploy ran against all hosts. Ansible ACME failure is separate issue (infra-hcr).","dependencies":[{"issue_id":"infra-egs","depends_on_id":"infra-oty","type":"blocks","created_at":"2026-01-04T22:21:53.976274-05:00","created_by":"coneill"}]}
{"id":"infra-eir","title":"Test 1: Manual Workflow Dispatch","description":"Verify manual trigger works before testing automatic trigger.\n\n## Commands\n```bash\n# Trigger workflow\ngh workflow run \"Deploy Ansible to Production\" -f hosts=k1 -f tags=hostname\n\n# Monitor the run\ngh run list --workflow=\"Deploy Ansible to Production\" --limit=1\ngh run watch  # Watch most recent run\n```\n\n## Verification Checklist\n- [ ] Workflow starts and acquires turnstyle lock\n- [ ] Tailscale connects successfully\n- [ ] Semaphore API ping succeeds\n- [ ] semaphore-deploy creates task\n- [ ] Task completes successfully\n- [ ] ARA Playbook URL appears in output\n- [ ] Job summary contains Semaphore and ARA links\n- [ ] Verify via ARA that the deploy ran on k1 with hostname tag as expected","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T22:19:17.975533-05:00","created_by":"coneill","updated_at":"2026-01-05T00:05:37.754775-05:00","closed_at":"2026-01-05T00:05:37.754775-05:00","close_reason":"Test 1 passed - workflow run 20705579196 created Semaphore task 125, completed successfully with correct external URLs","dependencies":[{"issue_id":"infra-eir","depends_on_id":"infra-oty","type":"blocks","created_at":"2026-01-04T22:19:26.511343-05:00","created_by":"coneill"}]}
{"id":"infra-frb","title":"Phase 5: Terraform DNS Changes","description":"Read the full plan and all comments in infra-ec1, then implement Phase 5.","status":"in_progress","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:32:30.032328-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-29T16:09:06.982308-05:00","dependencies":[{"issue_id":"infra-frb","depends_on_id":"infra-77h","type":"blocks","created_at":"2026-01-22T23:32:41.511237-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-gaf","title":"IPv6 Derivation Implementation in Terraform","description":"**Core Questions:**\n- Can HCL string/math functions implement the derivation formula reliably?\n- Which functions: `split()`, `tonumber()`, `format()` for hex conversion?\n- How to validate derived IPv6 addresses to prevent collisions?\n- Where to store VLAN-to-prefix mapping: locals, variables, or data structure?\n\n**Edge Cases to Investigate:**\n- Does formula work for all subnets (172.19.x.x, 172.20.x.x)?\n- How to handle broadcast/network addresses?\n- Multi-interface hosts with multiple IPs?\n\n**Critical Files:**\n- `opentofu/locals.tf` (lines 17-140) - current host definitions\n- `opentofu/modules/dns/oneill.tf` - DNS record patterns to follow\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:02.805627-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.473229-05:00","closed_at":"2026-01-22T23:32:54.473229-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1.","dependencies":[{"issue_id":"infra-gaf","depends_on_id":"infra-8sx","type":"blocks","created_at":"2026-01-20T14:07:42.456017-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-gbn","title":"External Tool Dependencies","description":"**Core Questions:**\n- Does UniFi Terraform provider support IPv6 DHCP reservations?\n- Does ESPHome need YAML changes to support IPv6?\n- IPv6 support status in community Ansible roles (tailscale, interfaces)?\n- Pre-commit hooks: IPv6 address validation capability?\n\n**Investigation Needed:**\n- Check `filipowm/unifi` provider documentation\n- Review ESPHome IPv6 documentation\n- Verify community roles are maintained and IPv6-ready\n\n**Impact:**\nIf tools don't support IPv6, may need workarounds or selective disabling.\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:22.346861-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.475501-05:00","closed_at":"2026-01-22T23:32:54.475501-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-gbs","title":"Investigate Immich photo upload failures","description":"Some photos in the photo library cannot be uploaded to Immich. Need to find existing Claude session with investigation/remediation plan using osxphotos app and evaluate next steps.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-06T20:13:26.036243-05:00","created_by":"coneill","updated_at":"2026-01-06T20:13:26.036243-05:00"}
{"id":"infra-gzw","title":"Evaluate PR #1149: Update mariadb Docker tag to v11.8","description":"Run the renovate-eval skill against PR #1149 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.207449-05:00","created_by":"coneill","updated_at":"2026-01-12T18:48:34.009402-05:00","closed_at":"2026-01-12T18:48:34.009402-05:00","close_reason":"PR merged or closed","dependencies":[{"issue_id":"infra-gzw","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.152376-05:00","created_by":"coneill"}]}
{"id":"infra-hcr","title":"Investigate ACME certificate failure on Proxmox nodes","description":"Ansible deploy fails on Proxmox nodes p1 and p3 during ACME certificate ordering.\n\n## Error Details\n- Task: \"Order ACME certificate\" \n- File: ansible/roles/proxmox/tasks/acme.yaml:74\n- Command: pvenode acme cert order\n- RC: 255\n\n## Error Message\n```\n400 Parameter verification failed.\nforce: Custom certificate exists but 'force' is not set.\n```\n\n## Affected Hosts\n- p1: failed=1\n- p3: failed=1\n\n## Context\nDiscovered during Test 3 of Semaphore auto-deploy validation (run 20718826880).\n\n## Questions to Investigate\n- Why do these nodes have custom certificates that conflict with ACME ordering?\n- When did this start failing? Was the task previously working?\n- What is the intended certificate management strategy for Proxmox nodes?\n- Is this task still needed or has certificate management changed?","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T09:49:22.592383-05:00","created_by":"coneill","updated_at":"2026-01-05T15:03:49.782374-05:00","closed_at":"2026-01-05T15:03:49.782374-05:00","close_reason":"Fixed by checking pveproxy-ssl.pem with x509_certificate_info instead of pve-ssl.pem with openssl verify"}
{"id":"infra-itg","title":"Evaluate PR #1147: Update Helm release tailscale-operator to v1.92.4","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.729617-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-iy9","title":"Evaluate PR #1144: Update Helm release argo-cd to v9.2.3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.958796-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-jhk","title":"Evaluate PR #1136: Update Helm release authentik to v2025.10.3","description":"Run the renovate-eval skill against PR #1136 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.180824-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.634563-05:00","closed_at":"2026-01-13T00:46:36.634563-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-jhk","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.12844-05:00","created_by":"coneill"}]}
{"id":"infra-knn","title":"PR #1051: Semaphore Auto-Deploy Fixes","description":"Implementation fixes required before merging PR #1051 for Semaphore auto-deploy.\n\nThe implementation is well-architected and follows established patterns from ArgoCD. However, critical Tailscale networking pieces are missing that will prevent the workflow from functioning correctly.\n\n**Issues:**\n1. Missing Tailscale ACL policy for Semaphore\n2. Missing Tailscale ingress for Semaphore\n3. ARA callback plugin race condition\n4. WebSocket error handling\n5. Hardcoded Semaphore URL\n\n**Source Plan:** /Users/coneill/.claude/plans/pure-sniffing-marble.md","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-04T12:48:48.134091-05:00","created_by":"coneill","updated_at":"2026-01-04T13:29:45.481383-05:00","closed_at":"2026-01-04T13:29:45.481383-05:00","close_reason":"All fixes implemented and tested: Tailscale ACL+ingress deployed, ARA callback restored, WebSocket error handling added, URL made configurable. Semaphore-deploy successfully tested against Tailscale ingress."}
{"id":"infra-kt0","title":"Phase 0: Preparation \u0026 k4 Restoration","description":"Read the entire plan in infra-ec1 and implement Phase 0 only.\n\nThis phase includes:\n- Add k4 back to Ansible inventory and deploy (separate PR)\n- Create IPv6 feature branch\n- Verify UniFi IPv6 configuration\n- Review current Terraform and Ansible state\n\n**CRITICAL**: Create PR for k4 restoration and wait for merge before proceeding to Phase 1.","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:28:48.117043-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-27T14:00:41.641431-05:00","closed_at":"2026-01-27T14:00:41.641431-05:00","close_reason":"k4 restored (PR #1247), IPv6 branch created, UniFi config verified, go-unifi-mcp integrated (PR #1308)","dependencies":[{"issue_id":"infra-kt0","depends_on_id":"infra-ec1","type":"blocks","created_at":"2026-01-22T23:32:41.115007-05:00","created_by":"Clayton O'Neill"}],"comments":[{"id":12,"issue_id":"infra-kt0","author":"Clayton O'Neill","text":"## Phase 0 Progress Update\n\n### Completed Tasks:\n- **0.1 k4 Restoration** ✓ - PR #1247 merged, k4 Ready in cluster\n- **0.2 IPv6 Feature Branch** ✓ - Created ipv6-infrastructure-enablement\n- **0.3 UniFi IPv6 Config** ✓ - Prefix 2600:4040:2ece:7500::/64 confirmed, k1 has SLAAC\n- **0.5 Terraform State** ✓ - Only flux AAAA exists, pantrypi-wifi to remove\n- **0.6 Ansible State** ✓ - ip_forwarding_enabled on flux+infra1 only\n- **0.7 Test Host** ✓ - infra1 selected (has SSH/HTTP/HTTPS)\n\n### Not Complete:\n- **0.4 ICMPv6 Firewall** - Reviewed rules, documented behavior (inbound IPv6 blocked), but need to determine if changes are required before Phase 4 external validation","created_at":"2026-01-23T05:12:21Z"}]}
{"id":"infra-ld7","title":"Evaluate PR #1146: Update Helm release grafana to v10.4.1","description":"Run the renovate-eval skill against PR #1146 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.439388-05:00","created_by":"coneill","updated_at":"2026-01-13T00:43:51.862469-05:00","closed_at":"2026-01-13T00:43:51.862469-05:00","close_reason":"PR merged","dependencies":[{"issue_id":"infra-ld7","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.146272-05:00","created_by":"coneill"}]}
{"id":"infra-ldf","title":"Migrate Home Assistant legacy template entities before 2026.6 deadline","description":"## Context\nHome Assistant 2025.12 deprecated legacy template entity syntax. These entities will **stop working entirely in 2026.6**.\n\n## Scope\nMigrate all legacy `platform: template` entities to modern `template:` syntax in kubernetes/hass/config/.\n\n### Affected Files\n- configuration.yaml (majority of templates)\n- packages/*.yaml (if any use legacy syntax)\n\n### Legacy Entities to Migrate\n\n**Sensors (platform: template → template: sensor:)**\n- airvisual_co2, airvisual_pm01, airvisual_pm25, airvisual_pm10, airvisual_pm25_sensor_life\n- harmony_vrroom_input\n- hdfury_vrroom_source  \n- plant_sensor_*_moisture_calibrated (4 sensors)\n\n**Binary Sensors**\n- freezer_door_open_delayed\n\n**Switches (platform: template → template: switch:)**\n- basement_tv, appletv, xbox, playstation, nintendo_switch, windows_pc\n- basement_humidifiers\n- foot_warmer_thermostat, office_heater_thermostat\n\n**Covers**\n- left_garage_door, right_garage_door\n\n## Migration Notes\n- Use `default_entity_id` to preserve entity IDs and history\n- Change `value_template:` to `state:`\n- Merge all into single `template:` block\n- Test config validation before deploying\n\n## Timeline\n- 2025.12: Deprecation warnings appear\n- 2026.6: Legacy entities stop working entirely\n\n## Reference\n- https://community.home-assistant.io/t/deprecation-of-legacy-template-entities-in-2025-12/955562","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-07T23:26:17.278761-05:00","created_by":"coneill","updated_at":"2026-01-13T13:37:51.73291-05:00","closed_at":"2026-01-13T13:37:51.73291-05:00","close_reason":"All legacy template entities migrated to modern syntax in commit f203762c"}
{"id":"infra-lgf","title":"Evaluate PR #1139: Update Helm release semaphore to v16.0.11","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.31902-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-luw","title":"Remove defunct plex group_vars files","description":"Remove ansible/group_vars/plex_server.yaml and ansible/group_vars/plex_client.yaml - no plex_server or plex_client groups exist in inventory. These files are unused.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-06T21:38:00.355927-05:00","created_by":"coneill","updated_at":"2026-01-06T22:55:42.739305-05:00","closed_at":"2026-01-06T22:55:42.739305-05:00","close_reason":"Closed"}
{"id":"infra-m7h","title":"Remove dead render script custom manager from .renovaterc","description":"Remove .renovaterc lines 106-115 - custom regex manager for kubernetes/**/render VERSION/GIT_URL pattern. Verified no render scripts match this pattern. Dead code from older workflow.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-06T21:38:04.072471-05:00","created_by":"coneill","updated_at":"2026-01-06T22:55:42.747165-05:00","closed_at":"2026-01-06T22:55:42.747165-05:00","close_reason":"Closed"}
{"id":"infra-mf7","title":"Update workflow to use Tailscale Semaphore URL","description":"**File:** /Users/coneill/src/infra2/.github/workflows/ansible-production-deploy.yml\n\n**Problem:** Once the Tailscale ingress is deployed, the workflow should use the Tailscale URL instead of the public URL.\n\n**Change Required:**\n\nAdd `SEMAPHORE_URL` environment variable to the \"Deploy via Semaphore\" step (lines 63-76).\n\nCurrent step:\n```yaml\n    - name: Deploy via Semaphore\n      if: steps.hosts.outputs.has_changes == 'true' \u0026\u0026 steps.hosts.outputs.hosts \\!= ''\n      env:\n        SEMAPHORE_API_TOKEN: ${{ secrets.SEMAPHORE_API_TOKEN }}\n      run: |\n```\n\nChange to:\n```yaml\n    - name: Deploy via Semaphore\n      if: steps.hosts.outputs.has_changes == 'true' \u0026\u0026 steps.hosts.outputs.hosts \\!= ''\n      env:\n        SEMAPHORE_API_TOKEN: ${{ secrets.SEMAPHORE_API_TOKEN }}\n        SEMAPHORE_URL: https://semaphore.tail39d37.ts.net\n      run: |\n```\n\n**Note:** The exact Tailscale hostname (`semaphore.tail39d37.ts.net`) may vary based on your tailnet. Verify with `tailscale status` after deploying the ingress.\n\n**Context:** Uses the private Tailscale endpoint instead of public internet, matching the pattern established for ArgoCD.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:54:20.034863-05:00","created_by":"coneill","updated_at":"2026-01-04T13:01:21.920913-05:00","closed_at":"2026-01-04T13:01:21.920924-05:00","dependencies":[{"issue_id":"infra-mf7","depends_on_id":"infra-6o5","type":"blocks","created_at":"2026-01-04T12:54:54.191018-05:00","created_by":"coneill"},{"issue_id":"infra-mf7","depends_on_id":"infra-e4z","type":"blocks","created_at":"2026-01-04T12:54:54.243921-05:00","created_by":"coneill"}]}
{"id":"infra-n9g","title":"Resolve false positive PR comments","description":"**PR:** #1051\n\n**Problem:** Several PR review comments are false positives that should be resolved with explanations.\n\n**Comments to resolve:**\n\n### CodeRabbit False Positives:\n\n1. **ID: 2659796656** - \"Provider source not in registry\"\n   - **Dismissal:** Wrong. The provider `CruGlobal/semaphoreui` exists at registry.terraform.io/providers/CruGlobal/semaphoreui with 7,114 downloads and versions up to 1.4.1. CodeRabbit's web search failed to find it.\n\n2. **ID: 2659801315** - \"websocket-client not declared\"\n   - **Dismissal:** Already declared. This project uses Nix for dependency management, not pip. The package is declared in `flake.nix` line 43: `websocket-client`. Per CLAUDE.md, all tools are provided via Nix.\n\n### Copilot False Positives:\n\n3. **ID: 2659796726** - \"Comment wording misleading\"\n   - **Dismissal:** Comment is accurate. Line 8 sets `ANSIBLE_STDOUT_CALLBACK=default` and line 9 sets `ANSIBLE_STDOUT_CALLBACK_RESULT_FORMAT=yaml`. This IS \"default callback with YAML output format\".\n\n4. **ID: 2659796739** - \"Callback plugins relative path\"\n   - **Dismissal:** Handled correctly. The script explicitly `cd ansible` at lines 56-58 when `ansible/ansible.cfg` exists. The relative path `callback_plugins` resolves correctly.\n\n5. **ID: 2659796749** - \"Checkout SHA outdated\"\n   - **Dismissal:** Factually wrong. SHA `8e8c483db84b4bee98b60c0593521ed34d9990e8` is **v6.0.1** released December 2025 - the LATEST version.\n\n6. **ID: 2659796752** - \"Argument parsing loop\"\n   - **Dismissal:** Wrong. The `*) shift ;;` case handles unknown arguments by shifting past them. The loop terminates when `$#` becomes 0. Standard bash argument parsing.\n\n7. **ID: 2659796754** - \"turnstyle SHA should be tag\"\n   - **Dismissal:** SHA `15f9da4059166900981058ba251e0b652511c68f` is **v3.2.2** released December 2025 - the LATEST version. SHA pinning is intentional security practice.\n\n8. **ID: 2659796756** - \"TAGS_ARG quoting\"\n   - **Dismissal:** Not an issue. Ansible tags are comma-separated identifiers (e.g., `setup,deploy,config`). They don't contain spaces. Unquoted expansion is intentional.\n\n9. **ID: 2659796765** - \"SSH host key checking\"\n   - **Dismissal:** Intentional for homelab. Semaphore pod runs on trusted internal network. All managed hosts are controlled endpoints. SSH host keys change during OS reinstalls.\n\n**Command to resolve comments:**\n```bash\ngh api repos/OWNER/REPO/pulls/1051/comments/COMMENT_ID/replies -X POST -f body=\"Dismissal reason...\"\n```\n\nOr use the pr-triage skill to handle these systematically.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-04T12:54:45.970631-05:00","created_by":"coneill","updated_at":"2026-01-04T19:25:06.895605-05:00","closed_at":"2026-01-04T19:25:06.895623-05:00"}
{"id":"infra-nfp","title":"Test ARA URL via semaphore-deploy after merge","description":"Run ./scripts/semaphore-deploy --hosts k1 --project Infra --template ansible-deploy --tags hostname and verify ARA URL appears in output.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T01:17:35.252344-05:00","created_by":"coneill","updated_at":"2026-01-04T02:06:54.943935-05:00","closed_at":"2026-01-04T02:06:54.943935-05:00","close_reason":"PR merged, callback tested locally with play UUID approach","dependencies":[{"issue_id":"infra-nfp","depends_on_id":"infra-59z","type":"blocks","created_at":"2026-01-04T01:17:38.599763-05:00","created_by":"coneill"},{"issue_id":"infra-nfp","depends_on_id":"infra-6cq","type":"parent-child","created_at":"2026-01-04T01:18:12.053259-05:00","created_by":"coneill"}]}
{"id":"infra-nj8","title":"Evaluate PR #1140: Update prom/prometheus Docker tag to v3.8.1","description":"Run the renovate-eval skill against PR #1140 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.89096-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.640675-05:00","closed_at":"2026-01-13T00:46:36.640675-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-nj8","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.135481-05:00","created_by":"coneill"}]}
{"id":"infra-nt7","title":"Fix ansible-lint non-deterministic galaxy manifest regeneration","description":"## Problem\n\nThe `ansible-lint --fix` flag in the pre-commit hook (`scripts/ansible-lint-wrapper` line 24) regenerates galaxy collection manifest files (FILES.json, MANIFEST.json) with a different file ordering on each run. This causes the pre-commit hook to report \"files were modified\" even though lint passes with 0 failures and 0 warnings.\n\n## Affected Files\n\nEvery commit touching `ansible/**` triggers regeneration of:\n- `ansible/galaxy/collections/ansible_collections/artis3n/tailscale/FILES.json`\n- `ansible/galaxy/collections/ansible_collections/artis3n/tailscale/MANIFEST.json`\n- `ansible/galaxy/collections/ansible_collections/community/proxmox/FILES.json`\n- `ansible/galaxy/collections/ansible_collections/community/proxmox/MANIFEST.json`\n\nThe diffs are always the same size (reordering, not content changes) -- e.g., `500 ++++++-------` for tailscale FILES.json, `820 ++++++++++-----------` for proxmox FILES.json.\n\n## Root Cause\n\n`scripts/ansible-lint-wrapper` runs `ansible-lint --fix` which regenerates collection manifests non-deterministically. The committed ordering and the regenerated ordering differ, so the hook always reports modifications to these 4 files.\n\nThis was also seen in commit 700074b1 (\"Update Ansible Galaxy roles and collections\") which regenerated the same manifests but didn't fix the underlying instability.\n\n## Fix Options\n\n1. **Exclude galaxy/ from ansible-lint** -- Add `.ansible-lint` config in `ansible/` with `exclude_paths: [galaxy/]`. These are third-party vendored files that shouldn't be linted. No `.ansible-lint` config file exists currently.\n2. **Remove `--fix` from wrapper** -- Change line 24 of `scripts/ansible-lint-wrapper` from `ansible-lint --fix` to `ansible-lint`. Prevents auto-modification but means lint fixes must be applied manually.\n3. **Git checkout galaxy/ in wrapper** -- Add `git checkout -- galaxy/` at the end of the wrapper script to discard manifest changes after lint runs.\n\nOption 1 is recommended -- galaxy/ is third-party code and shouldn't be linted or modified.","status":"open","priority":2,"issue_type":"bug","owner":"clayton@oneill.net","created_at":"2026-01-28T18:45:04.079301-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-28T18:45:04.079301-05:00"}
{"id":"infra-o3x","title":"Update zwavejs role","description":"Update zwavejs role to use comment annotation format (keep as template, has other variables).\n\n## Replace `ansible/roles/zwavejs/defaults/main.yaml`:\n\n```yaml\n---\n# renovate: datasource=docker\nzwavejs_image: \"zwavejs/zwave-js-ui:11.9.1@sha256:a7036e59a9d7916d1f92f2fa1e0b9f4a5ed317fc8bef38756368f7c865e0e95a\"\nzwavejs_data_dir: /etc/zwavejs\nzwavejs_device: /dev/serial/by-id/usb-Silicon_Labs_Zooz_ZST10_700_Z-Wave_Stick_26b0d03ffcc9ec1191cf65a341be1031-if00-port0\nzwavejs_hostname: zwavejs.oneill.net\n```\n\n## Update `ansible/roles/zwavejs/templates/docker-compose.yaml.j2` (line 4):\n\nChange:\n```yaml\n    image: zwavejs/zwave-js-ui:{{ zwavejs_image_tag }}\n```\n\nTo:\n```yaml\n    image: {{ zwavejs_image }}\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T11:35:41.758313-05:00","created_by":"coneill","updated_at":"2026-01-06T12:19:09.47646-05:00","closed_at":"2026-01-06T12:19:09.47646-05:00","close_reason":"Closed"}
{"id":"infra-ocu","title":"Make garage door covers tolerant to unavailable tilt sensors","description":"## Background\n\nInvestigation of the Jan 9 2026 midnight garage door incident revealed that the template covers don't handle unavailable tilt sensors well.\n\n## Current behavior\n\n- Binary sensor has `availability` template that marks it unavailable when angle sensor isn't a number\n- Cover template has NO `availability_template`\n- When binary sensor is unavailable, `is_state(..., 'off')` returns False → cover shows 'closed'\n- Cover remains 'available' and can accept commands\n- `close_cover` presses button unconditionally\n\n## Problem\n\nCover appears closed and available even when we don't actually know the door state. This allowed scene reproduction to call `close_cover` on an already-closed door, toggling it open.\n\n## Investigation needed\n\n1. Add `availability_template` to covers mirroring binary sensor availability\n2. Consider adding conditions to `open_cover`/`close_cover` actions\n3. Evaluate if `stop_cover` should be defined to prevent accidental toggles\n4. Test behavior with unavailable sensors after changes\n\n## Files\n\n- kubernetes/hass/config/configuration.yaml (lines 155-184 for covers, 340-354 for binary sensors)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-11T11:14:55.508597-05:00","created_by":"coneill","updated_at":"2026-01-13T13:35:09.560983-05:00","closed_at":"2026-01-13T13:35:09.560983-05:00","close_reason":"Garage door covers now have availability_template mirroring binary sensor availability"}
{"id":"infra-ofg","title":"Add Tailscale ACL policy for Semaphore","description":"**File:** /Users/coneill/src/infra2/opentofu/tailscale.tf\n\n**Problem:** GitHub Actions runners tagged with `tag:github-actions` can only reach ArgoCD per current ACL. Semaphore is NOT in the ACL, so the workflow will fail to reach Semaphore via Tailscale.\n\n**Changes Required:**\n\n1. **Add tag:semaphore to tagOwners (after line 16):**\n```hcl\n\"tag:semaphore\": [\"tag:k8s-operator\"],\n```\n\nInsert this after line 16 (`\"tag:argocd\": [\"tag:k8s-operator\"],`) in the tagOwners block.\n\n2. **Modify ACL rule at line 61 to include semaphore:**\n\nCurrent (line 61):\n```hcl\n\"dst\":    [\"tag:argocd:443\"],\n```\n\nChange to:\n```hcl\n\"dst\":    [\"tag:argocd:443\", \"tag:semaphore:443\"],\n```\n\n**Context:** This allows GitHub Actions runners to reach Semaphore via Tailscale, matching the pattern used for ArgoCD.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:49:00.962103-05:00","created_by":"coneill","updated_at":"2026-01-04T12:58:54.023356-05:00","closed_at":"2026-01-04T12:58:54.023362-05:00"}
{"id":"infra-op3","title":"Replace tbaczynski.chrony with maintained alternative","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T19:00:04.458887-05:00","created_by":"coneill","updated_at":"2026-01-18T10:08:41.027329-05:00","closed_at":"2026-01-18T10:08:41.027329-05:00","close_reason":"Completed in commit 6cdc98d3 - replaced tbaczynski.chrony with local chrony role"}
{"id":"infra-oty","title":"Semaphore Auto-Deploy for Ansible Implementation Plan","description":"Automatically deploy Ansible changes via Semaphore when merged to main, mirroring the ArgoCD pattern.\n\n## Branch\n`semaphore-auto-deploy`\n\n## Architecture\nPush to main (ansible/**) → GitHub Actions → ansible-detect-changes → semaphore-deploy → Semaphore API → Ansible runs\n\n## Smart Detection Logic\n- `host_vars/\u003chostname\u003e.yaml` changed → run against that hostname\n- Everything else (roles, group_vars, site.yaml, etc.) → run against all hosts\n\n## Files Created/Modified\n- `opentofu/modules/semaphore/` - OpenTofu module (versions.tf, main.tf, outputs.tf, variables.tf)\n- `opentofu/main.tf`, `opentofu/secrets.tf` - Root module integration\n- `opentofu/github-repos.tf` - GitHub Actions secrets/variables\n- `scripts/ansible-detect-changes` - Change detection script\n- `scripts/semaphore-deploy` - Deploy script (Python, streams logs, retries)\n- `.github/actions/setup-semaphore/action.yml` - Setup composite action\n- `.github/workflows/ansible-production-deploy.yml` - Deploy workflow\n- `ansible/callback_plugins/ara_url.py` - ARA URL callback plugin\n- `ansible/site.yaml` - Remove old ARA Reporting plays (replaced by callback)\n\n## Semaphore Resource IDs (Project 2)\n- Project: 2, Keys: none=3, ansible_ssh=5\n- Repository: 2, Inventory: 6, Environment: 2, Template: 9\n\n## Provider Schema Notes\nsemaphoreui provider uses different schema than docs:\n- Keys: `none = {}` or `ssh = { private_key = \"...\" }` (not `type = \"none\"`)\n- Inventory: `file = { path = \"...\", repository_id = ... }` (not `type = \"file\"`)\n- Environment: `environment = {}` (not `env = jsonencode({})`)\n\n## Completed Work\nTasks 1-15 completed: API token, OpenTofu module, GitHub secrets, detection script, deploy script, log tailing, setup action, deploy workflow, template rename, ARA callback plugin, end-to-end testing\n\n## Remaining\n- Merge semaphore-auto-deploy branch to main\n- Redeploy Semaphore to pick up site.yaml changes (removal of old ARA plays)","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-03T18:28:10.735568-05:00","created_by":"coneill","updated_at":"2026-01-05T10:03:36.3304-05:00","closed_at":"2026-01-05T10:03:36.3304-05:00","close_reason":"Semaphore auto-deploy feature validated. All 4 tests passed:\n- Test 1: Manual workflow dispatch ✓\n- Test 2: Single host change detection ✓\n- Test 3: All hosts detection ✓\n- Test 4: Non-ansible changes skip ✓\n\nBugs fixed during testing:\n- PR #1056: Python script execution in nix develop\n- PR #1068: detect-hosts composite action alignment with ArgoCD\n\nFollow-up issues created:\n- infra-hcr: ACME certificate failure on Proxmox\n- infra-4bz: Docker digest PRs not automerging\n- infra-vat: Workflow naming alignment"}
{"id":"infra-oty.1","title":"Add Log Tailing to semaphore-deploy","description":"Add real-time log streaming to semaphore-deploy script.\n\n## Problem\nWhen Semaphore tasks fail, we couldn't see why - the script only reported status without showing task output.\n\n## Solution\nStream task output logs while polling for completion.\n\n## Implementation\n- File: `scripts/semaphore-deploy`\n- API endpoint: `GET /api/project/{project_id}/tasks/{task_id}/output`\n- Returns array of output lines: `[{\"time\": \"...\", \"output\": \"line1\"}, ...]`\n\n## How It Works\n1. Track last output line position\n2. Each poll iteration:\n   - Fetch task output via API\n   - Print only new lines since last position\n   - Update position\n3. Output appears in real-time as task runs\n\n## Example Output\n```\nWaiting for task 105 to complete...\n  Task 105 status: running\n  \u003e PLAY [all] *********************************************************************\n  \u003e TASK [Gathering Facts] *********************************************************\n  \u003e ok: [k1]\n  \u003e ...\n  Task 105 status: success\n```\n\n## Status\nCompleted - implemented in scripts/semaphore-deploy on semaphore-auto-deploy branch","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:28:16.285017-05:00","created_by":"coneill","updated_at":"2026-01-04T11:13:53.458574-05:00","closed_at":"2026-01-03T18:29:35.979229-05:00","close_reason":"Already completed - log tailing implemented","dependencies":[{"issue_id":"infra-oty.1","depends_on_id":"infra-oty","type":"parent-child","created_at":"2026-01-03T18:28:16.295731-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2","title":"Create ARA URL Callback Plugin","description":"Create callback plugin to print ARA URL at end of every playbook run, regardless of success/failure.\n\n## Why a Callback Plugin?\nThe old approach (ARA Reporting plays in site.yaml) doesn't run if the playbook fails mid-execution. A callback plugin's `v2_playbook_on_stats` hook fires regardless of success/failure.\n\n## Implementation\n- File: `ansible/callback_plugins/ara_url.py`\n- Hook: `v2_playbook_on_stats` - runs at end of playbook\n- Merged to main: PR #1049\n\n## Key Technical Details\n- Uses `AraHttpClient` from `ara.clients.http`\n- Queries `/api/v1/playbooks?order=-id\u0026limit=1` for latest playbook\n- Gracefully handles missing ARA_API_SERVER (skips silently)\n- Auto-enabled via `CALLBACK_NEEDS_ENABLED = False`\n\n## Environment Variables Required\n- `ARA_API_SERVER` - ARA API endpoint (e.g., https://ara.k.oneill.net)\n- `ARA_API_USERNAME` - API auth username\n- `ARA_API_PASSWORD` - API auth password\n- `ARA_API_TIMEOUT` - Request timeout (default 30)\n\n## Related Files\n- `ansible/ansible.cfg` - Added `callback_plugins = callback_plugins`\n- `kubernetes/semaphore/ansible-wrapper.sh` - Sets ARA env vars for Semaphore\n- `ansible/site.yaml` - Old ARA plays need removal (infra-oty.2.5)\n\n## Status\nCallback plugin merged (PR #1049). Still need to remove old ARA plays from site.yaml.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:28:22.283534-05:00","created_by":"coneill","updated_at":"2026-01-04T13:30:05.66591-05:00","closed_at":"2026-01-04T13:30:05.66591-05:00","close_reason":"ARA callback plugin implemented with UUID-based tracking. All subtasks complete.","dependencies":[{"issue_id":"infra-oty.2","depends_on_id":"infra-oty","type":"parent-child","created_at":"2026-01-03T18:28:22.295448-05:00","created_by":"coneill"},{"issue_id":"infra-oty.2","depends_on_id":"infra-oty.1","type":"blocks","created_at":"2026-01-03T18:28:22.296808-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2.1","title":"Create Callback Plugin","description":"Create ansible/callback_plugins/ara_url.py","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:43:53.747944-05:00","created_by":"coneill","updated_at":"2026-01-03T18:47:59.563695-05:00","closed_at":"2026-01-03T18:47:59.563695-05:00","close_reason":"Closed","dependencies":[{"issue_id":"infra-oty.2.1","depends_on_id":"infra-oty.2","type":"parent-child","created_at":"2026-01-03T18:43:53.763981-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2.2","title":"Configure ansible.cfg","description":"Add callback_plugins path","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:44:00.823514-05:00","created_by":"coneill","updated_at":"2026-01-03T18:48:36.891073-05:00","closed_at":"2026-01-03T18:48:36.891073-05:00","close_reason":"Closed","dependencies":[{"issue_id":"infra-oty.2.2","depends_on_id":"infra-oty.2","type":"parent-child","created_at":"2026-01-03T18:44:00.842603-05:00","created_by":"coneill"},{"issue_id":"infra-oty.2.2","depends_on_id":"infra-oty.2.1","type":"blocks","created_at":"2026-01-03T18:44:00.846584-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2.3","title":"Update ansible-wrapper.sh","description":"Include custom callback path for Semaphore","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:44:06.209566-05:00","created_by":"coneill","updated_at":"2026-01-03T18:44:36.986503-05:00","closed_at":"2026-01-03T18:44:36.986503-05:00","close_reason":"wontfix","dependencies":[{"issue_id":"infra-oty.2.3","depends_on_id":"infra-oty.2","type":"parent-child","created_at":"2026-01-03T18:44:06.218582-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2.5","title":"Remove ARA Reporting Plays from site.yaml","description":"Remove the obsolete ARA Reporting play from site.yaml. This should have been in PR #1049 with the callback plugin but was missed.\n\n## Why This Is Needed\nThe callback plugin (ansible/callback_plugins/ara_url.py) was merged to main in PR #1049, but the OLD ARA Reporting play (lines 234-280 on main) was not removed. This play runs a Python shell task to query ARA, which fails in Semaphore:\n\n```\nTASK [Get current playbook ID from ARA] ****************************************\n[ERROR]: Task failed: Module failed: non-zero return code\n```\n\nThe callback plugin handles this properly via v2_playbook_on_stats, making the old play unnecessary.\n\n## The Old Play (lines 234-280 on main)\n```yaml\n- name: ARA Reporting\n  hosts: all\n  gather_facts: false\n  tasks:\n    - name: Get current playbook ID from ARA\n      ansible.builtin.shell: |\n        python3 -c \"from ara.clients.http import AraHttpClient; ...\"\n```\n\n## Fix\nDelete the entire \"ARA Reporting\" play section (lines 234-280 in ansible/site.yaml on main).\n\n## Steps\n```bash\ngit fetch origin\ngit checkout -b remove-ara-plays origin/main\n# Edit ansible/site.yaml to remove lines 234-280\ngit add ansible/site.yaml\ngit commit -m \"Remove obsolete ARA Reporting play from site.yaml\n\nThe ara_url callback plugin (added in PR #1049) handles ARA URL\nreporting via v2_playbook_on_stats. The old shell-based approach\nfails in Semaphore due to missing Python environment.\"\ngit push -u origin remove-ara-plays\ngh pr create --fill\n```\n\n## Verification\nAfter merge and Semaphore pod redeploy:\n- No \"TASK [Get current playbook ID from ARA]\" error\n- \"ARA Playbook URL: ...\" appears from callback plugin","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:44:15.542011-05:00","created_by":"coneill","updated_at":"2026-01-04T11:52:01.090405-05:00","closed_at":"2026-01-04T11:52:01.090405-05:00","close_reason":"Verified: PR #1050 merged, Semaphore redeployed, ARA URL callback working correctly","dependencies":[{"issue_id":"infra-oty.2.5","depends_on_id":"infra-oty.2","type":"parent-child","created_at":"2026-01-03T18:44:15.550305-05:00","created_by":"coneill"}]}
{"id":"infra-oty.2.6","title":"Test Locally","description":"Verify URL prints at end","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:44:21.135593-05:00","created_by":"coneill","updated_at":"2026-01-04T02:06:55.063088-05:00","closed_at":"2026-01-04T02:06:55.063088-05:00","close_reason":"Tested locally - callback plugin works correctly","dependencies":[{"issue_id":"infra-oty.2.6","depends_on_id":"infra-oty.2","type":"parent-child","created_at":"2026-01-03T18:44:21.14617-05:00","created_by":"coneill"},{"issue_id":"infra-oty.2.6","depends_on_id":"infra-oty.2.5","type":"blocks","created_at":"2026-01-03T18:44:21.150476-05:00","created_by":"coneill"}]}
{"id":"infra-oty.3","title":"Add Links to GitHub Actions Summary","description":"Add deployment summary with links to semaphore-deploy output.\n\n## Goal\nProvide easy access to Semaphore task and ARA playbook URLs after deployment.\n\n## Implementation\n- File: `scripts/semaphore-deploy`\n- Outputs to both console AND GitHub Actions job summary when `$GITHUB_STEP_SUMMARY` is set\n\n## Output Format\n```\n================================================================================\nDeployment Summary\n================================================================================\nSemaphore Task: https://semaphore.k.oneill.net/project/2/templates/9/tasks/123\nARA Playbook:   https://ara.k.oneill.net/playbooks/456.html\nStatus:         SUCCESS\nDuration:       2m 34s\n================================================================================\n```\n\n## GitHub Actions Integration\nWhen running in GitHub Actions, the summary is also written to `$GITHUB_STEP_SUMMARY` for display in the job summary page.\n\n## Status\nCompleted - implemented in scripts/semaphore-deploy on semaphore-auto-deploy branch","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:28:27.451892-05:00","created_by":"coneill","updated_at":"2026-01-04T11:13:54.487081-05:00","closed_at":"2026-01-03T19:16:30.809273-05:00","close_reason":"Closed","dependencies":[{"issue_id":"infra-oty.3","depends_on_id":"infra-oty","type":"parent-child","created_at":"2026-01-03T18:28:27.454591-05:00","created_by":"coneill"},{"issue_id":"infra-oty.3","depends_on_id":"infra-oty.2","type":"blocks","created_at":"2026-01-03T18:28:27.456294-05:00","created_by":"coneill"}]}
{"id":"infra-oty.4","title":"Test End-to-End","description":"Full end-to-end test of Semaphore auto-deploy:\n\n1. Create test branch with minor ansible change (e.g., comment in host_vars/k1.yaml)\n2. Push and merge to main\n3. Verify GitHub Actions workflow triggers\n4. Verify Semaphore task is created and runs\n5. Check ARA for playbook execution record\n6. Verify GitHub Actions job summary shows Semaphore and ARA links\n7. Test workflow_dispatch manual trigger with hosts=\"all\"\n8. Test workflow_dispatch with specific hosts and tags","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T18:28:33.126247-05:00","created_by":"coneill","updated_at":"2026-01-04T02:15:47.466672-05:00","closed_at":"2026-01-04T02:15:47.466672-05:00","close_reason":"End-to-end test successful - ARA URL appears in semaphore-deploy output","dependencies":[{"issue_id":"infra-oty.4","depends_on_id":"infra-oty","type":"parent-child","created_at":"2026-01-03T18:28:33.137812-05:00","created_by":"coneill"},{"issue_id":"infra-oty.4","depends_on_id":"infra-oty.3","type":"blocks","created_at":"2026-01-03T18:28:33.139202-05:00","created_by":"coneill"},{"issue_id":"infra-oty.4","depends_on_id":"infra-25p","type":"blocks","created_at":"2026-01-03T19:19:04.793326-05:00","created_by":"coneill"},{"issue_id":"infra-oty.4","depends_on_id":"infra-ziy","type":"blocks","created_at":"2026-01-04T01:04:54.556956-05:00","created_by":"coneill"}]}
{"id":"infra-oty.5","title":"Remove old ARA Reporting plays from site.yaml","description":"Remove the obsolete ARA Reporting play from site.yaml (lines 234-280 on main).\n\n## Why\nThe callback plugin (ansible/callback_plugins/ara_url.py, already on main via PR #1049) replaces this functionality. The old play runs a Python shell task that fails in Semaphore because it doesn't have the right Python environment.\n\n## The Problem\nThe old play executes Python code inline via ansible.builtin.shell:\n```yaml\n- name: Get current playbook ID from ARA\n  ansible.builtin.shell: |\n    python3 -c \"from ara.clients.http import AraHttpClient; ...\"\n```\nThis fails in Semaphore with \"non-zero return code\" because Semaphore's environment doesn't have ara installed.\n\n## The Fix\nDelete lines 234-280 from ansible/site.yaml (the entire \"ARA Reporting\" play section).\n\n## How to Create PR\n```bash\ngit fetch origin\ngit checkout -b remove-ara-plays origin/main\n# Remove the play (lines 234-280)\ngit add ansible/site.yaml\ngit commit -m \"Remove old ARA Reporting plays from site.yaml\n\nThe callback plugin (ara_url.py) now handles ARA URL reporting via\nv2_playbook_on_stats, which runs reliably at end of playbook regardless\nof success/failure.\"\ngit push -u origin remove-ara-plays\ngh pr create --fill\n```\n\n## Verification\nAfter merge and Semaphore redeploy, test via semaphore-deploy (on semaphore-auto-deploy branch):\n- Should NOT show \"TASK [Get current playbook ID from ARA]\" error\n- Should show \"ARA Playbook URL: https://ara.k.oneill.net/playbooks/\u003cid\u003e.html\"","status":"tombstone","priority":1,"issue_type":"task","created_at":"2026-01-04T02:23:58.014112-05:00","created_by":"coneill","updated_at":"2026-01-04T02:25:24.868341-05:00","deleted_at":"2026-01-04T02:25:24.868341-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-oty.6","title":"Merge semaphore-auto-deploy branch to main","description":"Merge the semaphore-auto-deploy branch to main to complete the Semaphore Auto-Deploy epic.\n\n## Prerequisites\n- infra-oty.2.5 must be merged first (removes old ARA plays from site.yaml)\n- Rebase semaphore-auto-deploy on main after infra-oty.2.5 is merged\n\n## Branch Contents\n- OpenTofu Semaphore module (`opentofu/modules/semaphore/`)\n- GitHub Actions workflow + setup action\n- Deploy scripts (`scripts/semaphore-deploy`, `scripts/ansible-detect-changes`)\n\n## Key Files\n```\n.github/actions/setup-semaphore/action.yml  # Tailscale + connectivity check\n.github/workflows/ansible-production-deploy.yml  # Main workflow\nopentofu/modules/semaphore/main.tf  # Semaphore resources\nopentofu/modules/semaphore/variables.tf  # SSH key variable\nopentofu/github-repos.tf  # GitHub secrets/variables\nscripts/semaphore-deploy  # Python deploy with log tailing\nscripts/ansible-detect-changes  # Bash change detection\n```\n\n## Merge Steps\n1. After infra-oty.2.5 merged:\n   ```bash\n   git fetch origin\n   git checkout semaphore-auto-deploy\n   git rebase origin/main\n   # Resolve any conflicts (site.yaml should auto-resolve)\n   ```\n2. Create PR: `gh pr create --fill`\n3. Wait for CI\n4. Merge: `gh pr merge --merge`\n\n## Post-Merge\n1. Redeploy Semaphore: `kubectl rollout restart deployment/semaphore -n semaphore`\n2. Test: `./scripts/semaphore-deploy --hosts k1 --project Infra --template ansible-deploy --tags hostname`\n3. Verify ARA URL appears, no errors","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T02:24:16.683589-05:00","created_by":"coneill","updated_at":"2026-01-04T22:02:48.357325-05:00","closed_at":"2026-01-04T22:02:48.357325-05:00","close_reason":"PR #1051 merged to main","dependencies":[{"issue_id":"infra-oty.6","depends_on_id":"infra-oty","type":"parent-child","created_at":"2026-01-04T02:24:16.693707-05:00","created_by":"coneill"},{"issue_id":"infra-oty.6","depends_on_id":"infra-oty.2.5","type":"blocks","created_at":"2026-01-04T11:12:47.481088-05:00","created_by":"coneill"}]}
{"id":"infra-pof","title":"Sysctl Configuration Strategy","description":"**Core Questions:**\n- Required IPv6 sysctl settings: forwarding, privacy, RA acceptance, tempaddr?\n- Global IPv6 forwarding vs. per-interface?\n- Conflicts with existing IPv4 settings?\n- Consolidate scattered sysctl into unified strategy, or keep role-based?\n- Where should IPv6 settings live: common, kubeadm, Tailscale, or new network role?\n\n**Current Issues:**\n- IPv4 forwarding configured in TWO places (duplication)\n- Inconsistent patterns: raw file copy (Tailscale) vs. ansible.posix.sysctl (others)\n\n**Investigation Needed:**\n- Document all `/etc/sysctl.d/99-*.conf` files currently deployed\n- Research Linux IPv6 kernel parameters: `disable_ipv6`, `conf.all.forwarding`, `conf.all.accept_ra`, `conf.all.use_tempaddr`\n- Identify duplicated settings to consolidate\n\n**Critical Files:**\n- `ansible/roles/common/tasks/main.yaml` (lines 97-117)\n- `ansible/roles/kubeadm/tasks/common.yaml` (lines 23-47)\n- `ansible/site.yaml` (lines 160-182) - Tailscale IPv6 forwarding\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:13.904164-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.477654-05:00","closed_at":"2026-01-22T23:32:54.477654-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1."}
{"id":"infra-qvc","title":"Evaluate PR #1134: Update booklore/booklore Docker tag to v1.16.1","description":"Run the renovate-eval skill against PR #1134 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.33022-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.633015-05:00","closed_at":"2026-01-13T00:46:36.633015-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-qvc","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.12497-05:00","created_by":"coneill"}]}
{"id":"infra-s14","title":"Audit Home Assistant logs via Alloy to discover issues","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-11T21:29:35.360137-05:00","created_by":"coneill","updated_at":"2026-01-11T21:29:35.360137-05:00"}
{"id":"infra-s4c","title":"Evaluate making parts of flake conditional for CI","description":"Investigate making heavy/platform-specific packages in flake.nix conditional to speed up CI builds. Examples: mcp-cli (binary, platform-specific), unifi-network-mcp (Python 3.13). Consider using flake outputs vs devShell composition, or environment variables to toggle packages.","status":"open","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-23T00:43:34.512818-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-23T00:43:43.455076-05:00"}
{"id":"infra-s7d","title":"Phase 1: Ansible IPv4 Network Management for RPis","description":"Read the full plan and all comments in infra-ec1, then implement Phase 1.","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-22T23:32:29.610519-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-28T00:26:55.115407-05:00","closed_at":"2026-01-28T00:26:55.115407-05:00","close_reason":"PR #1310 merged. pantrypi and garagepi both under Ansible network management with static IPv4.","dependencies":[{"issue_id":"infra-s7d","depends_on_id":"infra-kt0","type":"blocks","created_at":"2026-01-22T23:32:41.192031-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-sep","title":"Evaluate PR #1146: Update Helm release grafana to v10.4.0","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:10.806637-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-tbk","title":"Evaluate PR #1132: Update nginx:1.29-alpine Docker digest to c083c37","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.749612-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-ts6","title":"Fix INJECT_FACTS_AS_VARS in roles/kubeadm/templates/kubeadm.conf.j2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:54:07.320943-05:00","created_by":"coneill","updated_at":"2026-01-13T19:24:55.62728-05:00","closed_at":"2026-01-13T19:24:55.62728-05:00","close_reason":"Fixed: changed ansible_* to ansible_facts['*'] syntax"}
{"id":"infra-tvy","title":"Ansible Host-Level IPv6 Configuration","description":"**Core Questions:**\n- Configure IPv6 at interface level (interfaces role) or via sysctl?\n- SLAAC consistency across OS versions (Debian, Ubuntu, Proxmox)?\n- Does `michaelrigart.interfaces` role support IPv6 static addresses?\n- Dependency order: must IPv6 be enabled before Kubernetes/Tailscale?\n- Add IPv6 to `host_vars`, or derive in Jinja2 templates?\n\n**Investigation Needed:**\n- Test `michaelrigart.interfaces` role IPv6 capabilities\n- Check Proxmox VE management interface IPv6 handling\n- Verify ESPHome/IoT device IPv6 support (RA, DHCPv6, SLAAC-only?)\n\n**Critical Files:**\n- `ansible/group_vars/` and `ansible/host_vars/` - current network variables\n- Interfaces role configuration\n\n---\n**Research Plan:** `~/.claude/plans/eventual-meandering-mountain.md`","status":"closed","priority":3,"issue_type":"task","owner":"clayton@oneill.net","created_at":"2026-01-20T14:07:18.601664-05:00","created_by":"Clayton O'Neill","updated_at":"2026-01-22T23:32:54.479872-05:00","closed_at":"2026-01-22T23:32:54.479872-05:00","close_reason":"Research complete. Implementation plan created in infra-ec1.","dependencies":[{"issue_id":"infra-tvy","depends_on_id":"infra-66l","type":"blocks","created_at":"2026-01-20T14:07:47.368657-05:00","created_by":"Clayton O'Neill"}]}
{"id":"infra-ugw","title":"Create Tailscale ingress for Semaphore","description":"**File:** /Users/coneill/src/infra2/kubernetes/semaphore/semaphore-tailscale-ingress.yaml (NEW FILE)\n\n**Problem:** ArgoCD has a dedicated Tailscale ingress, but Semaphore only has the public NGINX ingress. Without a Tailscale ingress tagged with `tag:semaphore`, the ACL rule has nothing to match.\n\n**Create new file with this content:**\n```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\n\nmetadata:\n  name: semaphore-tailscale\n  namespace: semaphore\n  annotations:\n    tailscale.com/tags: tag:semaphore\n\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: semaphore\n      port:\n        number: 3000\n  tls:\n  - hosts:\n    - semaphore\n```\n\n**Reference:** Based on /Users/coneill/src/infra2/kubernetes/argocd/tailscale-ingress.yaml pattern. Key differences:\n- Uses port 3000 (Semaphore's port) instead of 443\n- Uses `tag:semaphore` annotation\n- Service name is `semaphore`\n\n**Context:** This creates a Tailscale-accessible endpoint for Semaphore that matches the ACL tag.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T12:49:13.662369-05:00","created_by":"coneill","updated_at":"2026-01-04T12:58:54.069069-05:00","closed_at":"2026-01-04T12:58:54.069072-05:00","dependencies":[{"issue_id":"infra-ugw","depends_on_id":"infra-ofg","type":"blocks","created_at":"2026-01-04T12:54:54.07233-05:00","created_by":"coneill"}]}
{"id":"infra-ujr","title":"Evaluate PR #1138: Update Helm release reloader to v2.2.7","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.390438-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-vat","title":"Align workflow naming between Ansible and ArgoCD deploys","description":"Standardize naming conventions between ansible-production-deploy.yml and production-deploy.yml (ArgoCD). Check file extensions (.yml vs .yaml), workflow names, and step naming patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T23:48:07.989163-05:00","created_by":"coneill","updated_at":"2026-01-06T20:08:20.377035-05:00","closed_at":"2026-01-06T20:08:20.377035-05:00","close_reason":"Closed"}
{"id":"infra-vgr","title":"Evaluate PR #1139: Update Helm release semaphore to v16.0.11","description":"Run the renovate-eval skill against PR #1139 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.963007-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.639274-05:00","closed_at":"2026-01-13T00:46:36.639274-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-vgr","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.133769-05:00","created_by":"coneill"}]}
{"id":"infra-vuk","title":"Evaluate PR #1144: Update Helm release argo-cd to v9.2.3","description":"Run the renovate-eval skill against PR #1144 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:00.590755-05:00","created_by":"coneill","updated_at":"2026-01-12T23:11:52.139714-05:00","closed_at":"2026-01-12T23:11:52.139714-05:00","close_reason":"PR #1144 automerge enabled","dependencies":[{"issue_id":"infra-vuk","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.142428-05:00","created_by":"coneill"}]}
{"id":"infra-wma","title":"Evaluate PR #1138: Update Helm release reloader to v2.2.7","description":"Run the renovate-eval skill against PR #1138 to analyze the update and provide a merge recommendation.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T09:11:01.035272-05:00","created_by":"coneill","updated_at":"2026-01-13T00:46:36.637758-05:00","closed_at":"2026-01-13T00:46:36.637758-05:00","close_reason":"Epic closed","dependencies":[{"issue_id":"infra-wma","depends_on_id":"infra-5z4","type":"parent-child","created_at":"2026-01-12T09:15:56.132015-05:00","created_by":"coneill"}]}
{"id":"infra-xoy","title":"Evaluate PR #1137: Update Helm release crowdsec to v0.21.1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.461859-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-xry","title":"Evaluate PR #1134: Update booklore/booklore Docker tag to v1.16.1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T09:07:11.678068-05:00","created_by":"coneill","updated_at":"2026-01-12T09:07:35.266525-05:00","deleted_at":"2026-01-12T09:07:35.266525-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"infra-yks","title":"Refactor semaphore-deploy into shared library","description":"## Files\n- Create: `scripts/lib/semaphore.py`\n- Modify: `scripts/semaphore-deploy`\n\n## Extract from semaphore-deploy\n- `SemaphoreDeployer` class (lines 46-363)\n- `DeploymentError` exception\n- Constants: `SEMAPHORE_URL`, `SEMAPHORE_DISPLAY_URL`, `ARA_URL`\n\n## Architecture\n```\nscripts/\n├── semaphore-deploy           # Existing - import from lib\n├── ansible-idempotency-test   # New script using shared lib\n└── lib/\n    └── semaphore.py           # Shared Semaphore API client code\n```\n\n## Requirements\n- Keep CLI interface of semaphore-deploy unchanged\n- Remove duplicated code from semaphore-deploy\n- Import pattern: `from lib.semaphore import SemaphoreDeployer, ARA_URL`","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T08:39:41.616169-05:00","created_by":"coneill","updated_at":"2026-01-07T08:55:03.784937-05:00","closed_at":"2026-01-07T08:55:03.784937-05:00","close_reason":"Closed"}
{"id":"infra-yx3","title":"Renovate audit cleanup","description":"Cleanup tasks from Renovate coverage audit (infra-2cw). Address defunct files, dead code, and version drift identified during audit.","status":"closed","priority":3,"issue_type":"epic","created_at":"2026-01-06T21:38:36.573986-05:00","created_by":"coneill","updated_at":"2026-01-06T22:55:42.768107-05:00","closed_at":"2026-01-06T22:55:42.768107-05:00","close_reason":"Closed","dependencies":[{"issue_id":"infra-yx3","depends_on_id":"infra-luw","type":"blocks","created_at":"2026-01-06T21:38:48.243868-05:00","created_by":"coneill"},{"issue_id":"infra-yx3","depends_on_id":"infra-m7h","type":"blocks","created_at":"2026-01-06T21:38:48.308455-05:00","created_by":"coneill"},{"issue_id":"infra-yx3","depends_on_id":"infra-b7y","type":"blocks","created_at":"2026-01-06T21:38:48.371412-05:00","created_by":"coneill"}]}
{"id":"infra-ziy","title":"Add ARA environment variables to Semaphore","description":"Add ARA_API_SERVER, ARA_API_USERNAME, ARA_API_PASSWORD to Semaphore environment in opentofu/modules/semaphore/main.tf. Required for ARA callback plugin to report playbook URLs.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T01:04:41.984767-05:00","created_by":"coneill","updated_at":"2026-01-04T02:06:54.996774-05:00","closed_at":"2026-01-04T02:06:54.996774-05:00","close_reason":"Obsolete - ARA env vars already configured via externalsecret-ara.yaml and ansible-wrapper.sh"}
