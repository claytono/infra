---
# NVIDIA GPU driver and container toolkit for Kubernetes nodes
# Conditional on nvidia_gpu: true in host_vars

# Workaround: Sequoia (sqv) rejects SHA1 binding signatures on GPG keys.
# The NVIDIA CUDA repo signing key uses SHA1, causing apt update to fail.
# Remove when NVIDIA re-signs their repo key.
# See: https://github.com/claytono/infra/issues/1334
- name: "Ensure crypto-policies directory exists"
  ansible.builtin.file:
    path: /etc/crypto-policies/back-ends
    state: directory
    mode: "0755"

- name: "Extend Sequoia SHA1 deadline for NVIDIA CUDA repo"
  ansible.builtin.copy:
    dest: /etc/crypto-policies/back-ends/apt-sequoia.config
    mode: "0644"
    content: |
      [hash_algorithms]
      sha1.second_preimage_resistance = 2027-02-01
      sha1.collision_resistance = 2027-02-01
# End Sequoia SHA1 workaround

- name: "Install NVIDIA driver prerequisites"
  ansible.builtin.apt:
    name:
      - linux-headers-amd64
      - dkms
      - build-essential

- name: "Download NVIDIA CUDA keyring"
  ansible.builtin.get_url:
    url: https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.1-1_all.deb
    dest: /tmp/cuda-keyring.deb
    mode: "0644"

- name: "Install NVIDIA CUDA keyring"
  ansible.builtin.apt:
    deb: /tmp/cuda-keyring.deb

- name: "Create NVIDIA driver package pin for version"
  ansible.builtin.copy:
    dest: /etc/apt/preferences.d/nvidia-driver
    content: |
      Package: nvidia-* libnvidia-* libnvcuvid* libcuda* libgles-nvidia* libnvoptix* libegl-nvidia* libgl1-nvidia* libglx-nvidia* xserver-xorg-video-nvidia* firmware-nvidia-*
      Pin: version {{ nvidia_driver_version }}
      Pin-Priority: 900
    mode: "0644"

- name: "Install NVIDIA driver from official repo"
  ansible.builtin.apt:
    name:
      - "nvidia-driver={{ nvidia_driver_version }}"
      - "nvidia-driver-cuda={{ nvidia_driver_version }}"
    state: present
    allow_downgrade: true
    update_cache: true
    dpkg_options: force-confnew
  register: kubeadm_nvidia_driver_install
  notify: "Update initramfs"

- name: "Ensure apt keyrings directory exists"
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: "0755"

- name: "Add NVIDIA container toolkit GPG key"
  ansible.builtin.get_url:
    url: https://nvidia.github.io/libnvidia-container/gpgkey
    dest: /etc/apt/keyrings/nvidia-container-toolkit.asc
    mode: "0644"

- name: "Add NVIDIA container toolkit repository"
  ansible.builtin.apt_repository:
    repo: "deb [signed-by=/etc/apt/keyrings/nvidia-container-toolkit.asc] https://nvidia.github.io/libnvidia-container/stable/deb/amd64 /"
    filename: nvidia-container-toolkit
    state: present

- name: "Create nvidia-container-toolkit package pin for version"
  ansible.builtin.copy:
    dest: /etc/apt/preferences.d/nvidia-container-toolkit
    content: |
      Package: nvidia-container-toolkit nvidia-container-toolkit-base libnvidia-container1 libnvidia-container-tools
      Pin: version {{ nvidia_container_toolkit_version }}
      Pin-Priority: 900
    mode: "0644"

- name: "Install nvidia-container-toolkit"
  ansible.builtin.apt:
    name: "nvidia-container-toolkit={{ nvidia_container_toolkit_version }}"
    state: present
    allow_downgrade: true
    update_cache: true

- name: "Configure containerd to use NVIDIA runtime"
  ansible.builtin.copy:
    dest: /etc/containerd/conf.d/99-nvidia.toml
    mode: "0644"
    content: |
      [plugins."io.containerd.cri.v1.runtime".containerd.runtimes.nvidia]
        runtime_type = "io.containerd.runc.v2"
        [plugins."io.containerd.cri.v1.runtime".containerd.runtimes.nvidia.options]
          BinaryName = "/usr/bin/nvidia-container-runtime"
          SystemdCgroup = true
  notify: "Restart containerd"

- name: "Ensure CDI directory exists"
  ansible.builtin.file:
    path: /etc/cdi
    state: directory
    mode: "0755"

- name: "Get checksum of CDI spec before"
  ansible.builtin.stat:
    path: /etc/cdi/nvidia.yaml
    checksum_algorithm: sha256
  register: kubeadm_cdi_spec_before

- name: "Generate NVIDIA CDI spec"
  ansible.builtin.command:
    cmd: nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
  changed_when: false

- name: "Get checksum of CDI spec after"
  ansible.builtin.stat:
    path: /etc/cdi/nvidia.yaml
    checksum_algorithm: sha256
  register: kubeadm_cdi_spec_after

- name: "Report CDI spec generation"
  ansible.builtin.debug:
    msg: "NVIDIA CDI spec generated/updated"
  changed_when: kubeadm_cdi_spec_before.stat.checksum | default('') != kubeadm_cdi_spec_after.stat.checksum

# Workaround for CUDA repo packages installing Vulkan ICD to different path than Debian
# See: https://github.com/NVIDIA/nvidia-container-toolkit/issues/1517
# TODO: Review when packages update: https://github.com/claytono/infra/issues/1120
- name: "Check for Vulkan ICD file from CUDA repo"
  ansible.builtin.stat:
    path: /usr/share/vulkan/icd.d/nvidia_icd.json
  register: kubeadm_vulkan_icd_file

- name: "Create Vulkan ICD symlink for CUDA repo packages"
  ansible.builtin.file:
    src: /usr/share/vulkan/icd.d/nvidia_icd.json
    dest: /etc/vulkan/icd.d/nvidia_icd.json
    state: link
    force: true
  when: kubeadm_vulkan_icd_file.stat.exists

- name: "Check for Vulkan layers file from CUDA repo"
  ansible.builtin.stat:
    path: /usr/share/vulkan/implicit_layer.d/nvidia_layers.json
  register: kubeadm_vulkan_layers_file

- name: "Create Vulkan layers symlink for CUDA repo packages"
  ansible.builtin.file:
    src: /usr/share/vulkan/implicit_layer.d/nvidia_layers.json
    dest: /etc/vulkan/implicit_layer.d/nvidia_layers.json
    state: link
    force: true
  when: kubeadm_vulkan_layers_file.stat.exists
